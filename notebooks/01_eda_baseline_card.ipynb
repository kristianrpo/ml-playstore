{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuraci√≥n de entorno\n",
    "\n",
    "En esta secci√≥n validamos que nuestro entorno de trabajo est√© correctamente configurado antes de comenzar el an√°lisis.  \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. **Versi√≥n de Python**  \n",
    "   - Se verifica que est√© instalada la versi√≥n **3.11 o superior** (se recomienda 3.13).  \n",
    "   - Esto garantiza compatibilidad con librer√≠as modernas de an√°lisis de datos y machine learning.\n",
    "\n",
    "2. **Importaci√≥n de librer√≠as base**  \n",
    "   - Se cargan librer√≠as fundamentales:  \n",
    "     - `numpy`, `pandas`: manipulaci√≥n y an√°lisis de datos.  \n",
    "     - `matplotlib`, `seaborn`: visualizaci√≥n de datos.  \n",
    "     - `scipy`: funciones estad√≠sticas.  \n",
    "   - Adem√°s se configuran estilos gr√°ficos y opciones de visualizaci√≥n en pandas para trabajar con tablas m√°s grandes.\n",
    "\n",
    "3. **Verificaci√≥n de versiones cr√≠ticas**  \n",
    "   - Se comprueba que `scikit-learn` est√© instalado y en una versi√≥n **>= 1.0.1**.  \n",
    "   - Esto es esencial ya que `scikit-learn` se usar√° para el modelado (baseline y posteriores).\n",
    "\n",
    "Con esta configuraci√≥n inicial aseguramos que el entorno sea reproducible y que todas las dependencias necesarias est√©n listas antes de continuar con el **EDA** y el **baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert sys.version_info >= (3, 11), \"Este notebook trabajo con python 3.11 o superiores (recomendado 3.13)\"\n",
    "\n",
    "print(f\"Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librer√≠as cr√≠ticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Metodolog√≠a CRISP-DM\n",
    "## 2.1. Comprensi√≥n del Negocio\n",
    "El problema de Google Play Store  \n",
    "\n",
    "**Contexto:**  \n",
    "Es 2025. El mercado de aplicaciones m√≥viles es altamente competitivo: millones de apps conviven en Google Play Store.  \n",
    "Los desarrolladores buscan mejorar la visibilidad de sus aplicaciones y los usuarios dependen del **rating promedio** para decidir qu√© descargar.  \n",
    "\n",
    "**Problema actual:**  \n",
    "- El rating se conoce **solo despu√©s** de que los usuarios descargan y rese√±an.  \n",
    "- Las valoraciones son **altamente variables** y pueden depender de m√∫ltiples factores (categor√≠a, descargas, precio, tama√±o, tipo de app).  \n",
    "- Los desarrolladores carecen de una herramienta para **estimar la calificaci√≥n potencial** de una app antes o durante su lanzamiento.  \n",
    "- La competencia es muy alta: una diferencia de d√©cimas en rating puede significar miles de descargas menos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.1. Soluci√≥n propuesta  \n",
    "Construir un **sistema autom√°tico de predicci√≥n de rating** de apps a partir de sus caracter√≠sticas disponibles en el dataset de Google Play Store.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. Definiendo el √©xito  \n",
    "\n",
    "**M√©trica de negocio:**  \n",
    "- Ayudar a los desarrolladores a anticipar la valoraci√≥n probable de su app.  \n",
    "- Reducir la dependencia de pruebas de mercado costosas o lentas.  \n",
    "- Identificar caracter√≠sticas clave que favorecen una alta valoraci√≥n (‚â• 4.3).  \n",
    "\n",
    "**M√©trica t√©cnica:**  \n",
    "- Lograr un **Error Absoluto Medio (MAE) < 0.5 estrellas** en la predicci√≥n de rating.  \n",
    "- Para la versi√≥n de clasificaci√≥n (alta vs. baja calificaci√≥n): obtener un **F1-score > 0.70**.  \n",
    "\n",
    "**¬øPor qu√© estos valores?**  \n",
    "- El rating va de 1 a 5 ‚Üí un error de 0.5 equivale a 10% de la escala.  \n",
    "- Una diferencia de medio punto puede marcar la visibilidad de la app en el ranking.  \n",
    "- Tasadores humanos (usuarios) tambi√©n muestran variabilidad similar en sus calificaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Preguntas cr√≠ticas antes de empezar  \n",
    "\n",
    "1. **¬øRealmente necesitamos ML?**  \n",
    "   - Alternativa 1: Calcular el promedio de ratings por categor√≠a ‚Üí demasiado simple, no captura variabilidad.  \n",
    "   - Alternativa 2: Reglas heur√≠sticas (ej. ‚Äúsi es gratis y tiene muchas descargas, tendr√° rating alto‚Äù) ‚Üí insuficiente.  \n",
    "   - **Conclusi√≥n:** S√≠, ML es apropiado para capturar relaciones no lineales y m√∫ltiples factores.  \n",
    "\n",
    "2. **¬øQu√© pasa si el modelo falla?**  \n",
    "   - Transparencia: aclarar que es una estimaci√≥n autom√°tica.  \n",
    "   - Complementar con rangos de predicci√≥n (ej: intervalo de confianza).  \n",
    "   - Mantener como referencia comparativa, no como √∫nico criterio de √©xito.  \n",
    "\n",
    "3. **¬øC√≥mo mediremos el impacto?**  \n",
    "   - Capacidad de anticipar apps con alta probabilidad de √©xito.  \n",
    "   - Ahorro de tiempo en validaciones preliminares.  \n",
    "   - Insights para desarrolladores sobre qu√© factores influyen m√°s en el rating.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2.2. Comprensi√≥n de los Datos  \n",
    "\n",
    "El objetivo de esta fase es explorar y entender el dataset de Google Play Store antes de construir modelos **(an√°lisis exploratorio)**.  \n",
    "Nos centraremos en:  \n",
    "\n",
    "1. **Vista r√°pida del dataset**  \n",
    "   - Identificar dimensiones (filas √ó columnas).  \n",
    "   - Tipos de datos (num√©ricos, categ√≥ricos, texto, fechas).  \n",
    "   - Valores faltantes obvios y rangos sospechosos.\n",
    "\n",
    "2. **Descripci√≥n de variables**  \n",
    "   - Revisar cada columna y entender su significado.  \n",
    "   - Detectar qu√© variables podr√≠an ser √∫tiles como predictores y cu√°l ser√° la variable objetivo (rating).  \n",
    "\n",
    "3. **Detecci√≥n de problemas en los datos**  \n",
    "   - An√°lisis de valores faltantes.  \n",
    "   - Estrategias: eliminar filas/columnas, imputar valores o crear indicadores de ‚Äúdato faltante‚Äù.  \n",
    "\n",
    "4. **Estad√≠sticas descriptivas y univariadas**  \n",
    "   - Media vs mediana (sesgo de la distribuci√≥n).  \n",
    "   - Desviaci√≥n est√°ndar (variabilidad, posibles outliers).  \n",
    "   - M√≠nimos/m√°ximos sospechosos.  \n",
    "   - Histogramas para ver forma (normal, sesgada, bimodal, uniforme, picos extra√±os).  \n",
    "\n",
    "5. **An√°lisis de variables categ√≥ricas**  \n",
    "   - Distribuci√≥n de categor√≠as (ej. categor√≠as de apps, tipo de app, content rating).  \n",
    "   - Detecci√≥n de clases dominantes o categor√≠as poco representadas.  \n",
    "\n",
    "6. **Correlaciones y relaciones entre variables**  \n",
    "   - Matriz de correlaci√≥n de Pearson para variables num√©ricas.  \n",
    "   - Identificar relaciones fuertes, moderadas o d√©biles.  \n",
    "   - Importante: recordar que **correlaci√≥n ‚â† causalidad**.  \n",
    "7. ** An√°lisis de outliers **\n",
    "   -  Tipos e identificaci√≥n de outliers a trav√©s de diferentes m√©todos.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**  \n",
    "No siempre es necesario aplicar todos los pasos con igual profundidad.  \n",
    "- Para este proyecto, el foco est√° en **identificar variables relevantes para predecir el rating** y **limpiar datos inconsistentes**.  \n",
    "- Otros an√°lisis m√°s complejos (ej. NLP sobre descripciones) se pueden dejar como trabajo futuro (seg√∫n trabajos de referencia investigados).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2.1 Descarga de datos  \n",
    "\n",
    "En este paso descargamos el dataset de Google Play Store desde Kaggle y lo organizamos en la estructura de carpetas del proyecto.  \n",
    "\n",
    "1. Usamos la librer√≠a `kagglehub` para acceder al dataset p√∫blico **`lava18/google-play-store-apps`** directamente desde Kaggle.  \n",
    "2. Se define una ruta clara dentro del proyecto para almacenar los datos originales: `../data/original/google-play-store/`. Esto ayuda a mantener la reproducibilidad y una estructura organizada.  \n",
    "3. Con la funci√≥n `shutil.copytree` copiamos los archivos descargados a la carpeta destino. De esta forma, el dataset queda disponible en nuestro directorio de trabajo para su an√°lisis posterior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "def download_data(origin_repository, target_folder):\n",
    "    # Descargar dataset\n",
    "    path = kagglehub.dataset_download(origin_repository)\n",
    "    \n",
    "    # Copiar los archivos descargados\n",
    "    shutil.copytree(path, target_folder, dirs_exist_ok=True)\n",
    "    \n",
    "\n",
    "download_data(\"lava18/google-play-store-apps\", \"../data/original/google-play-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2.2 Carga de datos  \n",
    "\n",
    "En este paso realizamos la **lectura del archivo CSV** que contiene el dataset descargado previamente.  \n",
    "\n",
    "- Definimos una funci√≥n `load_data(path, file)` que recibe la ruta y el nombre del archivo, y lo carga con `pandas.read_csv()`.  \n",
    "- Cargamos el dataset principal en la variable `applications_data` desde la carpeta `../data/original/google-play-store/`.  \n",
    "- Incluimos una verificaci√≥n simple:  \n",
    "  - Si el dataset se carga con √©xito, se imprime `\"Dataset loaded\"`.  \n",
    "  - En caso contrario, se muestra un mensaje de error.  \n",
    "\n",
    "Con esta validaci√≥n aseguramos que el archivo est√© disponible y correctamente le√≠do antes de continuar con el an√°lisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path, file):\n",
    "    return pd.read_csv(f\"{path}/{file}\")\n",
    "\n",
    "def convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte a num√©ricas solo las columnas que deber√≠an serlo, sin tocar 'Size'.\n",
    "    Usa to_numeric(errors='coerce') para evitar ValueError si aparece texto.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rating\n",
    "    if \"Rating\" in df.columns:\n",
    "        df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Reviews: quitar comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Reviews\" in df.columns:\n",
    "        df[\"Reviews\"] = (\n",
    "            df[\"Reviews\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Installs: quitar +, comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Installs\" in df.columns:\n",
    "        df[\"Installs Numeric\"] = (\n",
    "            df[\"Installs\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Price: quitar $ y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Price\" in df.columns:\n",
    "        df[\"Price\"] = (\n",
    "            df[\"Price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "\n",
    "    if \"Size\" in df.columns:\n",
    "        def parse_size(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip()\n",
    "                if x.endswith(\"M\"):\n",
    "                    return float(x[:-1])\n",
    "                elif x.endswith(\"k\") or x.endswith(\"K\"):\n",
    "                    return float(x[:-1]) / 1024  # KB -> MB\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        df[\"Size\"] = df[\"Size\"].apply(parse_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "temp_applications_data = load_data(\"../data/original/google-play-store\", \"googleplaystore.csv\")\n",
    "applications_data = convert_numeric_columns(temp_applications_data)\n",
    "\n",
    "\n",
    "if len(applications_data):\n",
    "    print(\"Dataset cargado\")\n",
    "else:\n",
    "    print(\"Error cargando dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2.3 Vista r√°pida del dataset\n",
    "\n",
    "**Dimensiones y columnas**\n",
    "- Registros: **10,841** filas.\n",
    "- Columnas: actualmente **14**; **originalmente eran 13** y se **a√±adi√≥** una columna derivada: **`Installs Numeric`** para an√°lisis con describe.\n",
    "- Memoria aproximada: **~1.2 MB**.\n",
    "\n",
    "**Tipos de datos (y transformaciones realizadas)**\n",
    "- Num√©ricas (`float64`): `Rating`, `Reviews`, `Size`, `Price`, **`Installs Numeric`**.\n",
    "- Categ√≥ricas / texto (`object`): `App`, `Category`, `Installs` *(forma original con ‚Äú1,000+‚Äù)*, `Type`, `Content Rating`, `Genres`, `Last Updated`, `Current Ver`, `Android Ver`.\n",
    "- Transformaciones ya aplicadas:\n",
    "  - **`Installs`** se **conserv√≥** en su formato original (categ√≥rico con ‚Äú+‚Äù y comas) **y** se cre√≥ **`Installs Numeric`** mapeando esos rangos a n√∫meros (0 ‚Ä¶ 1,000,000,000).\n",
    "  - **`Price`**, **`Reviews`** y **`Size`** fueron normalizadas/parseadas a **num√©rico** para an√°lisis y modelado.\n",
    "\n",
    "**Valores faltantes (no-null count ‚Üí faltantes aprox.)**\n",
    "- `Rating`: 9,367 ‚Üí **1,474 faltantes (~13.6%)**.\n",
    "- `Size`: 9,145 ‚Üí **1,696 faltantes (~15.6%)**.\n",
    "- `Current Ver`: 10,833 ‚Üí **8 faltantes (~0.07%)**.\n",
    "- `Android Ver`: 10,838 ‚Üí **3 faltantes (~0.03%)**.\n",
    "- `Content Rating`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Price`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Installs Numeric`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- Resto de columnas: **sin faltantes**.\n",
    "\n",
    "**Duplicados:**\n",
    "-   Se identificaron **483 filas duplicadas** (‚âà **4.46%** del\n",
    "    dataset).\\\n",
    "-   Ejemplos de duplicados incluyen apps como:\n",
    "    -   *Quick PDF Scanner + OCR FREE*\\\n",
    "    -   *Box*\\\n",
    "    -   *Google My Business*\\\n",
    "    -   *ZOOM Cloud Meetings*\\\n",
    "    -   *join.me -- Simple Meetings*\\\n",
    "\n",
    "**Rangos y valores sospechosos (seg√∫n `describe()`)**\n",
    "- `Rating`: **min = 1.0**, **max = 19.0** ‚Üí **19** es inv√°lido para la escala 1‚Äì5 (error de dato a corregir).\n",
    "- `Reviews`: media ~ **444k**, **p75 ‚âà 54,768**, **max ‚âà 78M** ‚Üí valores altos plausibles; tratar como **outliers**.\n",
    "- `Size` (MB): media ~ **21.5**, **p50 = 13**, **p75 = 30**, **max = 100** ‚Üí distribuci√≥n sesgada a la derecha; m√≠nimos muy bajos (**0.01**) a revisar.\n",
    "- `Price` (USD): **mediana = 0** y **p75 = 0** ‚Üí la mayor√≠a son **apps gratuitas**; **max = 400** sugiere outliers de precio.\n",
    "- `Installs Numeric`: **p25 = 1,000**, **p50 = 100,000**, **p75 = 5,000,000**, **max = 1,000,000,000** ‚Üí escala muy amplia; conviene usar **transformaciones log** o **binning** en el EDA/modelado.\n",
    "\n",
    "**Conclusi√≥n inicial**\n",
    "- Los **faltantes** m√°s relevantes est√°n en `Rating` y `Size`; habr√° que decidir estateg√≠a para aumentar, imputar o nivelar los datos.\n",
    "- Existen **outliers (no leg√≠timos)** (ej. `Rating = 19`) y variables con **colas largas** (ej. `Reviews`, `Installs Numeric`, `Price`).\n",
    "- Eliminar **duplicados** para evitar sesgos de an√°lisis y que no introduzcan ruidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(applications_data.head().style.background_gradient(cmap='RdYlGn', subset=['Rating']))\n",
    "\n",
    "# Informaci√≥n detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "applications_data.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(applications_data.describe().round(2).T)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar duplicados\n",
    "num_duplicados = applications_data.duplicated().sum()\n",
    "print(f\"Total de registros duplicados: {num_duplicados}\")\n",
    "\n",
    "# Mostrar ejemplos de duplicados si existen\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas:\\n\")\n",
    "    display(applications_data[applications_data.duplicated()].head())\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.4 Descripci√≥n de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price',\n",
    "    'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver',\n",
    "    'Installs Numeric'\n",
    "]\n",
    "\n",
    "tipos = [\n",
    "    'Categ√≥rica',          # App\n",
    "    'Categ√≥rica',          # Category\n",
    "    'Num√©rica (Target)',   # Rating\n",
    "    'Num√©rica',            # Reviews\n",
    "    'Num√©rica (MB)',       # Size\n",
    "    'Categ√≥rica (rango)',  # Installs\n",
    "    'Categ√≥rica',          # Type\n",
    "    'Num√©rica (USD)',      # Price\n",
    "    'Categ√≥rica',          # Content Rating\n",
    "    'Categ√≥rica',          # Genres\n",
    "    'Texto (fecha)',       # Last Updated (parseable a fecha)\n",
    "    'Texto',               # Current Ver\n",
    "    'Texto',               # Android Ver\n",
    "    'Num√©rica',            # Installs Numeric\n",
    "]\n",
    "\n",
    "descripciones = [\n",
    "    'Nombre de la aplicaci√≥n.',\n",
    "    'Categor√≠a oficial de la app en Google Play.',\n",
    "    'Calificaci√≥n promedio de usuarios (1 a 5).',\n",
    "    'N√∫mero de rese√±as reportadas.',\n",
    "    'Tama√±o aproximado de la app en MB.',\n",
    "    'Instalaciones en rango (p.ej., \"1,000+\").',\n",
    "    'Tipo de app (Free / Paid).',\n",
    "    'Precio en USD (0 para gratuitas).',\n",
    "    'Clasificaci√≥n de contenido (Everyone, Teen, etc.).',\n",
    "    'G√©nero(s) de la app.',\n",
    "    'Fecha de √∫ltima actualizaci√≥n (texto en origen).',\n",
    "    'Versi√≥n actual declarada por el desarrollador.',\n",
    "    'Versi√≥n m√≠nima de Android requerida.',\n",
    "    'Instalaciones convertidas a n√∫mero para an√°lisis.'\n",
    "]\n",
    "\n",
    "valores_faltantes = [applications_data[col].isnull().sum() if col in applications_data.columns else None for col in variables]\n",
    "\n",
    "metadata = {\n",
    "    'Variable': variables,\n",
    "    'Tipo': tipos,\n",
    "    'Descripci√≥n': descripciones,\n",
    "    'Valores Faltantes': valores_faltantes\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# Mostrar con resaltado de faltantes\n",
    "styled = df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x > 0 else '',\n",
    "    subset=['Valores Faltantes']\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Resumen de dtypes originales (informativo)\n",
    "dtypes_resumen = applications_data[variables].dtypes.astype(str).reset_index()\n",
    "dtypes_resumen.columns = ['Variable', 'dtype pandas']\n",
    "display(dtypes_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.2.5 Detecci√≥n de problemas en los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Resumen de hallazgos (valores faltantes):**\n",
    "- `Size` ‚âà 15.6% y `Rating` ‚âà 13.6% concentran la mayor√≠a de los faltantes.\n",
    "- Faltantes puntuales (‚âà0.01%): `Type`, `Price`, `Content Rating`, `Installs Numeric` ocurren en la misma(s) fila(s) ‚Üí patr√≥n conjunto.\n",
    "- `Android Ver` (0.03%) y `Current Ver` (0.07%) con faltantes residuales, parcialmente correlacionados con el grupo anterior.\n",
    "\n",
    "**Heatmap de correlaci√≥n de patrones de faltantes (interpretaci√≥n):**\n",
    "- Correlaci√≥n 1.00 entre `Type`, `Price`, `Content Rating`, `Installs Numeric`: las ausencias co-ocurren en el/los mismos registros. Acciones coordinadas.\n",
    "- `Android Ver` muestra correlaci√≥n moderada (~0.58) con ese grupo: algunas veces falta junto con ellos.\n",
    "- `Size`, `Rating`, `Current Ver` tienen patrones de faltantes independientes del grupo anterior (correlaciones cercanas a 0), lo que sugiere causas distintas.\n",
    "\n",
    "#### Posibles estrategias de correcci√≥n\n",
    "\n",
    "- Limpieza b√°sica\n",
    "  - Eliminar duplicados (483 filas) para evitar sesgos.\n",
    "  - Validar y corregir outliers imposibles, p. ej., `Rating = 19` ‚Üí convertir a NaN para tratarlo como faltante.\n",
    "\n",
    "- Imputaci√≥n (conservadora y por grupos)\n",
    "  - `Rating` (target): para modelado, eliminar filas sin `Rating`; para EDA descriptivo, imputar mediana por `Category` solo para visualizaci√≥n.\n",
    "  - `Size`: imputar mediana por `Category √ó Type` y crear indicador `size_missing`.\n",
    "  - `Android Ver`, `Current Ver`: imputar moda por `Category` y crear indicadores `androidver_missing`, `currentver_missing`.\n",
    "  - Faltantes conjuntos (`Type`, `Price`, `Content Rating`, `Installs Numeric`):\n",
    "    - Si es 1 fila: eliminarla es lo m√°s simple y seguro.\n",
    "    - Alternativa (si se prefiere imputar):\n",
    "      - `Type`: inferir desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid).\n",
    "      - `Price`: 0 si `Type == Free`, si `Paid` usar mediana por `Category`.\n",
    "      - `Content Rating`: moda por `Category`.\n",
    "      - `Installs Numeric`: mediana por `Category √ó Type` o por bin de `Installs`.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategias de ‚Äúnivelaci√≥n‚Äù seg√∫n los porcentajes observados\n",
    "\n",
    "- Size (~15.6% faltantes, >5% y <<60%)\n",
    "  - Acci√≥n: imputar mediana por grupo `Category √ó Type`.\n",
    "  - A√±adir flag: `size_missing = 1` cuando falte (conserva se√±al de ausencia).\n",
    "  - Justificaci√≥n: volumen relevante; la mediana por grupos respeta diferencias entre tipos/categor√≠as.\n",
    "\n",
    "- Rating (~13.6% faltantes, >5% y <<60%) [variable objetivo]\n",
    "  - Para modelado: eliminar filas sin `Rating` (evita sesgo por imputaci√≥n del target).\n",
    "  - Para EDA descriptivo: si se requiere visualizar completos, imputar mediana por `Category` solo para gr√°ficos/tablas (no para entrenamiento).\n",
    "  - Justificaci√≥n: imputar el target puede distorsionar m√©tricas.\n",
    "\n",
    "- Current Ver (0.07%) y Android Ver (0.03%) (<5%)\n",
    "  - Acci√≥n: imputar con la moda por `Category`. Flags opcionales `currentver_missing` y `androidver_missing`.\n",
    "  - Justificaci√≥n: impacto √≠nfimo; moda es suficiente y estable.\n",
    "\n",
    "- Faltantes ‚Äúen bloque‚Äù en la misma fila: Type, Price, Content Rating, Installs Numeric (‚âà0.01% cada uno; correlaci√≥n 1.00)\n",
    "  - Si es 1 fila: eliminarla directamente.\n",
    "  - Si hubiera m√°s en el futuro y se prefiriera imputar coordinadamente:\n",
    "    - `Type` desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid),\n",
    "    - `Price` = 0 si `Free`, si `Paid` usar mediana por `Category`,\n",
    "    - `Content Rating` = moda por `Category`,\n",
    "    - `Installs Numeric` = mediana por `Category √ó Type`.\n",
    "  - Justificaci√≥n: co-ocurren; eliminar 1 fila no afecta el conjunto y evita inconsistencias.\n",
    "\n",
    "- Transformaciones para estabilizar distribuciones (complementarias a la imputaci√≥n)\n",
    "  - `Reviews` y `Installs Numeric`: aplicar `log1p` para an√°lisis y futuros modelos. *****************************\n",
    "  - `Installs` (rangos): tratar como ordinal/bins en el EDA.\n",
    "\n",
    "- Limpieza previa necesaria\n",
    "  - Eliminar duplicados (483 filas).\n",
    "  - Corregir valores imposibles detectados en el EDA (ej. `Rating = 19` ‚Üí NaN) y re-entrar al flujo de imputaci√≥n/nivelaci√≥n anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"An√°lisis completo de valores faltantes con visualizaciones.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': missing_counts.values,\n",
    "        'Porcentaje': missing_pct.values,\n",
    "        'Tipo_Dato': df.dtypes.values\n",
    "    })\n",
    "\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "    if len(missing_df) == 0:\n",
    "        print(\"No hay valores faltantes en el dataset\")\n",
    "        return missing_df\n",
    "\n",
    "    # Visualizaci√≥n: barras y correlaci√≥n de patrones de faltantes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Gr√°fico de barras de % faltantes\n",
    "    ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "    ax1.set_xlabel('Columna')\n",
    "    ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "    ax1.set_title('Valores Faltantes por Columna')\n",
    "    ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "    ax1.axhline(y=60, color='purple', linestyle='--', label='Umbral 60%')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Heatmap de correlaci√≥n de patrones de faltantes\n",
    "    mask_df = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "    if mask_df.shape[1] >= 2:\n",
    "        corr = mask_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "        ax2.set_title('Correlaci√≥n de Patrones de Valores Faltantes')\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Correlaci√≥n de faltantes (no aplica: 1 columna)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "missing_analysis = analyze_missing_values(applications_data)\n",
    "if missing_analysis is not None and not missing_analysis.empty:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2.6 Estadisticas descriptivas y univariadas (n√∫merico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A partir de la tabla de estad√≠sticas y los gr√°ficos generados para `Rating`, `Reviews`, `Size`, `Price` e `Installs Numeric`, se observan los siguientes puntos clave.\n",
    "\n",
    "- Rating\n",
    "  - Media ‚âà 4.19 y mediana ‚âà 4.30 ‚Üí ligera cola a la izquierda (m√°s apps con rating alto). Hay un valor imposible (‚âà19), confirmado en el boxplot/Q-Q como outlier extremo.\n",
    "  - Outliers: ~5% por IQR, dominados por el valor inv√°lido y algunos ratings bajos.\n",
    "  - Q-Q plot: desviaci√≥n frente a normalidad, esperable para una variable acotada [1,5].\n",
    "  - Implicaci√≥n/acci√≥n: eliminar filas sin `Rating` para modelado; corregir `Rating=19 ‚Üí NaN` y excluir; no aplicar transformaciones (la escala es ya interpretables).\n",
    "\n",
    "- Reviews\n",
    "  - Media ‚â´ mediana (pico en 0‚Äìpocos miles; m√°ximo ‚âà 78M) ‚Üí cola muy larga a la derecha.\n",
    "  - Boxplot: ~18% outliers por IQR (muchas apps con rese√±as muy altas).\n",
    "  - Q-Q plot: gran desviaci√≥n de normalidad (heavy tail).\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva muy d√©bil (~0.07), tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Reviews)` para estabilizar la distribuci√≥n en an√°lisis/modelado; considerar winsorizar p99.9 para vistas tabulares si se desea.\n",
    "\n",
    "- Size (MB)\n",
    "  - Media > mediana (‚âà 21.5 vs 13) ‚Üí sesgo a la derecha; valores hasta 100 MB.\n",
    "  - ~6% outliers por IQR, especialmente en colas altas.\n",
    "  - Q-Q plot: curvatura en colas; no normal.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva d√©bil (~0.08); se√±al muy tenue.\n",
    "  - Implicaci√≥n/acci√≥n: imputar faltantes por `Category √ó Type` y a√±adir `size_missing`; opcionalmente probar `log1p(Size)` o binning para robustecer.\n",
    "\n",
    "- Price (USD)\n",
    "  - Mediana = 0 (mayor√≠a gratis) y cola a la derecha con m√°ximos altos (‚âà 400).\n",
    "  - ~7% outliers por IQR; Q-Q muestra heavy tail.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n negativa muy d√©bil (~-0.02).\n",
    "  - Implicaci√≥n/acci√≥n: crear `is_free = (Price == 0)` y, si se usa `Price` continuo, considerar `log1p(Price)` para las pocas apps pagas; validar coherencia `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Installs Numeric\n",
    "  - Media ‚â´ mediana (100k) con m√°ximo 1e9 ‚Üí distribuci√≥n extremadamente sesgada a la derecha.\n",
    "  - ~7‚Äì8% outliers por IQR; Q-Q muy alejado de normalidad.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n d√©bil positiva (~0.05) y tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Installs Numeric)` o bins ordinales para an√°lisis; verificar coherencia con `Installs` textual.\n",
    "\n",
    "Recomendaciones transversales\n",
    "- Eliminar duplicados antes de resumir para evitar sesgos.\n",
    "- Tratar outliers evidentes no-leg√≠timos (p. ej. `Rating=19`). Para colas largas leg√≠timas (`Reviews`, `Installs Numeric`, `Price`): preferir `log1p` o winsorizaci√≥n solo para visualizaciones.\n",
    "- Mantener consistencia: `Type=Free ‚áí Price=0`; `Installs Numeric` coherente con el rango de `Installs`.\n",
    "- Para relaciones con `Rating`, las correlaciones lineales observadas son d√©biles; la se√±al puede emerger mejor con interacciones (p. ej., `is_free √ó installs_bin`) o modelos no lineales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selecci√≥n de columnas num√©ricas relevantes\n",
    "numeric_cols = [c for c in ['Rating', 'Reviews', 'Size', 'Price', 'Installs Numeric'] if c in applications_data.columns]\n",
    "\n",
    "# Tabla de estad√≠sticas b√°sicas (media, mediana, std, min, p25, p50, p75, max)\n",
    "describe_tbl = applications_data[numeric_cols].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "# M√©tricas adicionales robustas\n",
    "extra = pd.DataFrame(index=numeric_cols)\n",
    "extra['mad'] = [stats.median_abs_deviation(applications_data[c].dropna()) for c in numeric_cols]\n",
    "extra['skew'] = [applications_data[c].skew(skipna=True) for c in numeric_cols]\n",
    "extra['kurtosis'] = [applications_data[c].kurtosis(skipna=True) for c in numeric_cols]\n",
    "extra['cv'] = [applications_data[c].std(skipna=True) / applications_data[c].mean(skipna=True) if applications_data[c].mean(skipna=True) not in [0, np.nan] else np.nan for c in numeric_cols]\n",
    "\n",
    "stats_table = describe_tbl.join(extra)\n",
    "display(stats_table.round(3))\n",
    "\n",
    "\n",
    "def univariate_analysis(df: pd.DataFrame, column: str, target: str | None = None):\n",
    "    \"\"\"An√°lisis univariado con histograma, boxplot, Q-Q plot y relaci√≥n con target.\"\"\"\n",
    "    series = df[column].dropna()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Histograma con l√≠neas de media y mediana\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(series, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(series.mean(), color='red', linestyle='--', label=f\"Media: {series.mean():.2f}\")\n",
    "    ax1.axvline(series.median(), color='green', linestyle='--', label=f\"Mediana: {series.median():.2f}\")\n",
    "    ax1.set_title(f\"Distribuci√≥n de {column}\")\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2) Boxplot + conteo de outliers (IQR)\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(series, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)\n",
    "    n_out = int(outliers_mask.sum())\n",
    "    pct_out = 100 * n_out / len(series) if len(series) else 0\n",
    "    ax2.set_title(f\"Boxplot de {column}\")\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.text(1.1, Q3, f\"Outliers: {n_out} ({pct_out:.1f}%)\", fontsize=10)\n",
    "\n",
    "    # 3) Q-Q plot normal\n",
    "    ax3 = axes[1, 0]\n",
    "    stats.probplot(series, dist='norm', plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # 4) Relaci√≥n con target si aplica\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns and column != target:\n",
    "        valid = df[[column, target]].dropna()\n",
    "        ax4.scatter(valid[column], valid[target], alpha=0.4, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f\"{column} vs {target}\")\n",
    "        # L√≠nea de tendencia (ajuste lineal simple)\n",
    "        if len(valid) > 1:\n",
    "            z = np.polyfit(valid[column], valid[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            xs = np.linspace(valid[column].min(), valid[column].max(), 200)\n",
    "            ax4.plot(xs, p(xs), 'r--', alpha=0.8, label='Tendencia')\n",
    "            corr = valid[column].corr(valid[target])\n",
    "            ax4.text(0.05, 0.95, f\"Correlaci√≥n: {corr:.3f}\", transform=ax4.transAxes,\n",
    "                     fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            ax4.legend()\n",
    "    else:\n",
    "        ax4.axis('off')\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"An√°lisis Univariado: {column}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar an√°lisis univariado para cada m√©trica num√©rica, relacionando con Rating\n",
    "for col in numeric_cols:\n",
    "    univariate_analysis(applications_data, col, target='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.2.7. An√°lisis Univariado Categ√≥rico\n",
    "- Category\n",
    "  - Distribuci√≥n: alta concentraci√≥n en `FAMILY` (~19%) y `GAME` (~12%). El resto de categor√≠as tienen menor peso individual; el grupo `Others` acumula ~31% del total.\n",
    "  - Rating por categor√≠a: diferencias moderadas; la **mediana** suele estar entre 4.2‚Äì4.4. Algunas categor√≠as muestran desviaci√≥n est√°ndar mayor (p. ej., `PRODUCTIVITY`, `LIFESTYLE`), indicando m√°s variabilidad de valoraci√≥n.\n",
    "  - Implicaciones: riesgo de sesgo por categor√≠as mayoritarias en an√°lisis agregados. Para modelado, conviene usar dummies Top-K o codificaci√≥n ordinal/target encoding con cuidado (evitar fuga). Agrupar colas largas en `Others` es adecuado para visualizaci√≥n.\n",
    "\n",
    "- Content Rating\n",
    "  - Distribuci√≥n: `Everyone` domina (~79%), seguido por `Teen` (~12%); `Mature 17+` y `Everyone 10+` suman ~9% en conjunto; clases raras casi nulas.\n",
    "  - Rating por nivel de contenido: medias similares (‚âà4.1‚Äì4.3). `Teen` tiende a mediana 4.3 y variabilidad algo menor; `Mature 17+` muestra algo m√°s de dispersi√≥n.\n",
    "  - Implicaciones: por el fuerte desbalance, esta variable aporta se√±al limitada por s√≠ sola. √ötil como interacci√≥n con `Category`/`Genres`.\n",
    "\n",
    "- Type\n",
    "  - Distribuci√≥n: `Free` ‚âà 93%, `Paid` ‚âà 7% (clase muy desbalanceada); existe un registro an√≥malo (valor 0) en los gr√°ficos que debe eliminarse/corregirse.\n",
    "  - Rating por tipo: medias muy cercanas (Free ‚âà 4.19, Paid ‚âà 4.27). La diferencia es peque√±a y probablemente no significativa sin controlar otras variables (p. ej., `Category`).\n",
    "  - Implicaciones: por el desbalance extremo, conviene usar `is_free` como binaria y, si se modela interacci√≥n con `Installs` o `Price`, puede emerger se√±al. Validar regla `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Genres Main (primer g√©nero)\n",
    "  - Distribuci√≥n: gran cola larga; `Others` concentra ~48%. Entre Top-12, `Tools`, `Entertainment` y `Education` destacan en frecuencia.\n",
    "  - Rating por g√©nero: diferencias peque√±as (medianas ~4.2‚Äì4.4), con algunas variaciones en dispersi√≥n (p. ej., `Medical` y `Lifestyle` m√°s variables).\n",
    "  - Implicaciones: por la alta cardinalidad y colas largas, mantener Top-K + `Others` en EDA ayuda a la legibilidad. Para modelado, preferir codificaci√≥n que reduzca dimensionalidad (Top-K dummies, hashing, o target encoding con validaci√≥n adecuada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_compact(df: pd.DataFrame, cat_col: str, target_col: str, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Versi√≥n compacta para variables con muchas categor√≠as:\n",
    "    - Ordena por frecuencia, muestra top_n y agrupa el resto en \"Others\".\n",
    "    - Barras horizontales, pie chart compacto, boxplot y tabla para top_n.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, target_col]].dropna(subset=[cat_col, target_col]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"Sin datos para {cat_col} y {target_col}\")\n",
    "        return\n",
    "\n",
    "    counts = data[cat_col].value_counts()\n",
    "    top_cats = counts.head(top_n)\n",
    "    others_count = counts.iloc[top_n:].sum()\n",
    "\n",
    "    # Mapeo a top_n + Others\n",
    "    mapping = {c: c for c in top_cats.index}\n",
    "    data['__cat__'] = data[cat_col].where(data[cat_col].isin(top_cats.index), other='Others')\n",
    "\n",
    "    # Recalcular conteos con Others\n",
    "    counts_compact = data['__cat__'].value_counts()\n",
    "    order = list(top_cats.index) + (['Others'] if 'Others' in counts_compact.index else [])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Barras horizontales (mejor legibilidad)\n",
    "    ax1 = axes[0, 0]\n",
    "    vals = counts_compact.loc[order]\n",
    "    ax1.barh(range(len(vals)), vals.values, color=plt.cm.Set3(range(len(vals))))\n",
    "    ax1.set_yticks(range(len(vals)))\n",
    "    ax1.set_yticklabels(order)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title(f'Distribuci√≥n (Top {top_n}) de {cat_col}')\n",
    "    ax1.set_xlabel('Frecuencia')\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax1.text(v, i, f'  {v} ({v/len(data)*100:.1f}%)', va='center')\n",
    "\n",
    "    # 2) Pie chart compacto\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(vals.values, labels=order, autopct='%1.1f%%', startangle=140,\n",
    "            colors=plt.cm.Set3(range(len(vals))))\n",
    "    ax2.set_title(f'Proporci√≥n (Top {top_n} + Others) de {cat_col}')\n",
    "\n",
    "    # 3) Boxplot del target por categor√≠a (solo top_n)\n",
    "    ax3 = axes[1, 0]\n",
    "    top_mask = data['__cat__'] != 'Others'\n",
    "    data_top = data[top_mask]\n",
    "    data_top.boxplot(column=target_col, by='__cat__', ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col} (Top {top_n})')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # 4) Tabla de estad√≠sticas por categor√≠a (solo top_n y Others si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    stats_by_cat = data.groupby('__cat__')[target_col].agg(['count', 'mean', 'median', 'std']).loc[order].round(2)\n",
    "    table = ax4.table(cellText=stats_by_cat.reset_index().values,\n",
    "                      colLabels=['Categor√≠a', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                      cellLoc='center', loc='center', colWidths=[0.35, 0.12, 0.16, 0.16, 0.16])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.05, 1.25)\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.suptitle(f'An√°lisis Categ√≥rico Compacto: {cat_col}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar la versi√≥n compacta para las categ√≥ricas clave\n",
    "for cat in [c for c in ['Category', 'Content Rating', 'Type', 'Genres Main', 'Installs'] if c in applications_data.columns]:\n",
    "    analyze_categorical_compact(applications_data, cat, 'Rating', top_n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 2.2.8. An√°lisis de correlaci√≥n entre variables\n",
    "\n",
    "#### 2.2.8.1. Variables con Mayor Relaci√≥n\n",
    "- Existe una fuerte correlaci√≥n positiva entre **Installs Numeric** y **Reviews**:\n",
    "  - Pearson: 0.64 (relaci√≥n lineal moderada-fuerte).\n",
    "  - Spearman: 0.97 (relaci√≥n mon√≥tonica muy fuerte).\n",
    "- Esto implica que a mayor n√∫mero de instalaciones, mayor n√∫mero de rese√±as.\n",
    "\n",
    "#### 2.2.8.2. Correlaci√≥n de Pearson\n",
    "- En general, las correlaciones de Pearson muestran relaciones m√°s d√©biles que Spearman, lo cual indica que las relaciones lineales no son tan marcadas.\n",
    "- **Installs Numeric y Reviews** presentan la correlaci√≥n lineal m√°s alta (0.64), siendo moderada-fuerte.\n",
    "- **Size y Reviews** muestran una correlaci√≥n positiva baja/D√©bil (0.24).\n",
    "- El resto de variables (Rating, Price) tienen correlaciones casi nulas con las dem√°s, lo que refleja poca relaci√≥n lineal.\n",
    "\n",
    "#### 2.2.8.3. Correlaci√≥n de Spearman\n",
    "- **Installs Numeric y Reviews** tienen la correlaci√≥n m√°s fuerte (0.97).\n",
    "- **Size** muestra correlaci√≥n moderada con **Reviews** (0.37) y con **Installs Numeric** (0.35).\n",
    "- **Price** presenta correlaciones negativas con **Reviews** (-0.17) e **Installs Numeric** (-0.24).\n",
    "\n",
    "#### 2.2.8.4. Observaciones Clave\n",
    "- El n√∫mero de instalaciones y las rese√±as son las variables m√°s relacionadas, lo cual es l√≥gico, ya que m√°s usuarios generan m√°s interacciones.\n",
    "- El tama√±o de la aplicaci√≥n influye ligeramente en rese√±as e instalaciones, pero no de forma determinante.\n",
    "- El precio no solo carece de relaci√≥n positiva, sino que parece tener un impacto negativo sobre la popularidad (menos instalaciones y rese√±as).\n",
    "\n",
    "#### 2.2.8.5. Conclusi√≥n\n",
    "- **Installs Numeric** y **Reviews** son las m√©tricas m√°s cr√≠ticas en el dataset de **Google Play Store**, ya que reflejan el √©xito y la popularidad de la aplicaci√≥n.\n",
    "- **Size** es un factor secundario con cierta relaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaci√≥n mejorado para el proyecto de Google Play Store\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"An√°lisis de correlaci√≥n con m√∫ltiples m√©tricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlaci√≥n de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlaci√≥n de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlaci√≥n de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlaci√≥n de Spearman (Monot√≥nica)')\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Correlaci√≥n Multi-m√©trica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\nüîó Correlaciones Significativas:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, corr_matrix in zip(['Pearson', 'Spearman'], [corr_pearson, corr_spearman]):\n",
    "        print(f\"\\n{method}:\")\n",
    "        significant_corr = corr_matrix[(abs(corr_matrix) > 0.3) & (corr_matrix != 1)].stack()\n",
    "        for (var1, var2), corr in significant_corr.items():\n",
    "            strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"D√©bil\"\n",
    "            direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "            print(f\"  ‚Ä¢ {var1} y {var2}: {corr:+.3f} ({strength} {direction})\")\n",
    "    \n",
    "# Ejecutar el an√°lisis de correlaci√≥n\n",
    "correlation_analysis(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.2.9. An√°lisis de Outliers (IQR, Z-Score e Isolation Forest)\n",
    "**Resumen cuantitativo**\n",
    "- Total de registros analizados: **10,841**.\n",
    "- Filas marcadas como outlier por m√©todo:\n",
    "  - IQR: **3,489** filas (32.18%) ‚Üí refleja colas largas especialmente en `Reviews`, `Installs Numeric`, `Price`.\n",
    "  - Z-Score (> |3|): **654** filas (6.03%) ‚Üí mucho m√°s selectivo, captura extremos verdaderamente alejados tras estandarizaci√≥n.\n",
    "  - Isolation Forest (contaminaci√≥n=10%): **1,084** filas (10.0%) ‚Üí patr√≥n no lineal de anomal√≠as combinadas.\n",
    "- Consenso entre m√©todos:\n",
    "  - Detectadas por los 3 m√©todos: **502** filas (casos altamente an√≥malos).\n",
    "  - Detectadas exactamente por 2 m√©todos: **731** filas (an√≥malas consistentes, revisar antes de decidir acci√≥n).\n",
    "\n",
    "**Variables m√°s afectadas (IQR)**\n",
    "- `Reviews`: **1,925** outliers ‚Üí distribuci√≥n extremadamente sesgada; valores muy altos representan apps masivas (probablemente leg√≠timos).\n",
    "- `Installs Numeric`: **828** outliers ‚Üí escalas de descargas masivas (1e7‚Äì1e9).\n",
    "- `Price`: **800** outliers ‚Üí pocos productos de precio elevado (‚â• p75 + 1.5¬∑IQR); revisar si son apps premium leg√≠timas.\n",
    "- `Size`: **564** outliers ‚Üí tama√±os extremos (muy grandes o inusualmente peque√±os).\n",
    "- `Rating`: **504** outliers ‚Üí incluye valores extremos bajos y el caso inv√°lido (`Rating=19`).\n",
    "\n",
    "**Interpretaci√≥n y criterios**\n",
    "- Muchos outliers provienen de fen√≥menos de cola larga t√≠picos (popularidad extrema o modelo freemium/premium).\n",
    "- No se recomienda eliminar masivamente outliers de `Reviews` o `Installs Numeric` sin antes transformar (`log1p`) o agrupar (binning), para no perder informaci√≥n sobre apps exitosas.\n",
    "- El valor inv√°lido `Rating=19` debe normalizarse a `NaN` y excluirse de modelado. Otros ratings muy bajos pueden mantenerse (aportan contraste).\n",
    "- Outliers en `Price` podr√≠an segmentarse: gratis (0), bajo costo (0 < p ‚â§ 10), premium (10 < p ‚â§ 50), ultra premium (>50).\n",
    "\n",
    "\n",
    "**Conclusi√≥n**\n",
    "El comportamiento extremo de `Reviews` e `Installs Numeric` refleja la naturaleza desigual del mercado (unas pocas apps concentran gran parte de la atenci√≥n). Un manejo cuidadoso (transformaciones y flags) preservar√° informaci√≥n √∫til sin distorsionar el entrenamiento. Se prioriza limpieza puntual (ratings inv√°lidos) sobre eliminaci√≥n agresiva de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detecci√≥n de outliers usando m√∫ltiples m√©todos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # M√©todo 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # M√©todo 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # M√©todo 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (M√©todo IQR)')\n",
    "    ax1.set_ylabel('N√∫mero de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribuci√≥n de outliers por m√©todo\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparaci√≥n de M√©todos de Detecci√≥n')\n",
    "    ax2.set_ylabel('N√∫mero de Outliers Detectados')\n",
    "    ax2.set_xlabel('M√©todo')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estad√≠stico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detecci√≥n de Anomal√≠as:\n",
    "    \n",
    "    ‚Ä¢ Total de observaciones: {len(df):,}\n",
    "    ‚Ä¢ Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables m√°s afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Investigar outliers antes de eliminar. \n",
    "    Pueden contener informaci√≥n valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Outliers y Anomal√≠as', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "# Ejecutar la detecci√≥n de outliers en el dataset de aplicaciones\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# 3. Transformaci√≥n de Datos\n",
    "\n",
    "En base a todos los hallazgos del **An√°lisis Exploratorio de Datos (EDA)**, aplicaremos las siguientes t√©cnicas de limpieza y transformaci√≥n:\n",
    "\n",
    "## 3.1. Resumen de problemas detectados\n",
    "\n",
    "Durante el EDA identificamos:\n",
    "\n",
    "1. **Duplicados**: 483 filas duplicadas (~4.46%)\n",
    "2. **Valores imposibles**: Rating = 19 (fuera del rango 1-5)\n",
    "3. **Valores faltantes**: \n",
    "   - Size ‚âà 15.6%\n",
    "   - Rating ‚âà 13.6%\n",
    "   - Current Ver, Android Ver, Content Rating, Type, Price (<1%)\n",
    "4. **Outliers leg√≠timos**: Distribuciones con colas largas en Reviews, Installs Numeric, Price, Size\n",
    "5. **Variables con distribuciones sesgadas**: Requieren transformaciones logar√≠tmicas\n",
    "6. **Inconsistencias**: Necesidad de validar coherencia entre Type y Price\n",
    "\n",
    "## 3.2. Plan de transformaci√≥n\n",
    "\n",
    "Aplicaremos las siguientes transformaciones en orden:\n",
    "\n",
    "1. **Eliminaci√≥n de duplicados**\n",
    "2. **Correcci√≥n de valores imposibles**\n",
    "3. **Imputaci√≥n de valores faltantes** (estrategia por variable)\n",
    "4. **Validaci√≥n de consistencia** entre variables relacionadas\n",
    "5. **Transformaciones de variables num√©ricas** (log, binning)\n",
    "6. **Creaci√≥n de variables derivadas** (features engineering b√°sico)\n",
    "7. **Resumen final** del dataset limpio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 3.3.1. Eliminaci√≥n de Duplicados\n",
    "\n",
    "Eliminamos las **483 filas duplicadas** detectadas en el EDA para evitar:\n",
    "- Sesgos en an√°lisis estad√≠sticos\n",
    "- Sobrepeso de ciertas apps en el modelado\n",
    "- Distorsi√≥n de m√©tricas de evaluaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset para transformaciones\n",
    "df_clean = applications_data.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 1: ELIMINACI√ìN DE DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado inicial\n",
    "print(f\"\\nüìä Registros antes de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"üîç Duplicados encontrados: {df_clean.duplicated().sum():,} ({df_clean.duplicated().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de duplicados antes de eliminar\n",
    "if df_clean.duplicated().sum() > 0:\n",
    "    print(\"\\nüìã Ejemplos de aplicaciones duplicadas:\")\n",
    "    duplicated_apps = df_clean[df_clean.duplicated(keep=False)].sort_values('App')\n",
    "    display(duplicated_apps[['App', 'Category', 'Rating', 'Reviews', 'Installs']].head(10))\n",
    "\n",
    "# Eliminar duplicados (manteniendo la primera ocurrencia)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Estado final\n",
    "print(f\"\\n‚úÖ Registros despu√©s de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"üóëÔ∏è  Filas eliminadas: {len(applications_data) - len(df_clean):,}\")\n",
    "print(f\"üìà Reducci√≥n: {((len(applications_data) - len(df_clean))/len(applications_data)*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 3.3.2. Correcci√≥n de Valores Imposibles\n",
    "\n",
    "Corregimos valores que est√°n fuera del rango v√°lido:\n",
    "- **Rating = 19**: valor imposible (escala 1-5) ‚Üí convertir a `NaN`\n",
    "- Cualquier Rating < 1 o > 5 ‚Üí convertir a `NaN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 2: CORRECCI√ìN DE VALORES IMPOSIBLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar valores de Rating fuera del rango [1, 5]\n",
    "invalid_ratings = df_clean['Rating'][(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5)]\n",
    "print(f\"\\nüîç Ratings inv√°lidos encontrados: {len(invalid_ratings)}\")\n",
    "\n",
    "if len(invalid_ratings) > 0:\n",
    "    print(\"\\nüìã Ejemplos de ratings inv√°lidos:\")\n",
    "    invalid_apps = df_clean[df_clean['Rating'].isin(invalid_ratings)]\n",
    "    display(invalid_apps[['App', 'Category', 'Rating', 'Reviews']].head())\n",
    "    \n",
    "    # Mostrar distribuci√≥n de valores inv√°lidos\n",
    "    print(f\"\\nüìä Valores inv√°lidos √∫nicos: {sorted(invalid_ratings.dropna().unique())}\")\n",
    "    \n",
    "    # Corregir: convertir valores inv√°lidos a NaN\n",
    "    df_clean.loc[(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5), 'Rating'] = np.nan\n",
    "    \n",
    "    print(f\"\\n‚úÖ Valores inv√°lidos corregidos (convertidos a NaN)\")\n",
    "    print(f\"üìà Total de NaN en Rating ahora: {df_clean['Rating'].isnull().sum()}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No se encontraron ratings inv√°lidos\")\n",
    "\n",
    "# Verificar otros valores imposibles (negativos en columnas que no pueden serlo)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificando valores negativos en columnas num√©ricas:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numeric_cols = ['Reviews', 'Size', 'Price', 'Installs Numeric']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  {col}: {negative_count} valores negativos encontrados\")\n",
    "            df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "            print(f\"   ‚úÖ Corregidos a NaN\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {col}: Sin valores negativos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 3.3.3. Validaci√≥n de Consistencia entre Variables\n",
    "\n",
    "Validamos y corregimos inconsistencias l√≥gicas entre variables relacionadas:\n",
    "- **Type vs Price**: Si `Type = 'Free'`, entonces `Price` debe ser 0\n",
    "- **Type vs Price**: Si `Price > 0`, entonces `Type` debe ser 'Paid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 3: VALIDACI√ìN DE CONSISTENCIA ENTRE VARIABLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validar consistencia Type vs Price\n",
    "print(\"\\nüîç Validando consistencia entre Type y Price:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Casos inconsistentes: Type='Free' pero Price > 0\n",
    "free_but_paid = df_clean[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0)]\n",
    "print(f\"\\n‚ö†Ô∏è  Apps marcadas como 'Free' pero con Price > 0: {len(free_but_paid)}\")\n",
    "if len(free_but_paid) > 0:\n",
    "    display(free_but_paid[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price > 0, cambiar Type a 'Paid'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    print(f\"   ‚úÖ Corregido: Type cambiado a 'Paid'\")\n",
    "\n",
    "# Casos inconsistentes: Type='Paid' pero Price = 0\n",
    "paid_but_free = df_clean[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0)]\n",
    "print(f\"\\n‚ö†Ô∏è  Apps marcadas como 'Paid' pero con Price = 0: {len(paid_but_free)}\")\n",
    "if len(paid_but_free) > 0:\n",
    "    display(paid_but_free[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price = 0, cambiar Type a 'Free'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    print(f\"   ‚úÖ Corregido: Type cambiado a 'Free'\")\n",
    "\n",
    "# Inferir Type desde Price cuando Type es NaN\n",
    "type_missing = df_clean['Type'].isnull()\n",
    "if type_missing.sum() > 0:\n",
    "    print(f\"\\nüîç Type faltante en {type_missing.sum()} registros\")\n",
    "    print(\"   ‚úÖ Infiriendo Type desde Price...\")\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    remaining_missing = df_clean['Type'].isnull().sum()\n",
    "    print(f\"   ‚úÖ Type inferido. Faltantes restantes: {remaining_missing}\")\n",
    "\n",
    "print(\"\\n‚úÖ Validaci√≥n de consistencia completada\")\n",
    "print(f\"üìä Distribuci√≥n final de Type:\")\n",
    "print(df_clean['Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 3.3.4. Imputaci√≥n de Valores Faltantes\n",
    "\n",
    "Aplicamos estrategias diferenciadas por variable seg√∫n lo detectado en el EDA:\n",
    "\n",
    "**Estrategia por variable:**\n",
    "1. **Size** (~15.6% faltantes): imputar mediana por `Category √ó Type` + flag `size_missing`\n",
    "2. **Rating** (~13.6% faltantes): **NO imputar** (es el target); para modelado eliminaremos filas sin Rating\n",
    "3. **Content Rating** (<1%): imputar moda por `Category`\n",
    "4. **Android Ver** (~0.03%): imputar moda por `Category`\n",
    "5. **Current Ver** (~0.07%): imputar moda por `Category`\n",
    "6. **Price** (<1%): imputar 0 si Type='Free', mediana por Category si Type='Paid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: IMPUTACI√ìN DE VALORES FALTANTES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Resumen inicial de valores faltantes\n",
    "print(\"\\nüìä Valores faltantes ANTES de imputaci√≥n:\")\n",
    "missing_before = df_clean.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
    "for col, count in missing_before.items():\n",
    "    pct = count / len(df_clean) * 100\n",
    "    print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# 1. SIZE: Imputar mediana por Category √ó Type + crear flag\n",
    "print(\"\\n1Ô∏è‚É£  Imputando Size (mediana por Category √ó Type)...\")\n",
    "df_clean['size_missing'] = df_clean['Size'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    for app_type in df_clean['Type'].unique():\n",
    "        mask = (df_clean['Category'] == category) & (df_clean['Type'] == app_type) & df_clean['Size'].isnull()\n",
    "        if mask.sum() > 0:\n",
    "            # Calcular mediana del grupo\n",
    "            median_size = df_clean.loc[\n",
    "                (df_clean['Category'] == category) & (df_clean['Type'] == app_type), 'Size'\n",
    "            ].median()\n",
    "            \n",
    "            # Si no hay mediana en el grupo, usar mediana global\n",
    "            if pd.isna(median_size):\n",
    "                median_size = df_clean['Size'].median()\n",
    "            \n",
    "            df_clean.loc[mask, 'Size'] = median_size\n",
    "\n",
    "print(f\"   ‚úÖ Size imputado. Faltantes restantes: {df_clean['Size'].isnull().sum()}\")\n",
    "print(f\"   üìå Flag 'size_missing' creado ({df_clean['size_missing'].sum()} registros marcados)\")\n",
    "\n",
    "# 2. CONTENT RATING: Imputar moda por Category\n",
    "print(\"\\n2Ô∏è‚É£  Imputando Content Rating (moda por Category)...\")\n",
    "df_clean['content_rating_missing'] = df_clean['Content Rating'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Content Rating'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_rating = df_clean.loc[df_clean['Category'] == category, 'Content Rating'].mode()\n",
    "        if len(mode_rating) > 0:\n",
    "            df_clean.loc[mask, 'Content Rating'] = mode_rating.iloc[0]\n",
    "        else:\n",
    "            # Si no hay moda en el grupo, usar moda global\n",
    "            df_clean.loc[mask, 'Content Rating'] = df_clean['Content Rating'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ‚úÖ Content Rating imputado. Faltantes restantes: {df_clean['Content Rating'].isnull().sum()}\")\n",
    "\n",
    "# 3. ANDROID VER: Imputar moda por Category\n",
    "print(\"\\n3Ô∏è‚É£  Imputando Android Ver (moda por Category)...\")\n",
    "df_clean['android_ver_missing'] = df_clean['Android Ver'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Android Ver'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_ver = df_clean.loc[df_clean['Category'] == category, 'Android Ver'].mode()\n",
    "        if len(mode_ver) > 0:\n",
    "            df_clean.loc[mask, 'Android Ver'] = mode_ver.iloc[0]\n",
    "        else:\n",
    "            df_clean.loc[mask, 'Android Ver'] = df_clean['Android Ver'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ‚úÖ Android Ver imputado. Faltantes restantes: {df_clean['Android Ver'].isnull().sum()}\")\n",
    "\n",
    "# 4. CURRENT VER: Imputar moda por Category\n",
    "print(\"\\n4Ô∏è‚É£  Imputando Current Ver (moda por Category)...\")\n",
    "df_clean['current_ver_missing'] = df_clean['Current Ver'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Current Ver'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_ver = df_clean.loc[df_clean['Category'] == category, 'Current Ver'].mode()\n",
    "        if len(mode_ver) > 0:\n",
    "            df_clean.loc[mask, 'Current Ver'] = mode_ver.iloc[0]\n",
    "        else:\n",
    "            df_clean.loc[mask, 'Current Ver'] = df_clean['Current Ver'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ‚úÖ Current Ver imputado. Faltantes restantes: {df_clean['Current Ver'].isnull().sum()}\")\n",
    "\n",
    "# 5. PRICE: Imputar seg√∫n Type\n",
    "print(\"\\n5Ô∏è‚É£  Imputando Price (0 si Free, mediana por Category si Paid)...\")\n",
    "df_clean['price_missing'] = df_clean['Price'].isnull().astype(int)\n",
    "\n",
    "# Free apps ‚Üí Price = 0\n",
    "mask_free = (df_clean['Type'] == 'Free') & df_clean['Price'].isnull()\n",
    "df_clean.loc[mask_free, 'Price'] = 0\n",
    "\n",
    "# Paid apps ‚Üí mediana por Category\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & (df_clean['Type'] == 'Paid') & df_clean['Price'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        median_price = df_clean.loc[\n",
    "            (df_clean['Category'] == category) & (df_clean['Type'] == 'Paid'), 'Price'\n",
    "        ].median()\n",
    "        if pd.isna(median_price):\n",
    "            median_price = df_clean.loc[df_clean['Type'] == 'Paid', 'Price'].median()\n",
    "        df_clean.loc[mask, 'Price'] = median_price\n",
    "\n",
    "print(f\"   ‚úÖ Price imputado. Faltantes restantes: {df_clean['Price'].isnull().sum()}\")\n",
    "\n",
    "# 6. RATING: NO IMPUTAR (es el target)\n",
    "print(\"\\n6Ô∏è‚É£  Rating: NO se imputa (es la variable objetivo)\")\n",
    "print(f\"   üìå Se mantendr√°n {df_clean['Rating'].isnull().sum()} registros con Rating faltante\")\n",
    "print(f\"   üìå Estos registros se eliminar√°n en la fase de preparaci√≥n para modelado\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Valores faltantes DESPU√âS de imputaci√≥n:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
    "if len(missing_after) > 0:\n",
    "    for col, count in missing_after.items():\n",
    "        pct = count / len(df_clean) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No quedan valores faltantes (excepto Rating, que es el target)\")\n",
    "\n",
    "print(\"\\n‚úÖ Imputaci√≥n completada exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 3.3.5. Transformaciones de Variables Numericas\n",
    "\n",
    "Aplicamos transformaciones para estabilizar distribuciones sesgadas:\n",
    "\n",
    "1. **Log-transformaciones**: Para variables con colas largas (Reviews, Installs Numeric)\n",
    "2. **Binning**: Crear versiones categoricas de variables numericas para analisis\n",
    "3. **Variables binarias**: Crear indicadores utiles (is_free, is_large_app, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 5: TRANSFORMACIONES DE VARIABLES NUMERICAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. LOG-TRANSFORMACIONES para variables con colas largas\n",
    "print(\"\\n1Ô∏è‚É£  Aplicando transformaciones logaritmicas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Reviews: log1p (log(1+x) para manejar 0s)\n",
    "df_clean['Reviews_log'] = np.log1p(df_clean['Reviews'])\n",
    "print(f\"   ‚úÖ Reviews_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Reviews'].mean():.0f}, Mediana: {df_clean['Reviews'].median():.0f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Reviews_log'].mean():.2f}, Mediana: {df_clean['Reviews_log'].median():.2f}\")\n",
    "\n",
    "# Installs Numeric: log1p\n",
    "df_clean['Installs_log'] = np.log1p(df_clean['Installs Numeric'])\n",
    "print(f\"\\n   ‚úÖ Installs_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Installs Numeric'].mean():.0f}, Mediana: {df_clean['Installs Numeric'].median():.0f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Installs_log'].mean():.2f}, Mediana: {df_clean['Installs_log'].median():.2f}\")\n",
    "\n",
    "# Size: log1p (opcional, menos sesgado que Reviews/Installs)\n",
    "df_clean['Size_log'] = np.log1p(df_clean['Size'])\n",
    "print(f\"\\n   ‚úÖ Size_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Size'].mean():.2f}, Mediana: {df_clean['Size'].median():.2f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Size_log'].mean():.2f}, Mediana: {df_clean['Size_log'].median():.2f}\")\n",
    "\n",
    "# 2. BINNING de variables num√©ricas\n",
    "print(\"\\n\\n2Ô∏è‚É£  Creando bins para variables num√©ricas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Price bins\n",
    "df_clean['Price_bin'] = pd.cut(\n",
    "    df_clean['Price'], \n",
    "    bins=[-0.01, 0, 2.99, 9.99, 49.99, 500],\n",
    "    labels=['Free', 'Low ($0-3)', 'Mid ($3-10)', 'High ($10-50)', 'Premium ($50+)']\n",
    ")\n",
    "print(f\"   ‚úÖ Price_bin creado\")\n",
    "print(f\"      Distribuci√≥n:\\n{df_clean['Price_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# Size bins (en MB)\n",
    "df_clean['Size_bin'] = pd.cut(\n",
    "    df_clean['Size'],\n",
    "    bins=[0, 10, 25, 50, 100, 1000],\n",
    "    labels=['Small (<10MB)', 'Medium (10-25MB)', 'Large (25-50MB)', 'Very Large (50-100MB)', 'Huge (>100MB)']\n",
    ")\n",
    "print(f\"\\n   ‚úÖ Size_bin creado\")\n",
    "print(f\"      Distribuci√≥n:\\n{df_clean['Size_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# Installs bins (rangos m√°s interpretables)\n",
    "df_clean['Installs_bin'] = pd.cut(\n",
    "    df_clean['Installs Numeric'],\n",
    "    bins=[0, 100, 1000, 10000, 100000, 1000000, 10000000, 1e10],\n",
    "    labels=['<100', '100-1K', '1K-10K', '10K-100K', '100K-1M', '1M-10M', '>10M']\n",
    ")\n",
    "print(f\"\\n   ‚úÖ Installs_bin creado\")\n",
    "print(f\"      Distribuci√≥n:\\n{df_clean['Installs_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# 3. VARIABLES BINARIAS √∫tiles\n",
    "print(\"\\n\\n3Ô∏è‚É£  Creando variables binarias (indicadores)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# is_free\n",
    "df_clean['is_free'] = (df_clean['Type'] == 'Free').astype(int)\n",
    "print(f\"   ‚úÖ is_free creado: {df_clean['is_free'].sum()} apps gratuitas ({df_clean['is_free'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_large_app (>50MB)\n",
    "df_clean['is_large_app'] = (df_clean['Size'] > 50).astype(int)\n",
    "print(f\"   ‚úÖ is_large_app creado: {df_clean['is_large_app'].sum()} apps grandes ({df_clean['is_large_app'].mean()*100:.1f}%)\")\n",
    "\n",
    "# has_high_installs (>1M)\n",
    "df_clean['has_high_installs'] = (df_clean['Installs Numeric'] > 1000000).astype(int)\n",
    "print(f\"   ‚úÖ has_high_installs creado: {df_clean['has_high_installs'].sum()} apps populares ({df_clean['has_high_installs'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_top_category (pertenece a FAMILY o GAME)\n",
    "df_clean['is_top_category'] = df_clean['Category'].isin(['FAMILY', 'GAME']).astype(int)\n",
    "print(f\"   ‚úÖ is_top_category creado: {df_clean['is_top_category'].sum()} apps en categor√≠as principales ({df_clean['is_top_category'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_everyone_rated (Content Rating = Everyone)\n",
    "df_clean['is_everyone_rated'] = (df_clean['Content Rating'] == 'Everyone').astype(int)\n",
    "print(f\"   ‚úÖ is_everyone_rated creado: {df_clean['is_everyone_rated'].sum()} apps para todos ({df_clean['is_everyone_rated'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Transformaciones num√©ricas completadas exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 3.3.6. Creacion de Variables Derivadas (Feature Engineering Basico)\n",
    "\n",
    "Creamos nuevas variables combinando informacion existente para capturar patrones mas complejos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 6: FEATURE ENGINEERING (VARIABLES DERIVADAS)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Review Rate: Reviews por instalaci√≥n (engagement)\n",
    "print(\"\\n1Ô∏è‚É£  Calculando Review Rate (engagement)...\")\n",
    "df_clean['review_rate'] = df_clean['Reviews'] / (df_clean['Installs Numeric'] + 1)  # +1 para evitar divisi√≥n por 0\n",
    "print(f\"   ‚úÖ review_rate creado\")\n",
    "print(f\"      Media: {df_clean['review_rate'].mean():.6f}, Mediana: {df_clean['review_rate'].median():.6f}\")\n",
    "print(f\"      Interpretaci√≥n: proporci√≥n de usuarios que dejan rese√±a\")\n",
    "\n",
    "# 2. Genres Main: Extraer primer g√©nero de la lista de g√©neros\n",
    "print(\"\\n2Ô∏è‚É£  Extrayendo g√©nero principal...\")\n",
    "df_clean['Genres Main'] = df_clean['Genres'].str.split(';').str[0]\n",
    "print(f\"   ‚úÖ Genres Main creado\")\n",
    "print(f\"      G√©neros √∫nicos: {df_clean['Genres Main'].nunique()}\")\n",
    "print(f\"      Top 5:\\n{df_clean['Genres Main'].value_counts().head().to_string()}\")\n",
    "\n",
    "# 3. Days Since Update: D√≠as desde √∫ltima actualizaci√≥n (requiere parsear fecha)\n",
    "print(\"\\n3Ô∏è‚É£  Calculando d√≠as desde √∫ltima actualizaci√≥n...\")\n",
    "try:\n",
    "    df_clean['Last Updated Parsed'] = pd.to_datetime(df_clean['Last Updated'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2025-10-02')  # Fecha actual del proyecto\n",
    "    df_clean['days_since_update'] = (reference_date - df_clean['Last Updated Parsed']).dt.days\n",
    "    \n",
    "    print(f\"   ‚úÖ days_since_update creado\")\n",
    "    print(f\"      Media: {df_clean['days_since_update'].mean():.0f} d√≠as\")\n",
    "    print(f\"      Mediana: {df_clean['days_since_update'].median():.0f} d√≠as\")\n",
    "    print(f\"      Rango: {df_clean['days_since_update'].min():.0f} - {df_clean['days_since_update'].max():.0f} d√≠as\")\n",
    "    \n",
    "    # Crear bins de actualizaci√≥n\n",
    "    df_clean['update_recency'] = pd.cut(\n",
    "        df_clean['days_since_update'],\n",
    "        bins=[-1, 30, 90, 180, 365, 730, 10000],\n",
    "        labels=['<1 month', '1-3 months', '3-6 months', '6-12 months', '1-2 years', '>2 years']\n",
    "    )\n",
    "    print(f\"\\n   ‚úÖ update_recency creado\")\n",
    "    print(f\"      Distribuci√≥n:\\n{df_clean['update_recency'].value_counts().to_string()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error calculando days_since_update: {e}\")\n",
    "\n",
    "# 4. Size per Install: Tama√±o promedio por instalaci√≥n (eficiencia)\n",
    "print(\"\\n4Ô∏è‚É£  Calculando tama√±o por instalaci√≥n...\")\n",
    "df_clean['size_per_install'] = df_clean['Size'] / (df_clean['Installs Numeric'] + 1)\n",
    "print(f\"   ‚úÖ size_per_install creado\")\n",
    "print(f\"      Media: {df_clean['size_per_install'].mean():.8f}\")\n",
    "\n",
    "# 5. Price Category: Categorizaci√≥n m√°s simple de precio\n",
    "print(\"\\n5Ô∏è‚É£  Creando categor√≠a simple de precio...\")\n",
    "df_clean['price_category'] = 'Free'\n",
    "df_clean.loc[df_clean['Price'] > 0, 'price_category'] = 'Paid'\n",
    "print(f\"   ‚úÖ price_category creado\")\n",
    "print(f\"      Distribuci√≥n:\\n{df_clean['price_category'].value_counts().to_string()}\")\n",
    "\n",
    "# 6. Popularity Score: Score compuesto simple\n",
    "print(\"\\n6Ô∏è‚É£  Calculando popularity score...\")\n",
    "# Normalizar componentes a [0,1] usando min-max\n",
    "installs_norm = (df_clean['Installs Numeric'] - df_clean['Installs Numeric'].min()) / (df_clean['Installs Numeric'].max() - df_clean['Installs Numeric'].min())\n",
    "reviews_norm = (df_clean['Reviews'] - df_clean['Reviews'].min()) / (df_clean['Reviews'].max() - df_clean['Reviews'].min())\n",
    "\n",
    "df_clean['popularity_score'] = (installs_norm * 0.7 + reviews_norm * 0.3) * 100  # Escala 0-100\n",
    "print(f\"   ‚úÖ popularity_score creado (0-100)\")\n",
    "print(f\"      Media: {df_clean['popularity_score'].mean():.2f}\")\n",
    "print(f\"      Mediana: {df_clean['popularity_score'].median():.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature Engineering completado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 3.3.7. Resumen Final del Dataset Limpio\n",
    "\n",
    "Comparacion del dataset original vs limpio y resumen de todas las transformaciones aplicadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN FINAL: DATASET LIMPIO\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"üîπ\" * 40)\n",
    "print(\"COMPARACI√ìN: ORIGINAL vs LIMPIO\")\n",
    "print(\"üîπ\" * 40)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'M√©trica': [\n",
    "        'Total de registros',\n",
    "        'Total de columnas',\n",
    "        'Duplicados',\n",
    "        'Rating faltantes',\n",
    "        'Size faltantes',\n",
    "        'Price faltantes',\n",
    "        'Type faltantes',\n",
    "        'Ratings inv√°lidos (>5 o <1)',\n",
    "        'Memoria (MB)'\n",
    "    ],\n",
    "    'Original': [\n",
    "        f\"{len(applications_data):,}\",\n",
    "        f\"{len(applications_data.columns)}\",\n",
    "        f\"{applications_data.duplicated().sum():,}\",\n",
    "        f\"{applications_data['Rating'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Size'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Price'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Type'].isnull().sum():,}\",\n",
    "        f\"{((applications_data['Rating'] > 5) | (applications_data['Rating'] < 1)).sum():,}\",\n",
    "        f\"{applications_data.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ],\n",
    "    'Limpio': [\n",
    "        f\"{len(df_clean):,}\",\n",
    "        f\"{len(df_clean.columns)}\",\n",
    "        f\"{df_clean.duplicated().sum():,}\",\n",
    "        f\"{df_clean['Rating'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Size'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Price'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Type'].isnull().sum():,}\",\n",
    "        f\"{((df_clean['Rating'] > 5) | (df_clean['Rating'] < 1)).sum():,}\",\n",
    "        f\"{df_clean.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\n\" + \"üîπ\" * 40)\n",
    "print(\"NUEVAS VARIABLES CREADAS\")\n",
    "print(\"üîπ\" * 40)\n",
    "\n",
    "new_features = [\n",
    "    'Reviews_log', 'Installs_log', 'Size_log',\n",
    "    'Price_bin', 'Size_bin', 'Installs_bin',\n",
    "    'is_free', 'is_large_app', 'has_high_installs', 'is_top_category', 'is_everyone_rated',\n",
    "    'review_rate', 'Genres Main', 'days_since_update', 'update_recency',\n",
    "    'size_per_install', 'price_category', 'popularity_score',\n",
    "    'size_missing', 'content_rating_missing', 'android_ver_missing', 'current_ver_missing', 'price_missing'\n",
    "]\n",
    "\n",
    "available_features = [f for f in new_features if f in df_clean.columns]\n",
    "print(f\"\\nTotal de nuevas variables: {len(available_features)}\")\n",
    "print(\"\\nListado:\")\n",
    "for i, feat in enumerate(available_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"üîπ\" * 40)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS (Variables num√©ricas principales)\")\n",
    "print(\"üîπ\" * 40)\n",
    "\n",
    "key_numeric = ['Rating', 'Reviews', 'Reviews_log', 'Size', 'Price', 'Installs Numeric', \n",
    "               'Installs_log', 'popularity_score', 'review_rate']\n",
    "available_numeric = [col for col in key_numeric if col in df_clean.columns]\n",
    "\n",
    "display(df_clean[available_numeric].describe().round(3).T)\n",
    "\n",
    "print(\"\\n\" + \"üîπ\" * 40)\n",
    "print(\"DISTRIBUCI√ìN DE VARIABLES CATEG√ìRICAS CLAVE\")\n",
    "print(\"üîπ\" * 40)\n",
    "\n",
    "print(\"\\nüìä Category (Top 10):\")\n",
    "print(df_clean['Category'].value_counts().head(10))\n",
    "\n",
    "print(\"\\nüìä Type:\")\n",
    "print(df_clean['Type'].value_counts())\n",
    "\n",
    "print(\"\\nüìä Content Rating:\")\n",
    "print(df_clean['Content Rating'].value_counts())\n",
    "\n",
    "print(\"\\nüìä Price Category:\")\n",
    "print(df_clean['price_category'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TRANSFORMACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÅ Dataset limpio disponible en: df_clean\")\n",
    "print(f\"üìä Dimensiones finales: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "print(f\"üíæ Memoria utilizada: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nüéØ Siguiente paso: Preparar datos para modelado (eliminar filas sin Rating)\")\n",
    "print(f\"   ‚Üí df_model = df_clean.dropna(subset=['Rating'])\")\n",
    "print(f\"   ‚Üí Filas para modelado: {df_clean['Rating'].notna().sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 3.3.8. Visualizacion Comparativa: Antes vs Despues\n",
    "\n",
    "Visualizamos el impacto de las transformaciones aplicadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Reviews: Original vs Log\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(applications_data['Reviews'].dropna(), bins=50, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax1.set_xlabel('Reviews (escala original)')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.set_title('Reviews - Original (cola larga)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(df_clean['Reviews_log'].dropna(), bins=50, alpha=0.6, label='Log-transformado', color='green', edgecolor='black')\n",
    "ax2.set_xlabel('Reviews (log1p)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Reviews - Despu√©s de log1p (estabilizado)')\n",
    "ax2.legend()\n",
    "\n",
    "# 2. Valores faltantes: Original vs Limpio\n",
    "ax3 = axes[0, 2]\n",
    "missing_orig = applications_data.isnull().sum().sort_values(ascending=False).head(6)\n",
    "missing_clean = df_clean[missing_orig.index].isnull().sum()\n",
    "\n",
    "x = np.arange(len(missing_orig))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, missing_orig.values, width, label='Original', alpha=0.7, color='coral')\n",
    "ax3.bar(x + width/2, missing_clean.values, width, label='Limpio', alpha=0.7, color='lightgreen')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(missing_orig.index, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Valores Faltantes')\n",
    "ax3.set_title('Valores Faltantes: Original vs Limpio')\n",
    "ax3.legend()\n",
    "\n",
    "# 3. Rating distribution (solo v√°lidos)\n",
    "ax4 = axes[1, 0]\n",
    "valid_ratings_orig = applications_data['Rating'][(applications_data['Rating'] >= 1) & (applications_data['Rating'] <= 5)]\n",
    "valid_ratings_clean = df_clean['Rating'].dropna()\n",
    "ax4.hist(valid_ratings_orig, bins=30, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax4.hist(valid_ratings_clean, bins=30, alpha=0.6, label='Limpio', color='orange', edgecolor='black')\n",
    "ax4.set_xlabel('Rating')\n",
    "ax4.set_ylabel('Frecuencia')\n",
    "ax4.set_title('Distribuci√≥n de Rating (valores v√°lidos)')\n",
    "ax4.legend()\n",
    "\n",
    "# 4. Installs: Original vs Log\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(applications_data['Installs Numeric'].dropna(), bins=50, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax5.set_xlabel('Installs (escala original)')\n",
    "ax5.set_ylabel('Frecuencia')\n",
    "ax5.set_title('Installs - Original (muy sesgado)')\n",
    "ax5.legend()\n",
    "ax5.set_xlim(0, applications_data['Installs Numeric'].quantile(0.95))  # Truncar para visualizaci√≥n\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "ax6.hist(df_clean['Installs_log'].dropna(), bins=50, alpha=0.6, label='Log-transformado', color='purple', edgecolor='black')\n",
    "ax6.set_xlabel('Installs (log1p)')\n",
    "ax6.set_ylabel('Frecuencia')\n",
    "ax6.set_title('Installs - Despu√©s de log1p')\n",
    "ax6.legend()\n",
    "\n",
    "plt.suptitle('Impacto de las Transformaciones: Antes vs Despu√©s', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizaci√≥n comparativa completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playstore-kgOgXMXp-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
