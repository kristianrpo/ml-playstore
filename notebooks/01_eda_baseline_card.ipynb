{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuraci√≥n de entorno\n",
    "\n",
    "En esta secci√≥n validamos que nuestro entorno de trabajo est√© correctamente configurado antes de comenzar el an√°lisis.  \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. **Versi√≥n de Python**  \n",
    "   - Se verifica que est√© instalada la versi√≥n **3.11 o superior** (se recomienda 3.13).  \n",
    "   - Esto garantiza compatibilidad con librer√≠as modernas de an√°lisis de datos y machine learning.\n",
    "\n",
    "2. **Importaci√≥n de librer√≠as base**  \n",
    "   - Se cargan librer√≠as fundamentales:  \n",
    "     - `numpy`, `pandas`: manipulaci√≥n y an√°lisis de datos.  \n",
    "     - `matplotlib`, `seaborn`: visualizaci√≥n de datos.  \n",
    "     - `scipy`: funciones estad√≠sticas.  \n",
    "   - Adem√°s se configuran estilos gr√°ficos y opciones de visualizaci√≥n en pandas para trabajar con tablas m√°s grandes.\n",
    "\n",
    "3. **Verificaci√≥n de versiones cr√≠ticas**  \n",
    "   - Se comprueba que `scikit-learn` est√© instalado y en una versi√≥n **>= 1.0.1**.  \n",
    "   - Esto es esencial ya que `scikit-learn` se usar√° para el modelado (baseline y posteriores).\n",
    "\n",
    "Con esta configuraci√≥n inicial aseguramos que el entorno sea reproducible y que todas las dependencias necesarias est√©n listas antes de continuar con el **EDA** y el **baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert sys.version_info >= (3, 11), \"Este notebook trabajo con python 3.11 o superiores (recomendado 3.13)\"\n",
    "\n",
    "print(f\"Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librer√≠as cr√≠ticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Metodolog√≠a CRISP-DM\n",
    "## 2.1. Comprensi√≥n del Negocio\n",
    "El problema de Google Play Store  \n",
    "\n",
    "**Contexto:**  \n",
    "Es 2025. El mercado de aplicaciones m√≥viles es altamente competitivo: millones de apps conviven en Google Play Store.  \n",
    "Los desarrolladores buscan mejorar la visibilidad de sus aplicaciones y los usuarios dependen del **rating promedio** para decidir qu√© descargar.  \n",
    "\n",
    "**Problema actual:**  \n",
    "- El rating se conoce **solo despu√©s** de que los usuarios descargan y rese√±an.  \n",
    "- Las valoraciones son **altamente variables** y pueden depender de m√∫ltiples factores (categor√≠a, descargas, precio, tama√±o, tipo de app).  \n",
    "- Los desarrolladores carecen de una herramienta para **estimar la calificaci√≥n potencial** de una app antes o durante su lanzamiento.  \n",
    "- La competencia es muy alta: una diferencia de d√©cimas en rating puede significar miles de descargas menos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.1. Soluci√≥n propuesta  \n",
    "Construir un **sistema autom√°tico de predicci√≥n de rating** de apps a partir de sus caracter√≠sticas disponibles en el dataset de Google Play Store.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. Definiendo el √©xito  \n",
    "\n",
    "**M√©trica de negocio:**  \n",
    "- Ayudar a los desarrolladores a anticipar la valoraci√≥n probable de su app.  \n",
    "- Reducir la dependencia de pruebas de mercado costosas o lentas.  \n",
    "- Identificar caracter√≠sticas clave que favorecen una alta valoraci√≥n (‚â• 4.3).  \n",
    "\n",
    "**M√©trica t√©cnica:**  \n",
    "- Lograr un **Error Absoluto Medio (MAE) < 0.5 estrellas** en la predicci√≥n de rating.  \n",
    "- Para la versi√≥n de clasificaci√≥n (alta vs. baja calificaci√≥n): obtener un **F1-score > 0.70**.  \n",
    "\n",
    "**¬øPor qu√© estos valores?**  \n",
    "- El rating va de 1 a 5 ‚Üí un error de 0.5 equivale a 10% de la escala.  \n",
    "- Una diferencia de medio punto puede marcar la visibilidad de la app en el ranking.  \n",
    "- Tasadores humanos (usuarios) tambi√©n muestran variabilidad similar en sus calificaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Preguntas cr√≠ticas antes de empezar  \n",
    "\n",
    "1. **¬øRealmente necesitamos ML?**  \n",
    "   - Alternativa 1: Calcular el promedio de ratings por categor√≠a ‚Üí demasiado simple, no captura variabilidad.  \n",
    "   - Alternativa 2: Reglas heur√≠sticas (ej. ‚Äúsi es gratis y tiene muchas descargas, tendr√° rating alto‚Äù) ‚Üí insuficiente.  \n",
    "   - **Conclusi√≥n:** S√≠, ML es apropiado para capturar relaciones no lineales y m√∫ltiples factores.  \n",
    "\n",
    "2. **¬øQu√© pasa si el modelo falla?**  \n",
    "   - Transparencia: aclarar que es una estimaci√≥n autom√°tica.  \n",
    "   - Complementar con rangos de predicci√≥n (ej: intervalo de confianza).  \n",
    "   - Mantener como referencia comparativa, no como √∫nico criterio de √©xito.  \n",
    "\n",
    "3. **¬øC√≥mo mediremos el impacto?**  \n",
    "   - Capacidad de anticipar apps con alta probabilidad de √©xito.  \n",
    "   - Ahorro de tiempo en validaciones preliminares.  \n",
    "   - Insights para desarrolladores sobre qu√© factores influyen m√°s en el rating.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2.2. Comprensi√≥n de los Datos  \n",
    "\n",
    "El objetivo de esta fase es explorar y entender el dataset de Google Play Store antes de construir modelos **(an√°lisis exploratorio)**.  \n",
    "Nos centraremos en:  \n",
    "\n",
    "1. **Vista r√°pida del dataset**  \n",
    "   - Identificar dimensiones (filas √ó columnas).  \n",
    "   - Tipos de datos (num√©ricos, categ√≥ricos, texto, fechas).  \n",
    "   - Valores faltantes obvios y rangos sospechosos.\n",
    "\n",
    "2. **Descripci√≥n de variables**  \n",
    "   - Revisar cada columna y entender su significado.  \n",
    "   - Detectar qu√© variables podr√≠an ser √∫tiles como predictores y cu√°l ser√° la variable objetivo (rating).  \n",
    "\n",
    "3. **Detecci√≥n de problemas en los datos**  \n",
    "   - An√°lisis de valores faltantes.  \n",
    "   - Estrategias: eliminar filas/columnas, imputar valores o crear indicadores de ‚Äúdato faltante‚Äù.  \n",
    "\n",
    "4. **Estad√≠sticas descriptivas y univariadas**  \n",
    "   - Media vs mediana (sesgo de la distribuci√≥n).  \n",
    "   - Desviaci√≥n est√°ndar (variabilidad, posibles outliers).  \n",
    "   - M√≠nimos/m√°ximos sospechosos.  \n",
    "   - Histogramas para ver forma (normal, sesgada, bimodal, uniforme, picos extra√±os).  \n",
    "\n",
    "5. **An√°lisis de variables categ√≥ricas**  \n",
    "   - Distribuci√≥n de categor√≠as (ej. categor√≠as de apps, tipo de app, content rating).  \n",
    "   - Detecci√≥n de clases dominantes o categor√≠as poco representadas.  \n",
    "\n",
    "6. **Correlaciones y relaciones entre variables**  \n",
    "   - Matriz de correlaci√≥n de Pearson para variables num√©ricas.  \n",
    "   - Identificar relaciones fuertes, moderadas o d√©biles.  \n",
    "   - Importante: recordar que **correlaci√≥n ‚â† causalidad**.  \n",
    "7. ** An√°lisis de outliers **\n",
    "   -  Tipos e identificaci√≥n de outliers a trav√©s de diferentes m√©todos.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**  \n",
    "No siempre es necesario aplicar todos los pasos con igual profundidad.  \n",
    "- Para este proyecto, el foco est√° en **identificar variables relevantes para predecir el rating** y **limpiar datos inconsistentes**.  \n",
    "- Otros an√°lisis m√°s complejos (ej. NLP sobre descripciones) se pueden dejar como trabajo futuro (seg√∫n trabajos de referencia investigados).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2.1 Descarga de datos  \n",
    "\n",
    "En este paso descargamos el dataset de Google Play Store desde Kaggle y lo organizamos en la estructura de carpetas del proyecto.  \n",
    "\n",
    "1. Usamos la librer√≠a `kagglehub` para acceder al dataset p√∫blico **`lava18/google-play-store-apps`** directamente desde Kaggle.  \n",
    "2. Se define una ruta clara dentro del proyecto para almacenar los datos originales: `../data/original/google-play-store/`. Esto ayuda a mantener la reproducibilidad y una estructura organizada.  \n",
    "3. Con la funci√≥n `shutil.copytree` copiamos los archivos descargados a la carpeta destino. De esta forma, el dataset queda disponible en nuestro directorio de trabajo para su an√°lisis posterior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "def download_data(origin_repository, target_folder):\n",
    "    # Descargar dataset\n",
    "    path = kagglehub.dataset_download(origin_repository)\n",
    "    \n",
    "    # Copiar los archivos descargados\n",
    "    shutil.copytree(path, target_folder, dirs_exist_ok=True)\n",
    "    \n",
    "\n",
    "download_data(\"lava18/google-play-store-apps\", \"../data/original/google-play-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2.2 Carga de datos  \n",
    "\n",
    "En este paso realizamos la **lectura del archivo CSV** que contiene el dataset descargado previamente.  \n",
    "\n",
    "- Definimos una funci√≥n `load_data(path, file)` que recibe la ruta y el nombre del archivo, y lo carga con `pandas.read_csv()`.  \n",
    "- Cargamos el dataset principal en la variable `applications_data` desde la carpeta `../data/original/google-play-store/`.  \n",
    "- Incluimos una verificaci√≥n simple:  \n",
    "  - Si el dataset se carga con √©xito, se imprime `\"Dataset loaded\"`.  \n",
    "  - En caso contrario, se muestra un mensaje de error.  \n",
    "\n",
    "Con esta validaci√≥n aseguramos que el archivo est√© disponible y correctamente le√≠do antes de continuar con el an√°lisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path, file):\n",
    "    return pd.read_csv(f\"{path}/{file}\")\n",
    "\n",
    "def convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte a num√©ricas solo las columnas que deber√≠an serlo, sin tocar 'Size'.\n",
    "    Usa to_numeric(errors='coerce') para evitar ValueError si aparece texto.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rating\n",
    "    if \"Rating\" in df.columns:\n",
    "        df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Reviews: quitar comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Reviews\" in df.columns:\n",
    "        df[\"Reviews\"] = (\n",
    "            df[\"Reviews\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Installs: quitar +, comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Installs\" in df.columns:\n",
    "        df[\"Installs Numeric\"] = (\n",
    "            df[\"Installs\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Price: quitar $ y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Price\" in df.columns:\n",
    "        df[\"Price\"] = (\n",
    "            df[\"Price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "\n",
    "    if \"Size\" in df.columns:\n",
    "        def parse_size(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip()\n",
    "                if x.endswith(\"M\"):\n",
    "                    return float(x[:-1])\n",
    "                elif x.endswith(\"k\") or x.endswith(\"K\"):\n",
    "                    return float(x[:-1]) / 1024  # KB -> MB\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        df[\"Size\"] = df[\"Size\"].apply(parse_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "temp_applications_data = load_data(\"../data/original/google-play-store\", \"googleplaystore.csv\")\n",
    "applications_data = convert_numeric_columns(temp_applications_data)\n",
    "\n",
    "\n",
    "if len(applications_data):\n",
    "    print(\"Dataset cargado\")\n",
    "else:\n",
    "    print(\"Error cargando dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2.3 Vista r√°pida del dataset\n",
    "\n",
    "**Dimensiones y columnas**\n",
    "- Registros: **10,841** filas.\n",
    "- Columnas: actualmente **14**; **originalmente eran 13** y se **a√±adi√≥** una columna derivada: **`Installs Numeric`** para an√°lisis con describe.\n",
    "- Memoria aproximada: **~1.2 MB**.\n",
    "\n",
    "**Tipos de datos (y transformaciones realizadas)**\n",
    "- Num√©ricas (`float64`): `Rating`, `Reviews`, `Size`, `Price`, **`Installs Numeric`**.\n",
    "- Categ√≥ricas / texto (`object`): `App`, `Category`, `Installs` *(forma original con ‚Äú1,000+‚Äù)*, `Type`, `Content Rating`, `Genres`, `Last Updated`, `Current Ver`, `Android Ver`.\n",
    "- Transformaciones ya aplicadas:\n",
    "  - **`Installs`** se **conserv√≥** en su formato original (categ√≥rico con ‚Äú+‚Äù y comas) **y** se cre√≥ **`Installs Numeric`** mapeando esos rangos a n√∫meros (0 ‚Ä¶ 1,000,000,000).\n",
    "  - **`Price`**, **`Reviews`** y **`Size`** fueron normalizadas/parseadas a **num√©rico** para an√°lisis y modelado.\n",
    "\n",
    "**Valores faltantes (no-null count ‚Üí faltantes aprox.)**\n",
    "- `Rating`: 9,367 ‚Üí **1,474 faltantes (~13.6%)**.\n",
    "- `Size`: 9,145 ‚Üí **1,696 faltantes (~15.6%)**.\n",
    "- `Current Ver`: 10,833 ‚Üí **8 faltantes (~0.07%)**.\n",
    "- `Android Ver`: 10,838 ‚Üí **3 faltantes (~0.03%)**.\n",
    "- `Content Rating`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Price`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Installs Numeric`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- Resto de columnas: **sin faltantes**.\n",
    "\n",
    "**Duplicados:**\n",
    "-   Se identificaron **483 filas duplicadas** (‚âà **4.46%** del\n",
    "    dataset).\\\n",
    "-   Ejemplos de duplicados incluyen apps como:\n",
    "    -   *Quick PDF Scanner + OCR FREE*\\\n",
    "    -   *Box*\\\n",
    "    -   *Google My Business*\\\n",
    "    -   *ZOOM Cloud Meetings*\\\n",
    "    -   *join.me -- Simple Meetings*\\\n",
    "\n",
    "**Rangos y valores sospechosos (seg√∫n `describe()`)**\n",
    "- `Rating`: **min = 1.0**, **max = 19.0** ‚Üí **19** es inv√°lido para la escala 1‚Äì5 (error de dato a corregir).\n",
    "- `Reviews`: media ~ **444k**, **p75 ‚âà 54,768**, **max ‚âà 78M** ‚Üí valores altos plausibles; tratar como **outliers**.\n",
    "- `Size` (MB): media ~ **21.5**, **p50 = 13**, **p75 = 30**, **max = 100** ‚Üí distribuci√≥n sesgada a la derecha; m√≠nimos muy bajos (**0.01**) a revisar.\n",
    "- `Price` (USD): **mediana = 0** y **p75 = 0** ‚Üí la mayor√≠a son **apps gratuitas**; **max = 400** sugiere outliers de precio.\n",
    "- `Installs Numeric`: **p25 = 1,000**, **p50 = 100,000**, **p75 = 5,000,000**, **max = 1,000,000,000** ‚Üí escala muy amplia; conviene usar **transformaciones log** o **binning** en el EDA/modelado.\n",
    "\n",
    "**Conclusi√≥n inicial**\n",
    "- Los **faltantes** m√°s relevantes est√°n en `Rating` y `Size`; habr√° que decidir estateg√≠a para aumentar, imputar o nivelar los datos.\n",
    "- Existen **outliers (no leg√≠timos)** (ej. `Rating = 19`) y variables con **colas largas** (ej. `Reviews`, `Installs Numeric`, `Price`).\n",
    "- Eliminar **duplicados** para evitar sesgos de an√°lisis y que no introduzcan ruidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(applications_data.head().style.background_gradient(cmap='RdYlGn', subset=['Rating']))\n",
    "\n",
    "# Informaci√≥n detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "applications_data.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(applications_data.describe().round(2).T)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar duplicados\n",
    "num_duplicados = applications_data.duplicated().sum()\n",
    "print(f\"Total de registros duplicados: {num_duplicados}\")\n",
    "\n",
    "# Mostrar ejemplos de duplicados si existen\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas:\\n\")\n",
    "    display(applications_data[applications_data.duplicated()].head())\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.4 Descripci√≥n de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price',\n",
    "    'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver',\n",
    "    'Installs Numeric'\n",
    "]\n",
    "\n",
    "tipos = [\n",
    "    'Categ√≥rica',          # App\n",
    "    'Categ√≥rica',          # Category\n",
    "    'Num√©rica (Target)',   # Rating\n",
    "    'Num√©rica',            # Reviews\n",
    "    'Num√©rica (MB)',       # Size\n",
    "    'Categ√≥rica (rango)',  # Installs\n",
    "    'Categ√≥rica',          # Type\n",
    "    'Num√©rica (USD)',      # Price\n",
    "    'Categ√≥rica',          # Content Rating\n",
    "    'Categ√≥rica',          # Genres\n",
    "    'Texto (fecha)',       # Last Updated (parseable a fecha)\n",
    "    'Texto',               # Current Ver\n",
    "    'Texto',               # Android Ver\n",
    "    'Num√©rica',            # Installs Numeric\n",
    "]\n",
    "\n",
    "descripciones = [\n",
    "    'Nombre de la aplicaci√≥n.',\n",
    "    'Categor√≠a oficial de la app en Google Play.',\n",
    "    'Calificaci√≥n promedio de usuarios (1 a 5).',\n",
    "    'N√∫mero de rese√±as reportadas.',\n",
    "    'Tama√±o aproximado de la app en MB.',\n",
    "    'Instalaciones en rango (p.ej., \"1,000+\").',\n",
    "    'Tipo de app (Free / Paid).',\n",
    "    'Precio en USD (0 para gratuitas).',\n",
    "    'Clasificaci√≥n de contenido (Everyone, Teen, etc.).',\n",
    "    'G√©nero(s) de la app.',\n",
    "    'Fecha de √∫ltima actualizaci√≥n (texto en origen).',\n",
    "    'Versi√≥n actual declarada por el desarrollador.',\n",
    "    'Versi√≥n m√≠nima de Android requerida.',\n",
    "    'Instalaciones convertidas a n√∫mero para an√°lisis.'\n",
    "]\n",
    "\n",
    "valores_faltantes = [applications_data[col].isnull().sum() if col in applications_data.columns else None for col in variables]\n",
    "\n",
    "metadata = {\n",
    "    'Variable': variables,\n",
    "    'Tipo': tipos,\n",
    "    'Descripci√≥n': descripciones,\n",
    "    'Valores Faltantes': valores_faltantes\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# Mostrar con resaltado de faltantes\n",
    "styled = df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x > 0 else '',\n",
    "    subset=['Valores Faltantes']\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Resumen de dtypes originales (informativo)\n",
    "dtypes_resumen = applications_data[variables].dtypes.astype(str).reset_index()\n",
    "dtypes_resumen.columns = ['Variable', 'dtype pandas']\n",
    "display(dtypes_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.2.5 Detecci√≥n de problemas en los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Resumen de hallazgos (valores faltantes):**\n",
    "- `Size` ‚âà 15.6% y `Rating` ‚âà 13.6% concentran la mayor√≠a de los faltantes.\n",
    "- Faltantes puntuales (‚âà0.01%): `Type`, `Price`, `Content Rating`, `Installs Numeric` ocurren en la misma(s) fila(s) ‚Üí patr√≥n conjunto.\n",
    "- `Android Ver` (0.03%) y `Current Ver` (0.07%) con faltantes residuales, parcialmente correlacionados con el grupo anterior.\n",
    "\n",
    "**Heatmap de correlaci√≥n de patrones de faltantes (interpretaci√≥n):**\n",
    "- Correlaci√≥n 1.00 entre `Type`, `Price`, `Content Rating`, `Installs Numeric`: las ausencias co-ocurren en el/los mismos registros. Acciones coordinadas.\n",
    "- `Android Ver` muestra correlaci√≥n moderada (~0.58) con ese grupo: algunas veces falta junto con ellos.\n",
    "- `Size`, `Rating`, `Current Ver` tienen patrones de faltantes independientes del grupo anterior (correlaciones cercanas a 0), lo que sugiere causas distintas.\n",
    "\n",
    "#### Posibles estrategias de correcci√≥n\n",
    "\n",
    "- Limpieza b√°sica\n",
    "  - Eliminar duplicados (483 filas) para evitar sesgos.\n",
    "  - Validar y corregir outliers imposibles, p. ej., `Rating = 19` ‚Üí convertir a NaN para tratarlo como faltante.\n",
    "\n",
    "- Imputaci√≥n (conservadora y por grupos)\n",
    "  - `Rating` (target): para modelado, eliminar filas sin `Rating`; para EDA descriptivo, imputar mediana por `Category` solo para visualizaci√≥n.\n",
    "  - `Size`: imputar mediana por `Category √ó Type` y crear indicador `size_missing`.\n",
    "  - `Android Ver`, `Current Ver`: imputar moda por `Category` y crear indicadores `androidver_missing`, `currentver_missing`.\n",
    "  - Faltantes conjuntos (`Type`, `Price`, `Content Rating`, `Installs Numeric`):\n",
    "    - Si es 1 fila: eliminarla es lo m√°s simple y seguro.\n",
    "    - Alternativa (si se prefiere imputar):\n",
    "      - `Type`: inferir desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid).\n",
    "      - `Price`: 0 si `Type == Free`, si `Paid` usar mediana por `Category`.\n",
    "      - `Content Rating`: moda por `Category`.\n",
    "      - `Installs Numeric`: mediana por `Category √ó Type` o por bin de `Installs`.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategias de ‚Äúnivelaci√≥n‚Äù seg√∫n los porcentajes observados\n",
    "\n",
    "- Size (~15.6% faltantes, >5% y <<60%)\n",
    "  - Acci√≥n: imputar mediana por grupo `Category √ó Type`.\n",
    "  - A√±adir flag: `size_missing = 1` cuando falte (conserva se√±al de ausencia).\n",
    "  - Justificaci√≥n: volumen relevante; la mediana por grupos respeta diferencias entre tipos/categor√≠as.\n",
    "\n",
    "- Rating (~13.6% faltantes, >5% y <<60%) [variable objetivo]\n",
    "  - Para modelado: eliminar filas sin `Rating` (evita sesgo por imputaci√≥n del target).\n",
    "  - Para EDA descriptivo: si se requiere visualizar completos, imputar mediana por `Category` solo para gr√°ficos/tablas (no para entrenamiento).\n",
    "  - Justificaci√≥n: imputar el target puede distorsionar m√©tricas.\n",
    "\n",
    "- Current Ver (0.07%) y Android Ver (0.03%) (<5%)\n",
    "  - Acci√≥n: imputar con la moda por `Category`. Flags opcionales `currentver_missing` y `androidver_missing`.\n",
    "  - Justificaci√≥n: impacto √≠nfimo; moda es suficiente y estable.\n",
    "\n",
    "- Faltantes ‚Äúen bloque‚Äù en la misma fila: Type, Price, Content Rating, Installs Numeric (‚âà0.01% cada uno; correlaci√≥n 1.00)\n",
    "  - Si es 1 fila: eliminarla directamente.\n",
    "  - Si hubiera m√°s en el futuro y se prefiriera imputar coordinadamente:\n",
    "    - `Type` desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid),\n",
    "    - `Price` = 0 si `Free`, si `Paid` usar mediana por `Category`,\n",
    "    - `Content Rating` = moda por `Category`,\n",
    "    - `Installs Numeric` = mediana por `Category √ó Type`.\n",
    "  - Justificaci√≥n: co-ocurren; eliminar 1 fila no afecta el conjunto y evita inconsistencias.\n",
    "\n",
    "- Transformaciones para estabilizar distribuciones (complementarias a la imputaci√≥n)\n",
    "  - `Reviews` y `Installs Numeric`: aplicar `log1p` para an√°lisis y futuros modelos. *****************************\n",
    "  - `Installs` (rangos): tratar como ordinal/bins en el EDA.\n",
    "\n",
    "- Limpieza previa necesaria\n",
    "  - Eliminar duplicados (483 filas).\n",
    "  - Corregir valores imposibles detectados en el EDA (ej. `Rating = 19` ‚Üí NaN) y re-entrar al flujo de imputaci√≥n/nivelaci√≥n anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"An√°lisis completo de valores faltantes con visualizaciones.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': missing_counts.values,\n",
    "        'Porcentaje': missing_pct.values,\n",
    "        'Tipo_Dato': df.dtypes.values\n",
    "    })\n",
    "\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "    if len(missing_df) == 0:\n",
    "        print(\"No hay valores faltantes en el dataset\")\n",
    "        return missing_df\n",
    "\n",
    "    # Visualizaci√≥n: barras y correlaci√≥n de patrones de faltantes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Gr√°fico de barras de % faltantes\n",
    "    ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "    ax1.set_xlabel('Columna')\n",
    "    ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "    ax1.set_title('Valores Faltantes por Columna')\n",
    "    ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "    ax1.axhline(y=60, color='purple', linestyle='--', label='Umbral 60%')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Heatmap de correlaci√≥n de patrones de faltantes\n",
    "    mask_df = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "    if mask_df.shape[1] >= 2:\n",
    "        corr = mask_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "        ax2.set_title('Correlaci√≥n de Patrones de Valores Faltantes')\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Correlaci√≥n de faltantes (no aplica: 1 columna)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "missing_analysis = analyze_missing_values(applications_data)\n",
    "if missing_analysis is not None and not missing_analysis.empty:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2.6 Estadisticas descriptivas y univariadas (n√∫merico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A partir de la tabla de estad√≠sticas y los gr√°ficos generados para `Rating`, `Reviews`, `Size`, `Price` e `Installs Numeric`, se observan los siguientes puntos clave.\n",
    "\n",
    "- Rating\n",
    "  - Media ‚âà 4.19 y mediana ‚âà 4.30 ‚Üí ligera cola a la izquierda (m√°s apps con rating alto). Hay un valor imposible (‚âà19), confirmado en el boxplot/Q-Q como outlier extremo.\n",
    "  - Outliers: ~5% por IQR, dominados por el valor inv√°lido y algunos ratings bajos.\n",
    "  - Q-Q plot: desviaci√≥n frente a normalidad, esperable para una variable acotada [1,5].\n",
    "  - Implicaci√≥n/acci√≥n: eliminar filas sin `Rating` para modelado; corregir `Rating=19 ‚Üí NaN` y excluir; no aplicar transformaciones (la escala es ya interpretables).\n",
    "\n",
    "- Reviews\n",
    "  - Media ‚â´ mediana (pico en 0‚Äìpocos miles; m√°ximo ‚âà 78M) ‚Üí cola muy larga a la derecha.\n",
    "  - Boxplot: ~18% outliers por IQR (muchas apps con rese√±as muy altas).\n",
    "  - Q-Q plot: gran desviaci√≥n de normalidad (heavy tail).\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva muy d√©bil (~0.07), tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Reviews)` para estabilizar la distribuci√≥n en an√°lisis/modelado; considerar winsorizar p99.9 para vistas tabulares si se desea.\n",
    "\n",
    "- Size (MB)\n",
    "  - Media > mediana (‚âà 21.5 vs 13) ‚Üí sesgo a la derecha; valores hasta 100 MB.\n",
    "  - ~6% outliers por IQR, especialmente en colas altas.\n",
    "  - Q-Q plot: curvatura en colas; no normal.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva d√©bil (~0.08); se√±al muy tenue.\n",
    "  - Implicaci√≥n/acci√≥n: imputar faltantes por `Category √ó Type` y a√±adir `size_missing`; opcionalmente probar `log1p(Size)` o binning para robustecer.\n",
    "\n",
    "- Price (USD)\n",
    "  - Mediana = 0 (mayor√≠a gratis) y cola a la derecha con m√°ximos altos (‚âà 400).\n",
    "  - ~7% outliers por IQR; Q-Q muestra heavy tail.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n negativa muy d√©bil (~-0.02).\n",
    "  - Implicaci√≥n/acci√≥n: crear `is_free = (Price == 0)` y, si se usa `Price` continuo, considerar `log1p(Price)` para las pocas apps pagas; validar coherencia `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Installs Numeric\n",
    "  - Media ‚â´ mediana (100k) con m√°ximo 1e9 ‚Üí distribuci√≥n extremadamente sesgada a la derecha.\n",
    "  - ~7‚Äì8% outliers por IQR; Q-Q muy alejado de normalidad.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n d√©bil positiva (~0.05) y tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Installs Numeric)` o bins ordinales para an√°lisis; verificar coherencia con `Installs` textual.\n",
    "\n",
    "Recomendaciones transversales\n",
    "- Eliminar duplicados antes de resumir para evitar sesgos.\n",
    "- Tratar outliers evidentes no-leg√≠timos (p. ej. `Rating=19`). Para colas largas leg√≠timas (`Reviews`, `Installs Numeric`, `Price`): preferir `log1p` o winsorizaci√≥n solo para visualizaciones.\n",
    "- Mantener consistencia: `Type=Free ‚áí Price=0`; `Installs Numeric` coherente con el rango de `Installs`.\n",
    "- Para relaciones con `Rating`, las correlaciones lineales observadas son d√©biles; la se√±al puede emerger mejor con interacciones (p. ej., `is_free √ó installs_bin`) o modelos no lineales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selecci√≥n de columnas num√©ricas relevantes\n",
    "numeric_cols = [c for c in ['Rating', 'Reviews', 'Size', 'Price', 'Installs Numeric'] if c in applications_data.columns]\n",
    "\n",
    "# Tabla de estad√≠sticas b√°sicas (media, mediana, std, min, p25, p50, p75, max)\n",
    "describe_tbl = applications_data[numeric_cols].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "# M√©tricas adicionales robustas\n",
    "extra = pd.DataFrame(index=numeric_cols)\n",
    "extra['mad'] = [stats.median_abs_deviation(applications_data[c].dropna()) for c in numeric_cols]\n",
    "extra['skew'] = [applications_data[c].skew(skipna=True) for c in numeric_cols]\n",
    "extra['kurtosis'] = [applications_data[c].kurtosis(skipna=True) for c in numeric_cols]\n",
    "extra['cv'] = [applications_data[c].std(skipna=True) / applications_data[c].mean(skipna=True) if applications_data[c].mean(skipna=True) not in [0, np.nan] else np.nan for c in numeric_cols]\n",
    "\n",
    "stats_table = describe_tbl.join(extra)\n",
    "display(stats_table.round(3))\n",
    "\n",
    "\n",
    "def univariate_analysis(df: pd.DataFrame, column: str, target: str | None = None):\n",
    "    \"\"\"An√°lisis univariado con histograma, boxplot, Q-Q plot y relaci√≥n con target.\"\"\"\n",
    "    series = df[column].dropna()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Histograma con l√≠neas de media y mediana\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(series, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(series.mean(), color='red', linestyle='--', label=f\"Media: {series.mean():.2f}\")\n",
    "    ax1.axvline(series.median(), color='green', linestyle='--', label=f\"Mediana: {series.median():.2f}\")\n",
    "    ax1.set_title(f\"Distribuci√≥n de {column}\")\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2) Boxplot + conteo de outliers (IQR)\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(series, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)\n",
    "    n_out = int(outliers_mask.sum())\n",
    "    pct_out = 100 * n_out / len(series) if len(series) else 0\n",
    "    ax2.set_title(f\"Boxplot de {column}\")\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.text(1.1, Q3, f\"Outliers: {n_out} ({pct_out:.1f}%)\", fontsize=10)\n",
    "\n",
    "    # 3) Q-Q plot normal\n",
    "    ax3 = axes[1, 0]\n",
    "    stats.probplot(series, dist='norm', plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # 4) Relaci√≥n con target si aplica\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns and column != target:\n",
    "        valid = df[[column, target]].dropna()\n",
    "        ax4.scatter(valid[column], valid[target], alpha=0.4, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f\"{column} vs {target}\")\n",
    "        # L√≠nea de tendencia (ajuste lineal simple)\n",
    "        if len(valid) > 1:\n",
    "            z = np.polyfit(valid[column], valid[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            xs = np.linspace(valid[column].min(), valid[column].max(), 200)\n",
    "            ax4.plot(xs, p(xs), 'r--', alpha=0.8, label='Tendencia')\n",
    "            corr = valid[column].corr(valid[target])\n",
    "            ax4.text(0.05, 0.95, f\"Correlaci√≥n: {corr:.3f}\", transform=ax4.transAxes,\n",
    "                     fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            ax4.legend()\n",
    "    else:\n",
    "        ax4.axis('off')\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"An√°lisis Univariado: {column}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar an√°lisis univariado para cada m√©trica num√©rica, relacionando con Rating\n",
    "for col in numeric_cols:\n",
    "    univariate_analysis(applications_data, col, target='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.2.7. An√°lisis Univariado Categ√≥rico\n",
    "- Category\n",
    "  - Distribuci√≥n: alta concentraci√≥n en `FAMILY` (~19%) y `GAME` (~12%). El resto de categor√≠as tienen menor peso individual; el grupo `Others` acumula ~31% del total.\n",
    "  - Rating por categor√≠a: diferencias moderadas; la **mediana** suele estar entre 4.2‚Äì4.4. Algunas categor√≠as muestran desviaci√≥n est√°ndar mayor (p. ej., `PRODUCTIVITY`, `LIFESTYLE`), indicando m√°s variabilidad de valoraci√≥n.\n",
    "  - Implicaciones: riesgo de sesgo por categor√≠as mayoritarias en an√°lisis agregados. Para modelado, conviene usar dummies Top-K o codificaci√≥n ordinal/target encoding con cuidado (evitar fuga). Agrupar colas largas en `Others` es adecuado para visualizaci√≥n.\n",
    "\n",
    "- Content Rating\n",
    "  - Distribuci√≥n: `Everyone` domina (~79%), seguido por `Teen` (~12%); `Mature 17+` y `Everyone 10+` suman ~9% en conjunto; clases raras casi nulas.\n",
    "  - Rating por nivel de contenido: medias similares (‚âà4.1‚Äì4.3). `Teen` tiende a mediana 4.3 y variabilidad algo menor; `Mature 17+` muestra algo m√°s de dispersi√≥n.\n",
    "  - Implicaciones: por el fuerte desbalance, esta variable aporta se√±al limitada por s√≠ sola. √ötil como interacci√≥n con `Category`/`Genres`.\n",
    "\n",
    "- Type\n",
    "  - Distribuci√≥n: `Free` ‚âà 93%, `Paid` ‚âà 7% (clase muy desbalanceada); existe un registro an√≥malo (valor 0) en los gr√°ficos que debe eliminarse/corregirse.\n",
    "  - Rating por tipo: medias muy cercanas (Free ‚âà 4.19, Paid ‚âà 4.27). La diferencia es peque√±a y probablemente no significativa sin controlar otras variables (p. ej., `Category`).\n",
    "  - Implicaciones: por el desbalance extremo, conviene usar `is_free` como binaria y, si se modela interacci√≥n con `Installs` o `Price`, puede emerger se√±al. Validar regla `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Genres Main (primer g√©nero)\n",
    "  - Distribuci√≥n: gran cola larga; `Others` concentra ~48%. Entre Top-12, `Tools`, `Entertainment` y `Education` destacan en frecuencia.\n",
    "  - Rating por g√©nero: diferencias peque√±as (medianas ~4.2‚Äì4.4), con algunas variaciones en dispersi√≥n (p. ej., `Medical` y `Lifestyle` m√°s variables).\n",
    "  - Implicaciones: por la alta cardinalidad y colas largas, mantener Top-K + `Others` en EDA ayuda a la legibilidad. Para modelado, preferir codificaci√≥n que reduzca dimensionalidad (Top-K dummies, hashing, o target encoding con validaci√≥n adecuada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_compact(df: pd.DataFrame, cat_col: str, target_col: str, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Versi√≥n compacta para variables con muchas categor√≠as:\n",
    "    - Ordena por frecuencia, muestra top_n y agrupa el resto en \"Others\".\n",
    "    - Barras horizontales, pie chart compacto, boxplot y tabla para top_n.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, target_col]].dropna(subset=[cat_col, target_col]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"Sin datos para {cat_col} y {target_col}\")\n",
    "        return\n",
    "\n",
    "    counts = data[cat_col].value_counts()\n",
    "    top_cats = counts.head(top_n)\n",
    "    others_count = counts.iloc[top_n:].sum()\n",
    "\n",
    "    # Mapeo a top_n + Others\n",
    "    mapping = {c: c for c in top_cats.index}\n",
    "    data['__cat__'] = data[cat_col].where(data[cat_col].isin(top_cats.index), other='Others')\n",
    "\n",
    "    # Recalcular conteos con Others\n",
    "    counts_compact = data['__cat__'].value_counts()\n",
    "    order = list(top_cats.index) + (['Others'] if 'Others' in counts_compact.index else [])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Barras horizontales (mejor legibilidad)\n",
    "    ax1 = axes[0, 0]\n",
    "    vals = counts_compact.loc[order]\n",
    "    ax1.barh(range(len(vals)), vals.values, color=plt.cm.Set3(range(len(vals))))\n",
    "    ax1.set_yticks(range(len(vals)))\n",
    "    ax1.set_yticklabels(order)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title(f'Distribuci√≥n (Top {top_n}) de {cat_col}')\n",
    "    ax1.set_xlabel('Frecuencia')\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax1.text(v, i, f'  {v} ({v/len(data)*100:.1f}%)', va='center')\n",
    "\n",
    "    # 2) Pie chart compacto\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(vals.values, labels=order, autopct='%1.1f%%', startangle=140,\n",
    "            colors=plt.cm.Set3(range(len(vals))))\n",
    "    ax2.set_title(f'Proporci√≥n (Top {top_n} + Others) de {cat_col}')\n",
    "\n",
    "    # 3) Boxplot del target por categor√≠a (solo top_n)\n",
    "    ax3 = axes[1, 0]\n",
    "    top_mask = data['__cat__'] != 'Others'\n",
    "    data_top = data[top_mask]\n",
    "    data_top.boxplot(column=target_col, by='__cat__', ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col} (Top {top_n})')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # 4) Tabla de estad√≠sticas por categor√≠a (solo top_n y Others si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    stats_by_cat = data.groupby('__cat__')[target_col].agg(['count', 'mean', 'median', 'std']).loc[order].round(2)\n",
    "    table = ax4.table(cellText=stats_by_cat.reset_index().values,\n",
    "                      colLabels=['Categor√≠a', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                      cellLoc='center', loc='center', colWidths=[0.35, 0.12, 0.16, 0.16, 0.16])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.05, 1.25)\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.suptitle(f'An√°lisis Categ√≥rico Compacto: {cat_col}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar la versi√≥n compacta para las categ√≥ricas clave\n",
    "for cat in [c for c in ['Category', 'Content Rating', 'Type', 'Genres Main', 'Installs'] if c in applications_data.columns]:\n",
    "    analyze_categorical_compact(applications_data, cat, 'Rating', top_n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 2.2.8. An√°lisis de correlaci√≥n entre variables\n",
    "\n",
    "#### 2.2.8.1. Variables con Mayor Relaci√≥n\n",
    "- Existe una fuerte correlaci√≥n positiva entre **Installs Numeric** y **Reviews**:\n",
    "  - Pearson: 0.64 (relaci√≥n lineal moderada-fuerte).\n",
    "  - Spearman: 0.97 (relaci√≥n mon√≥tonica muy fuerte).\n",
    "- Esto implica que a mayor n√∫mero de instalaciones, mayor n√∫mero de rese√±as.\n",
    "\n",
    "#### 2.2.8.2. Correlaci√≥n de Pearson\n",
    "- En general, las correlaciones de Pearson muestran relaciones m√°s d√©biles que Spearman, lo cual indica que las relaciones lineales no son tan marcadas.\n",
    "- **Installs Numeric y Reviews** presentan la correlaci√≥n lineal m√°s alta (0.64), siendo moderada-fuerte.\n",
    "- **Size y Reviews** muestran una correlaci√≥n positiva baja/D√©bil (0.24).\n",
    "- El resto de variables (Rating, Price) tienen correlaciones casi nulas con las dem√°s, lo que refleja poca relaci√≥n lineal.\n",
    "\n",
    "#### 2.2.8.3. Correlaci√≥n de Spearman\n",
    "- **Installs Numeric y Reviews** tienen la correlaci√≥n m√°s fuerte (0.97).\n",
    "- **Size** muestra correlaci√≥n moderada con **Reviews** (0.37) y con **Installs Numeric** (0.35).\n",
    "- **Price** presenta correlaciones negativas con **Reviews** (-0.17) e **Installs Numeric** (-0.24).\n",
    "\n",
    "#### 2.2.8.4. Observaciones Clave\n",
    "- El n√∫mero de instalaciones y las rese√±as son las variables m√°s relacionadas, lo cual es l√≥gico, ya que m√°s usuarios generan m√°s interacciones.\n",
    "- El tama√±o de la aplicaci√≥n influye ligeramente en rese√±as e instalaciones, pero no de forma determinante.\n",
    "- El precio no solo carece de relaci√≥n positiva, sino que parece tener un impacto negativo sobre la popularidad (menos instalaciones y rese√±as).\n",
    "\n",
    "#### 2.2.8.5. Conclusi√≥n\n",
    "- **Installs Numeric** y **Reviews** son las m√©tricas m√°s cr√≠ticas en el dataset de **Google Play Store**, ya que reflejan el √©xito y la popularidad de la aplicaci√≥n.\n",
    "- **Size** es un factor secundario con cierta relaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaci√≥n mejorado para el proyecto de Google Play Store\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"An√°lisis de correlaci√≥n con m√∫ltiples m√©tricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlaci√≥n de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlaci√≥n de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlaci√≥n de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlaci√≥n de Spearman (Monot√≥nica)')\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Correlaci√≥n Multi-m√©trica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\nüîó Correlaciones Significativas:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, corr_matrix in zip(['Pearson', 'Spearman'], [corr_pearson, corr_spearman]):\n",
    "        print(f\"\\n{method}:\")\n",
    "        significant_corr = corr_matrix[(abs(corr_matrix) > 0.3) & (corr_matrix != 1)].stack()\n",
    "        for (var1, var2), corr in significant_corr.items():\n",
    "            strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"D√©bil\"\n",
    "            direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "            print(f\"  ‚Ä¢ {var1} y {var2}: {corr:+.3f} ({strength} {direction})\")\n",
    "    \n",
    "# Ejecutar el an√°lisis de correlaci√≥n\n",
    "correlation_analysis(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.2.9. An√°lisis de Outliers (IQR, Z-Score e Isolation Forest)\n",
    "**Resumen cuantitativo**\n",
    "- Total de registros analizados: **10,841**.\n",
    "- Filas marcadas como outlier por m√©todo:\n",
    "  - IQR: **3,489** filas (32.18%) ‚Üí refleja colas largas especialmente en `Reviews`, `Installs Numeric`, `Price`.\n",
    "  - Z-Score (> |3|): **654** filas (6.03%) ‚Üí mucho m√°s selectivo, captura extremos verdaderamente alejados tras estandarizaci√≥n.\n",
    "  - Isolation Forest (contaminaci√≥n=10%): **1,084** filas (10.0%) ‚Üí patr√≥n no lineal de anomal√≠as combinadas.\n",
    "- Consenso entre m√©todos:\n",
    "  - Detectadas por los 3 m√©todos: **502** filas (casos altamente an√≥malos).\n",
    "  - Detectadas exactamente por 2 m√©todos: **731** filas (an√≥malas consistentes, revisar antes de decidir acci√≥n).\n",
    "\n",
    "**Variables m√°s afectadas (IQR)**\n",
    "- `Reviews`: **1,925** outliers ‚Üí distribuci√≥n extremadamente sesgada; valores muy altos representan apps masivas (probablemente leg√≠timos).\n",
    "- `Installs Numeric`: **828** outliers ‚Üí escalas de descargas masivas (1e7‚Äì1e9).\n",
    "- `Price`: **800** outliers ‚Üí pocos productos de precio elevado (‚â• p75 + 1.5¬∑IQR); revisar si son apps premium leg√≠timas.\n",
    "- `Size`: **564** outliers ‚Üí tama√±os extremos (muy grandes o inusualmente peque√±os).\n",
    "- `Rating`: **504** outliers ‚Üí incluye valores extremos bajos y el caso inv√°lido (`Rating=19`).\n",
    "\n",
    "**Interpretaci√≥n y criterios**\n",
    "- Muchos outliers provienen de fen√≥menos de cola larga t√≠picos (popularidad extrema o modelo freemium/premium).\n",
    "- No se recomienda eliminar masivamente outliers de `Reviews` o `Installs Numeric` sin antes transformar (`log1p`) o agrupar (binning), para no perder informaci√≥n sobre apps exitosas.\n",
    "- El valor inv√°lido `Rating=19` debe normalizarse a `NaN` y excluirse de modelado. Otros ratings muy bajos pueden mantenerse (aportan contraste).\n",
    "- Outliers en `Price` podr√≠an segmentarse: gratis (0), bajo costo (0 < p ‚â§ 10), premium (10 < p ‚â§ 50), ultra premium (>50).\n",
    "\n",
    "\n",
    "**Conclusi√≥n**\n",
    "El comportamiento extremo de `Reviews` e `Installs Numeric` refleja la naturaleza desigual del mercado (unas pocas apps concentran gran parte de la atenci√≥n). Un manejo cuidadoso (transformaciones y flags) preservar√° informaci√≥n √∫til sin distorsionar el entrenamiento. Se prioriza limpieza puntual (ratings inv√°lidos) sobre eliminaci√≥n agresiva de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detecci√≥n de outliers usando m√∫ltiples m√©todos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # M√©todo 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # M√©todo 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # M√©todo 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (M√©todo IQR)')\n",
    "    ax1.set_ylabel('N√∫mero de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribuci√≥n de outliers por m√©todo\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparaci√≥n de M√©todos de Detecci√≥n')\n",
    "    ax2.set_ylabel('N√∫mero de Outliers Detectados')\n",
    "    ax2.set_xlabel('M√©todo')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estad√≠stico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detecci√≥n de Anomal√≠as:\n",
    "    \n",
    "    ‚Ä¢ Total de observaciones: {len(df):,}\n",
    "    ‚Ä¢ Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables m√°s afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Investigar outliers antes de eliminar. \n",
    "    Pueden contener informaci√≥n valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Outliers y Anomal√≠as', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "# Ejecutar la detecci√≥n de outliers en el dataset de aplicaciones\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(applications_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playstore-kgOgXMXp-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
