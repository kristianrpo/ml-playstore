{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuración de entorno\n",
    "\n",
    "En esta sección validamos que nuestro entorno de trabajo esté correctamente configurado antes de comenzar el análisis.  \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. **Versión de Python**  \n",
    "   - Se verifica que esté instalada la versión **3.11 o superior** (se recomienda 3.13).  \n",
    "   - Esto garantiza compatibilidad con librerías modernas de análisis de datos y machine learning.\n",
    "\n",
    "2. **Importación de librerías base**  \n",
    "   - Se cargan librerías fundamentales:  \n",
    "     - `numpy`, `pandas`: manipulación y análisis de datos.  \n",
    "     - `matplotlib`, `seaborn`: visualización de datos.  \n",
    "     - `scipy`: funciones estadísticas.  \n",
    "   - Además se configuran estilos gráficos y opciones de visualización en pandas para trabajar con tablas más grandes.\n",
    "\n",
    "3. **Verificación de versiones críticas**  \n",
    "   - Se comprueba que `scikit-learn` esté instalado y en una versión **>= 1.0.1**.  \n",
    "   - Esto es esencial ya que `scikit-learn` se usará para el modelado (baseline y posteriores).\n",
    "\n",
    "Con esta configuración inicial aseguramos que el entorno sea reproducible y que todas las dependencias necesarias estén listas antes de continuar con el **EDA** y el **baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert sys.version_info >= (3, 11), \"Este notebook trabajo con python 3.11 o superiores (recomendado 3.13)\"\n",
    "\n",
    "print(f\"Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librerías críticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Metodología CRISP-DM\n",
    "## 2.1. Comprensión del Negocio\n",
    "El problema de Google Play Store  \n",
    "\n",
    "**Contexto:**  \n",
    "Es 2025. El mercado de aplicaciones móviles es altamente competitivo: millones de apps conviven en Google Play Store.  \n",
    "Los desarrolladores buscan mejorar la visibilidad de sus aplicaciones y los usuarios dependen del **rating promedio** para decidir qué descargar.  \n",
    "\n",
    "**Problema actual:**  \n",
    "- El rating se conoce **solo después** de que los usuarios descargan y reseñan.  \n",
    "- Las valoraciones son **altamente variables** y pueden depender de múltiples factores (categoría, descargas, precio, tamaño, tipo de app).  \n",
    "- Los desarrolladores carecen de una herramienta para **estimar la calificación potencial** de una app antes o durante su lanzamiento.  \n",
    "- La competencia es muy alta: una diferencia de décimas en rating puede significar miles de descargas menos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.1. Solución propuesta  \n",
    "Construir un **sistema automático de predicción de rating** de apps a partir de sus características disponibles en el dataset de Google Play Store.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. Definiendo el éxito  \n",
    "\n",
    "**Métrica de negocio:**  \n",
    "- Ayudar a los desarrolladores a anticipar la valoración probable de su app.  \n",
    "- Reducir la dependencia de pruebas de mercado costosas o lentas.  \n",
    "- Identificar características clave que favorecen una alta valoración (≥ 4.3).  \n",
    "\n",
    "**Métrica técnica:**  \n",
    "- Lograr un **Error Absoluto Medio (MAE) < 0.5 estrellas** en la predicción de rating.  \n",
    "- Para la versión de clasificación (alta vs. baja calificación): obtener un **F1-score > 0.70**.  \n",
    "\n",
    "**¿Por qué estos valores?**  \n",
    "- El rating va de 1 a 5 → un error de 0.5 equivale a 10% de la escala.  \n",
    "- Una diferencia de medio punto puede marcar la visibilidad de la app en el ranking.  \n",
    "- Tasadores humanos (usuarios) también muestran variabilidad similar en sus calificaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Preguntas críticas antes de empezar  \n",
    "\n",
    "1. **¿Realmente necesitamos ML?**  \n",
    "   - Alternativa 1: Calcular el promedio de ratings por categoría → demasiado simple, no captura variabilidad.  \n",
    "   - Alternativa 2: Reglas heurísticas (ej. “si es gratis y tiene muchas descargas, tendrá rating alto”) → insuficiente.  \n",
    "   - **Conclusión:** Sí, ML es apropiado para capturar relaciones no lineales y múltiples factores.  \n",
    "\n",
    "2. **¿Qué pasa si el modelo falla?**  \n",
    "   - Transparencia: aclarar que es una estimación automática.  \n",
    "   - Complementar con rangos de predicción (ej: intervalo de confianza).  \n",
    "   - Mantener como referencia comparativa, no como único criterio de éxito.  \n",
    "\n",
    "3. **¿Cómo mediremos el impacto?**  \n",
    "   - Capacidad de anticipar apps con alta probabilidad de éxito.  \n",
    "   - Ahorro de tiempo en validaciones preliminares.  \n",
    "   - Insights para desarrolladores sobre qué factores influyen más en el rating.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2.2. Comprensión de los Datos  \n",
    "\n",
    "El objetivo de esta fase es explorar y entender el dataset de Google Play Store antes de construir modelos **(análisis exploratorio)**.  \n",
    "Nos centraremos en:  \n",
    "\n",
    "1. **Vista rápida del dataset**  \n",
    "   - Identificar dimensiones (filas × columnas).  \n",
    "   - Tipos de datos (numéricos, categóricos, texto, fechas).  \n",
    "   - Valores faltantes obvios y rangos sospechosos.\n",
    "\n",
    "2. **Descripción de variables**  \n",
    "   - Revisar cada columna y entender su significado.  \n",
    "   - Detectar qué variables podrían ser útiles como predictores y cuál será la variable objetivo (rating).  \n",
    "\n",
    "3. **Detección de problemas en los datos**  \n",
    "   - Análisis de valores faltantes.  \n",
    "   - Estrategias: eliminar filas/columnas, imputar valores o crear indicadores de “dato faltante”.  \n",
    "\n",
    "4. **Estadísticas descriptivas y univariadas**  \n",
    "   - Media vs mediana (sesgo de la distribución).  \n",
    "   - Desviación estándar (variabilidad, posibles outliers).  \n",
    "   - Mínimos/máximos sospechosos.  \n",
    "   - Histogramas para ver forma (normal, sesgada, bimodal, uniforme, picos extraños).  \n",
    "\n",
    "5. **Análisis de variables categóricas**  \n",
    "   - Distribución de categorías (ej. categorías de apps, tipo de app, content rating).  \n",
    "   - Detección de clases dominantes o categorías poco representadas.  \n",
    "\n",
    "6. **Correlaciones y relaciones entre variables**  \n",
    "   - Matriz de correlación de Pearson para variables numéricas.  \n",
    "   - Identificar relaciones fuertes, moderadas o débiles.  \n",
    "   - Importante: recordar que **correlación ≠ causalidad**.  \n",
    "7. ** Análisis de outliers **\n",
    "   -  Tipos e identificación de outliers a través de diferentes métodos.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**  \n",
    "No siempre es necesario aplicar todos los pasos con igual profundidad.  \n",
    "- Para este proyecto, el foco está en **identificar variables relevantes para predecir el rating** y **limpiar datos inconsistentes**.  \n",
    "- Otros análisis más complejos (ej. NLP sobre descripciones) se pueden dejar como trabajo futuro (según trabajos de referencia investigados).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2.1 Descarga de datos  \n",
    "\n",
    "En este paso descargamos el dataset de Google Play Store desde Kaggle y lo organizamos en la estructura de carpetas del proyecto.  \n",
    "\n",
    "1. Usamos la librería `kagglehub` para acceder al dataset público **`lava18/google-play-store-apps`** directamente desde Kaggle.  \n",
    "2. Se define una ruta clara dentro del proyecto para almacenar los datos originales: `../data/original/google-play-store/`. Esto ayuda a mantener la reproducibilidad y una estructura organizada.  \n",
    "3. Con la función `shutil.copytree` copiamos los archivos descargados a la carpeta destino. De esta forma, el dataset queda disponible en nuestro directorio de trabajo para su análisis posterior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "def download_data(origin_repository, target_folder):\n",
    "    # Descargar dataset\n",
    "    path = kagglehub.dataset_download(origin_repository)\n",
    "    \n",
    "    # Copiar los archivos descargados\n",
    "    shutil.copytree(path, target_folder, dirs_exist_ok=True)\n",
    "    \n",
    "\n",
    "download_data(\"lava18/google-play-store-apps\", \"../data/original/google-play-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2.2 Carga de datos  \n",
    "\n",
    "En este paso realizamos la **lectura del archivo CSV** que contiene el dataset descargado previamente.  \n",
    "\n",
    "- Definimos una función `load_data(path, file)` que recibe la ruta y el nombre del archivo, y lo carga con `pandas.read_csv()`.  \n",
    "- Cargamos el dataset principal en la variable `applications_data` desde la carpeta `../data/original/google-play-store/`.  \n",
    "- Incluimos una verificación simple:  \n",
    "  - Si el dataset se carga con éxito, se imprime `\"Dataset loaded\"`.  \n",
    "  - En caso contrario, se muestra un mensaje de error.  \n",
    "\n",
    "Con esta validación aseguramos que el archivo esté disponible y correctamente leído antes de continuar con el análisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path, file):\n",
    "    return pd.read_csv(f\"{path}/{file}\")\n",
    "\n",
    "def convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte a numéricas solo las columnas que deberían serlo, sin tocar 'Size'.\n",
    "    Usa to_numeric(errors='coerce') para evitar ValueError si aparece texto.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rating\n",
    "    if \"Rating\" in df.columns:\n",
    "        df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Reviews: quitar comas y cualquier carácter no numérico/punto\n",
    "    if \"Reviews\" in df.columns:\n",
    "        df[\"Reviews\"] = (\n",
    "            df[\"Reviews\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Installs: quitar +, comas y cualquier carácter no numérico/punto\n",
    "    if \"Installs\" in df.columns:\n",
    "        df[\"Installs Numeric\"] = (\n",
    "            df[\"Installs\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Price: quitar $ y cualquier carácter no numérico/punto\n",
    "    if \"Price\" in df.columns:\n",
    "        df[\"Price\"] = (\n",
    "            df[\"Price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "\n",
    "    if \"Size\" in df.columns:\n",
    "        def parse_size(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip()\n",
    "                if x.endswith(\"M\"):\n",
    "                    return float(x[:-1])\n",
    "                elif x.endswith(\"k\") or x.endswith(\"K\"):\n",
    "                    return float(x[:-1]) / 1024  # KB -> MB\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        df[\"Size\"] = df[\"Size\"].apply(parse_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "temp_applications_data = load_data(\"../data/original/google-play-store\", \"googleplaystore.csv\")\n",
    "applications_data = convert_numeric_columns(temp_applications_data)\n",
    "\n",
    "\n",
    "if len(applications_data):\n",
    "    print(\"Dataset cargado\")\n",
    "else:\n",
    "    print(\"Error cargando dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2.3 Vista rápida del dataset\n",
    "\n",
    "**Dimensiones y columnas**\n",
    "- Registros: **10,841** filas.\n",
    "- Columnas: actualmente **14**; **originalmente eran 13** y se **añadió** una columna derivada: **`Installs Numeric`** para análisis con describe.\n",
    "- Memoria aproximada: **~1.2 MB**.\n",
    "\n",
    "**Tipos de datos (y transformaciones realizadas)**\n",
    "- Numéricas (`float64`): `Rating`, `Reviews`, `Size`, `Price`, **`Installs Numeric`**.\n",
    "- Categóricas / texto (`object`): `App`, `Category`, `Installs` *(forma original con “1,000+”)*, `Type`, `Content Rating`, `Genres`, `Last Updated`, `Current Ver`, `Android Ver`.\n",
    "- Transformaciones ya aplicadas:\n",
    "  - **`Installs`** se **conservó** en su formato original (categórico con “+” y comas) **y** se creó **`Installs Numeric`** mapeando esos rangos a números (0 … 1,000,000,000).\n",
    "  - **`Price`**, **`Reviews`** y **`Size`** fueron normalizadas/parseadas a **numérico** para análisis y modelado.\n",
    "\n",
    "**Valores faltantes (no-null count → faltantes aprox.)**\n",
    "- `Rating`: 9,367 → **1,474 faltantes (~13.6%)**.\n",
    "- `Size`: 9,145 → **1,696 faltantes (~15.6%)**.\n",
    "- `Current Ver`: 10,833 → **8 faltantes (~0.07%)**.\n",
    "- `Android Ver`: 10,838 → **3 faltantes (~0.03%)**.\n",
    "- `Content Rating`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- `Price`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- `Installs Numeric`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- Resto de columnas: **sin faltantes**.\n",
    "\n",
    "**Duplicados:**\n",
    "-   Se identificaron **483 filas duplicadas** (≈ **4.46%** del\n",
    "    dataset).\\\n",
    "-   Ejemplos de duplicados incluyen apps como:\n",
    "    -   *Quick PDF Scanner + OCR FREE*\\\n",
    "    -   *Box*\\\n",
    "    -   *Google My Business*\\\n",
    "    -   *ZOOM Cloud Meetings*\\\n",
    "    -   *join.me -- Simple Meetings*\\\n",
    "\n",
    "**Rangos y valores sospechosos (según `describe()`)**\n",
    "- `Rating`: **min = 1.0**, **max = 19.0** → **19** es inválido para la escala 1–5 (error de dato a corregir).\n",
    "- `Reviews`: media ~ **444k**, **p75 ≈ 54,768**, **max ≈ 78M** → valores altos plausibles; tratar como **outliers**.\n",
    "- `Size` (MB): media ~ **21.5**, **p50 = 13**, **p75 = 30**, **max = 100** → distribución sesgada a la derecha; mínimos muy bajos (**0.01**) a revisar.\n",
    "- `Price` (USD): **mediana = 0** y **p75 = 0** → la mayoría son **apps gratuitas**; **max = 400** sugiere outliers de precio.\n",
    "- `Installs Numeric`: **p25 = 1,000**, **p50 = 100,000**, **p75 = 5,000,000**, **max = 1,000,000,000** → escala muy amplia; conviene usar **transformaciones log** o **binning** en el EDA/modelado.\n",
    "\n",
    "**Conclusión inicial**\n",
    "- Los **faltantes** más relevantes están en `Rating` y `Size`; habrá que decidir estategía para aumentar, imputar o nivelar los datos.\n",
    "- Existen **outliers (no legítimos)** (ej. `Rating = 19`) y variables con **colas largas** (ej. `Reviews`, `Installs Numeric`, `Price`).\n",
    "- Eliminar **duplicados** para evitar sesgos de análisis y que no introduzcan ruidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(applications_data.head().style.background_gradient(cmap='RdYlGn', subset=['Rating']))\n",
    "\n",
    "# Información detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "applications_data.info()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(applications_data.describe().round(2).T)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar duplicados\n",
    "num_duplicados = applications_data.duplicated().sum()\n",
    "print(f\"Total de registros duplicados: {num_duplicados}\")\n",
    "\n",
    "# Mostrar ejemplos de duplicados si existen\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas:\\n\")\n",
    "    display(applications_data[applications_data.duplicated()].head())\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.4 Descripción de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price',\n",
    "    'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver',\n",
    "    'Installs Numeric'\n",
    "]\n",
    "\n",
    "tipos = [\n",
    "    'Categórica',          # App\n",
    "    'Categórica',          # Category\n",
    "    'Numérica (Target)',   # Rating\n",
    "    'Numérica',            # Reviews\n",
    "    'Numérica (MB)',       # Size\n",
    "    'Categórica (rango)',  # Installs\n",
    "    'Categórica',          # Type\n",
    "    'Numérica (USD)',      # Price\n",
    "    'Categórica',          # Content Rating\n",
    "    'Categórica',          # Genres\n",
    "    'Texto (fecha)',       # Last Updated (parseable a fecha)\n",
    "    'Texto',               # Current Ver\n",
    "    'Texto',               # Android Ver\n",
    "    'Numérica',            # Installs Numeric\n",
    "]\n",
    "\n",
    "descripciones = [\n",
    "    'Nombre de la aplicación.',\n",
    "    'Categoría oficial de la app en Google Play.',\n",
    "    'Calificación promedio de usuarios (1 a 5).',\n",
    "    'Número de reseñas reportadas.',\n",
    "    'Tamaño aproximado de la app en MB.',\n",
    "    'Instalaciones en rango (p.ej., \"1,000+\").',\n",
    "    'Tipo de app (Free / Paid).',\n",
    "    'Precio en USD (0 para gratuitas).',\n",
    "    'Clasificación de contenido (Everyone, Teen, etc.).',\n",
    "    'Género(s) de la app.',\n",
    "    'Fecha de última actualización (texto en origen).',\n",
    "    'Versión actual declarada por el desarrollador.',\n",
    "    'Versión mínima de Android requerida.',\n",
    "    'Instalaciones convertidas a número para análisis.'\n",
    "]\n",
    "\n",
    "valores_faltantes = [applications_data[col].isnull().sum() if col in applications_data.columns else None for col in variables]\n",
    "\n",
    "metadata = {\n",
    "    'Variable': variables,\n",
    "    'Tipo': tipos,\n",
    "    'Descripción': descripciones,\n",
    "    'Valores Faltantes': valores_faltantes\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# Mostrar con resaltado de faltantes\n",
    "styled = df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x > 0 else '',\n",
    "    subset=['Valores Faltantes']\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Resumen de dtypes originales (informativo)\n",
    "dtypes_resumen = applications_data[variables].dtypes.astype(str).reset_index()\n",
    "dtypes_resumen.columns = ['Variable', 'dtype pandas']\n",
    "display(dtypes_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.2.5 Detección de problemas en los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Resumen de hallazgos (valores faltantes):**\n",
    "- `Size` ≈ 15.6% y `Rating` ≈ 13.6% concentran la mayoría de los faltantes.\n",
    "- Faltantes puntuales (≈0.01%): `Type`, `Price`, `Content Rating`, `Installs Numeric` ocurren en la misma(s) fila(s) → patrón conjunto.\n",
    "- `Android Ver` (0.03%) y `Current Ver` (0.07%) con faltantes residuales, parcialmente correlacionados con el grupo anterior.\n",
    "\n",
    "**Heatmap de correlación de patrones de faltantes (interpretación):**\n",
    "- Correlación 1.00 entre `Type`, `Price`, `Content Rating`, `Installs Numeric`: las ausencias co-ocurren en el/los mismos registros. Acciones coordinadas.\n",
    "- `Android Ver` muestra correlación moderada (~0.58) con ese grupo: algunas veces falta junto con ellos.\n",
    "- `Size`, `Rating`, `Current Ver` tienen patrones de faltantes independientes del grupo anterior (correlaciones cercanas a 0), lo que sugiere causas distintas.\n",
    "\n",
    "#### Posibles estrategias de corrección\n",
    "\n",
    "- Limpieza básica\n",
    "  - Eliminar duplicados (483 filas) para evitar sesgos.\n",
    "  - Validar y corregir outliers imposibles, p. ej., `Rating = 19` → convertir a NaN para tratarlo como faltante.\n",
    "\n",
    "- Imputación (conservadora y por grupos)\n",
    "  - `Rating` (target): para modelado, eliminar filas sin `Rating`; para EDA descriptivo, imputar mediana por `Category` solo para visualización.\n",
    "  - `Size`: imputar mediana por `Category × Type` y crear indicador `size_missing`.\n",
    "  - `Android Ver`, `Current Ver`: imputar moda por `Category` y crear indicadores `androidver_missing`, `currentver_missing`.\n",
    "  - Faltantes conjuntos (`Type`, `Price`, `Content Rating`, `Installs Numeric`):\n",
    "    - Si es 1 fila: eliminarla es lo más simple y seguro.\n",
    "    - Alternativa (si se prefiere imputar):\n",
    "      - `Type`: inferir desde `Price` (0 → Free, >0 → Paid).\n",
    "      - `Price`: 0 si `Type == Free`, si `Paid` usar mediana por `Category`.\n",
    "      - `Content Rating`: moda por `Category`.\n",
    "      - `Installs Numeric`: mediana por `Category × Type` o por bin de `Installs`.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategias de “nivelación” según los porcentajes observados\n",
    "\n",
    "- Size (~15.6% faltantes, >5% y <<60%)\n",
    "  - Acción: imputar mediana por grupo `Category × Type`.\n",
    "  - Añadir flag: `size_missing = 1` cuando falte (conserva señal de ausencia).\n",
    "  - Justificación: volumen relevante; la mediana por grupos respeta diferencias entre tipos/categorías.\n",
    "\n",
    "- Rating (~13.6% faltantes, >5% y <<60%) [variable objetivo]\n",
    "  - Para modelado: eliminar filas sin `Rating` (evita sesgo por imputación del target).\n",
    "  - Para EDA descriptivo: si se requiere visualizar completos, imputar mediana por `Category` solo para gráficos/tablas (no para entrenamiento).\n",
    "  - Justificación: imputar el target puede distorsionar métricas.\n",
    "\n",
    "- Current Ver (0.07%) y Android Ver (0.03%) (<5%)\n",
    "  - Acción: imputar con la moda por `Category`. Flags opcionales `currentver_missing` y `androidver_missing`.\n",
    "  - Justificación: impacto ínfimo; moda es suficiente y estable.\n",
    "\n",
    "- Faltantes “en bloque” en la misma fila: Type, Price, Content Rating, Installs Numeric (≈0.01% cada uno; correlación 1.00)\n",
    "  - Si es 1 fila: eliminarla directamente.\n",
    "  - Si hubiera más en el futuro y se prefiriera imputar coordinadamente:\n",
    "    - `Type` desde `Price` (0 → Free, >0 → Paid),\n",
    "    - `Price` = 0 si `Free`, si `Paid` usar mediana por `Category`,\n",
    "    - `Content Rating` = moda por `Category`,\n",
    "    - `Installs Numeric` = mediana por `Category × Type`.\n",
    "  - Justificación: co-ocurren; eliminar 1 fila no afecta el conjunto y evita inconsistencias.\n",
    "\n",
    "- Transformaciones para estabilizar distribuciones (complementarias a la imputación)\n",
    "  - `Reviews` y `Installs Numeric`: aplicar `log1p` para análisis y futuros modelos. *****************************\n",
    "  - `Installs` (rangos): tratar como ordinal/bins en el EDA.\n",
    "\n",
    "- Limpieza previa necesaria\n",
    "  - Eliminar duplicados (483 filas).\n",
    "  - Corregir valores imposibles detectados en el EDA (ej. `Rating = 19` → NaN) y re-entrar al flujo de imputación/nivelación anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Análisis completo de valores faltantes con visualizaciones.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': missing_counts.values,\n",
    "        'Porcentaje': missing_pct.values,\n",
    "        'Tipo_Dato': df.dtypes.values\n",
    "    })\n",
    "\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "    if len(missing_df) == 0:\n",
    "        print(\"No hay valores faltantes en el dataset\")\n",
    "        return missing_df\n",
    "\n",
    "    # Visualización: barras y correlación de patrones de faltantes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Gráfico de barras de % faltantes\n",
    "    ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "    ax1.set_xlabel('Columna')\n",
    "    ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "    ax1.set_title('Valores Faltantes por Columna')\n",
    "    ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "    ax1.axhline(y=60, color='purple', linestyle='--', label='Umbral 60%')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Heatmap de correlación de patrones de faltantes\n",
    "    mask_df = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "    if mask_df.shape[1] >= 2:\n",
    "        corr = mask_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "        ax2.set_title('Correlación de Patrones de Valores Faltantes')\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Correlación de faltantes (no aplica: 1 columna)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "missing_analysis = analyze_missing_values(applications_data)\n",
    "if missing_analysis is not None and not missing_analysis.empty:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2.6 Estadisticas descriptivas y univariadas (númerico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A partir de la tabla de estadísticas y los gráficos generados para `Rating`, `Reviews`, `Size`, `Price` e `Installs Numeric`, se observan los siguientes puntos clave.\n",
    "\n",
    "- Rating\n",
    "  - Media ≈ 4.19 y mediana ≈ 4.30 → ligera cola a la izquierda (más apps con rating alto). Hay un valor imposible (≈19), confirmado en el boxplot/Q-Q como outlier extremo.\n",
    "  - Outliers: ~5% por IQR, dominados por el valor inválido y algunos ratings bajos.\n",
    "  - Q-Q plot: desviación frente a normalidad, esperable para una variable acotada [1,5].\n",
    "  - Implicación/acción: eliminar filas sin `Rating` para modelado; corregir `Rating=19 → NaN` y excluir; no aplicar transformaciones (la escala es ya interpretables).\n",
    "\n",
    "- Reviews\n",
    "  - Media ≫ mediana (pico en 0–pocos miles; máximo ≈ 78M) → cola muy larga a la derecha.\n",
    "  - Boxplot: ~18% outliers por IQR (muchas apps con reseñas muy altas).\n",
    "  - Q-Q plot: gran desviación de normalidad (heavy tail).\n",
    "  - Relación con Rating: correlación positiva muy débil (~0.07), tendencia casi plana.\n",
    "  - Implicación/acción: usar `log1p(Reviews)` para estabilizar la distribución en análisis/modelado; considerar winsorizar p99.9 para vistas tabulares si se desea.\n",
    "\n",
    "- Size (MB)\n",
    "  - Media > mediana (≈ 21.5 vs 13) → sesgo a la derecha; valores hasta 100 MB.\n",
    "  - ~6% outliers por IQR, especialmente en colas altas.\n",
    "  - Q-Q plot: curvatura en colas; no normal.\n",
    "  - Relación con Rating: correlación positiva débil (~0.08); señal muy tenue.\n",
    "  - Implicación/acción: imputar faltantes por `Category × Type` y añadir `size_missing`; opcionalmente probar `log1p(Size)` o binning para robustecer.\n",
    "\n",
    "- Price (USD)\n",
    "  - Mediana = 0 (mayoría gratis) y cola a la derecha con máximos altos (≈ 400).\n",
    "  - ~7% outliers por IQR; Q-Q muestra heavy tail.\n",
    "  - Relación con Rating: correlación negativa muy débil (~-0.02).\n",
    "  - Implicación/acción: crear `is_free = (Price == 0)` y, si se usa `Price` continuo, considerar `log1p(Price)` para las pocas apps pagas; validar coherencia `Type=Free ⇒ Price=0`.\n",
    "\n",
    "- Installs Numeric\n",
    "  - Media ≫ mediana (100k) con máximo 1e9 → distribución extremadamente sesgada a la derecha.\n",
    "  - ~7–8% outliers por IQR; Q-Q muy alejado de normalidad.\n",
    "  - Relación con Rating: correlación débil positiva (~0.05) y tendencia casi plana.\n",
    "  - Implicación/acción: usar `log1p(Installs Numeric)` o bins ordinales para análisis; verificar coherencia con `Installs` textual.\n",
    "\n",
    "Recomendaciones transversales\n",
    "- Eliminar duplicados antes de resumir para evitar sesgos.\n",
    "- Tratar outliers evidentes no-legítimos (p. ej. `Rating=19`). Para colas largas legítimas (`Reviews`, `Installs Numeric`, `Price`): preferir `log1p` o winsorización solo para visualizaciones.\n",
    "- Mantener consistencia: `Type=Free ⇒ Price=0`; `Installs Numeric` coherente con el rango de `Installs`.\n",
    "- Para relaciones con `Rating`, las correlaciones lineales observadas son débiles; la señal puede emerger mejor con interacciones (p. ej., `is_free × installs_bin`) o modelos no lineales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selección de columnas numéricas relevantes\n",
    "numeric_cols = [c for c in ['Rating', 'Reviews', 'Size', 'Price', 'Installs Numeric'] if c in applications_data.columns]\n",
    "\n",
    "# Tabla de estadísticas básicas (media, mediana, std, min, p25, p50, p75, max)\n",
    "describe_tbl = applications_data[numeric_cols].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "# Métricas adicionales robustas\n",
    "extra = pd.DataFrame(index=numeric_cols)\n",
    "extra['mad'] = [stats.median_abs_deviation(applications_data[c].dropna()) for c in numeric_cols]\n",
    "extra['skew'] = [applications_data[c].skew(skipna=True) for c in numeric_cols]\n",
    "extra['kurtosis'] = [applications_data[c].kurtosis(skipna=True) for c in numeric_cols]\n",
    "extra['cv'] = [applications_data[c].std(skipna=True) / applications_data[c].mean(skipna=True) if applications_data[c].mean(skipna=True) not in [0, np.nan] else np.nan for c in numeric_cols]\n",
    "\n",
    "stats_table = describe_tbl.join(extra)\n",
    "display(stats_table.round(3))\n",
    "\n",
    "\n",
    "def univariate_analysis(df: pd.DataFrame, column: str, target: str | None = None):\n",
    "    \"\"\"Análisis univariado con histograma, boxplot, Q-Q plot y relación con target.\"\"\"\n",
    "    series = df[column].dropna()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Histograma con líneas de media y mediana\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(series, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(series.mean(), color='red', linestyle='--', label=f\"Media: {series.mean():.2f}\")\n",
    "    ax1.axvline(series.median(), color='green', linestyle='--', label=f\"Mediana: {series.median():.2f}\")\n",
    "    ax1.set_title(f\"Distribución de {column}\")\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2) Boxplot + conteo de outliers (IQR)\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(series, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)\n",
    "    n_out = int(outliers_mask.sum())\n",
    "    pct_out = 100 * n_out / len(series) if len(series) else 0\n",
    "    ax2.set_title(f\"Boxplot de {column}\")\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.text(1.1, Q3, f\"Outliers: {n_out} ({pct_out:.1f}%)\", fontsize=10)\n",
    "\n",
    "    # 3) Q-Q plot normal\n",
    "    ax3 = axes[1, 0]\n",
    "    stats.probplot(series, dist='norm', plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # 4) Relación con target si aplica\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns and column != target:\n",
    "        valid = df[[column, target]].dropna()\n",
    "        ax4.scatter(valid[column], valid[target], alpha=0.4, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f\"{column} vs {target}\")\n",
    "        # Línea de tendencia (ajuste lineal simple)\n",
    "        if len(valid) > 1:\n",
    "            z = np.polyfit(valid[column], valid[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            xs = np.linspace(valid[column].min(), valid[column].max(), 200)\n",
    "            ax4.plot(xs, p(xs), 'r--', alpha=0.8, label='Tendencia')\n",
    "            corr = valid[column].corr(valid[target])\n",
    "            ax4.text(0.05, 0.95, f\"Correlación: {corr:.3f}\", transform=ax4.transAxes,\n",
    "                     fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            ax4.legend()\n",
    "    else:\n",
    "        ax4.axis('off')\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"Análisis Univariado: {column}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar análisis univariado para cada métrica numérica, relacionando con Rating\n",
    "for col in numeric_cols:\n",
    "    univariate_analysis(applications_data, col, target='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.2.7. Análisis Univariado Categórico\n",
    "- Category\n",
    "  - Distribución: alta concentración en `FAMILY` (~19%) y `GAME` (~12%). El resto de categorías tienen menor peso individual; el grupo `Others` acumula ~31% del total.\n",
    "  - Rating por categoría: diferencias moderadas; la **mediana** suele estar entre 4.2–4.4. Algunas categorías muestran desviación estándar mayor (p. ej., `PRODUCTIVITY`, `LIFESTYLE`), indicando más variabilidad de valoración.\n",
    "  - Implicaciones: riesgo de sesgo por categorías mayoritarias en análisis agregados. Para modelado, conviene usar dummies Top-K o codificación ordinal/target encoding con cuidado (evitar fuga). Agrupar colas largas en `Others` es adecuado para visualización.\n",
    "\n",
    "- Content Rating\n",
    "  - Distribución: `Everyone` domina (~79%), seguido por `Teen` (~12%); `Mature 17+` y `Everyone 10+` suman ~9% en conjunto; clases raras casi nulas.\n",
    "  - Rating por nivel de contenido: medias similares (≈4.1–4.3). `Teen` tiende a mediana 4.3 y variabilidad algo menor; `Mature 17+` muestra algo más de dispersión.\n",
    "  - Implicaciones: por el fuerte desbalance, esta variable aporta señal limitada por sí sola. Útil como interacción con `Category`/`Genres`.\n",
    "\n",
    "- Type\n",
    "  - Distribución: `Free` ≈ 93%, `Paid` ≈ 7% (clase muy desbalanceada); existe un registro anómalo (valor 0) en los gráficos que debe eliminarse/corregirse.\n",
    "  - Rating por tipo: medias muy cercanas (Free ≈ 4.19, Paid ≈ 4.27). La diferencia es pequeña y probablemente no significativa sin controlar otras variables (p. ej., `Category`).\n",
    "  - Implicaciones: por el desbalance extremo, conviene usar `is_free` como binaria y, si se modela interacción con `Installs` o `Price`, puede emerger señal. Validar regla `Type=Free ⇒ Price=0`.\n",
    "\n",
    "- Genres Main (primer género)\n",
    "  - Distribución: gran cola larga; `Others` concentra ~48%. Entre Top-12, `Tools`, `Entertainment` y `Education` destacan en frecuencia.\n",
    "  - Rating por género: diferencias pequeñas (medianas ~4.2–4.4), con algunas variaciones en dispersión (p. ej., `Medical` y `Lifestyle` más variables).\n",
    "  - Implicaciones: por la alta cardinalidad y colas largas, mantener Top-K + `Others` en EDA ayuda a la legibilidad. Para modelado, preferir codificación que reduzca dimensionalidad (Top-K dummies, hashing, o target encoding con validación adecuada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_compact(df: pd.DataFrame, cat_col: str, target_col: str, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Versión compacta para variables con muchas categorías:\n",
    "    - Ordena por frecuencia, muestra top_n y agrupa el resto en \"Others\".\n",
    "    - Barras horizontales, pie chart compacto, boxplot y tabla para top_n.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, target_col]].dropna(subset=[cat_col, target_col]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"Sin datos para {cat_col} y {target_col}\")\n",
    "        return\n",
    "\n",
    "    counts = data[cat_col].value_counts()\n",
    "    top_cats = counts.head(top_n)\n",
    "    others_count = counts.iloc[top_n:].sum()\n",
    "\n",
    "    # Mapeo a top_n + Others\n",
    "    mapping = {c: c for c in top_cats.index}\n",
    "    data['__cat__'] = data[cat_col].where(data[cat_col].isin(top_cats.index), other='Others')\n",
    "\n",
    "    # Recalcular conteos con Others\n",
    "    counts_compact = data['__cat__'].value_counts()\n",
    "    order = list(top_cats.index) + (['Others'] if 'Others' in counts_compact.index else [])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Barras horizontales (mejor legibilidad)\n",
    "    ax1 = axes[0, 0]\n",
    "    vals = counts_compact.loc[order]\n",
    "    ax1.barh(range(len(vals)), vals.values, color=plt.cm.Set3(range(len(vals))))\n",
    "    ax1.set_yticks(range(len(vals)))\n",
    "    ax1.set_yticklabels(order)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title(f'Distribución (Top {top_n}) de {cat_col}')\n",
    "    ax1.set_xlabel('Frecuencia')\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax1.text(v, i, f'  {v} ({v/len(data)*100:.1f}%)', va='center')\n",
    "\n",
    "    # 2) Pie chart compacto\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(vals.values, labels=order, autopct='%1.1f%%', startangle=140,\n",
    "            colors=plt.cm.Set3(range(len(vals))))\n",
    "    ax2.set_title(f'Proporción (Top {top_n} + Others) de {cat_col}')\n",
    "\n",
    "    # 3) Boxplot del target por categoría (solo top_n)\n",
    "    ax3 = axes[1, 0]\n",
    "    top_mask = data['__cat__'] != 'Others'\n",
    "    data_top = data[top_mask]\n",
    "    data_top.boxplot(column=target_col, by='__cat__', ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col} (Top {top_n})')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # 4) Tabla de estadísticas por categoría (solo top_n y Others si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    stats_by_cat = data.groupby('__cat__')[target_col].agg(['count', 'mean', 'median', 'std']).loc[order].round(2)\n",
    "    table = ax4.table(cellText=stats_by_cat.reset_index().values,\n",
    "                      colLabels=['Categoría', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                      cellLoc='center', loc='center', colWidths=[0.35, 0.12, 0.16, 0.16, 0.16])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.05, 1.25)\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.suptitle(f'Análisis Categórico Compacto: {cat_col}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar la versión compacta para las categóricas clave\n",
    "for cat in [c for c in ['Category', 'Content Rating', 'Type', 'Genres Main', 'Installs'] if c in applications_data.columns]:\n",
    "    analyze_categorical_compact(applications_data, cat, 'Rating', top_n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 2.2.8. Análisis de correlación entre variables\n",
    "\n",
    "#### 2.2.8.1. Variables con Mayor Relación\n",
    "- Existe una fuerte correlación positiva entre **Installs Numeric** y **Reviews**:\n",
    "  - Pearson: 0.64 (relación lineal moderada-fuerte).\n",
    "  - Spearman: 0.97 (relación monótonica muy fuerte).\n",
    "- Esto implica que a mayor número de instalaciones, mayor número de reseñas.\n",
    "\n",
    "#### 2.2.8.2. Correlación de Pearson\n",
    "- En general, las correlaciones de Pearson muestran relaciones más débiles que Spearman, lo cual indica que las relaciones lineales no son tan marcadas.\n",
    "- **Installs Numeric y Reviews** presentan la correlación lineal más alta (0.64), siendo moderada-fuerte.\n",
    "- **Size y Reviews** muestran una correlación positiva baja/Débil (0.24).\n",
    "- El resto de variables (Rating, Price) tienen correlaciones casi nulas con las demás, lo que refleja poca relación lineal.\n",
    "\n",
    "#### 2.2.8.3. Correlación de Spearman\n",
    "- **Installs Numeric y Reviews** tienen la correlación más fuerte (0.97).\n",
    "- **Size** muestra correlación moderada con **Reviews** (0.37) y con **Installs Numeric** (0.35).\n",
    "- **Price** presenta correlaciones negativas con **Reviews** (-0.17) e **Installs Numeric** (-0.24).\n",
    "\n",
    "#### 2.2.8.4. Observaciones Clave\n",
    "- El número de instalaciones y las reseñas son las variables más relacionadas, lo cual es lógico, ya que más usuarios generan más interacciones.\n",
    "- El tamaño de la aplicación influye ligeramente en reseñas e instalaciones, pero no de forma determinante.\n",
    "- El precio no solo carece de relación positiva, sino que parece tener un impacto negativo sobre la popularidad (menos instalaciones y reseñas).\n",
    "\n",
    "#### 2.2.8.5. Conclusión\n",
    "- **Installs Numeric** y **Reviews** son las métricas más críticas en el dataset de **Google Play Store**, ya que reflejan el éxito y la popularidad de la aplicación.\n",
    "- **Size** es un factor secundario con cierta relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de correlación mejorado para el proyecto de Google Play Store\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Análisis de correlación con múltiples métricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlación de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlación de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlación de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlación de Spearman (Monotónica)')\n",
    "    \n",
    "    plt.suptitle('Análisis de Correlación Multi-métrica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\n🔗 Correlaciones Significativas:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, corr_matrix in zip(['Pearson', 'Spearman'], [corr_pearson, corr_spearman]):\n",
    "        print(f\"\\n{method}:\")\n",
    "        significant_corr = corr_matrix[(abs(corr_matrix) > 0.3) & (corr_matrix != 1)].stack()\n",
    "        for (var1, var2), corr in significant_corr.items():\n",
    "            strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Débil\"\n",
    "            direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "            print(f\"  • {var1} y {var2}: {corr:+.3f} ({strength} {direction})\")\n",
    "    \n",
    "# Ejecutar el análisis de correlación\n",
    "correlation_analysis(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.2.9. Análisis de Outliers (IQR, Z-Score e Isolation Forest)\n",
    "**Resumen cuantitativo**\n",
    "- Total de registros analizados: **10,841**.\n",
    "- Filas marcadas como outlier por método:\n",
    "  - IQR: **3,489** filas (32.18%) → refleja colas largas especialmente en `Reviews`, `Installs Numeric`, `Price`.\n",
    "  - Z-Score (> |3|): **654** filas (6.03%) → mucho más selectivo, captura extremos verdaderamente alejados tras estandarización.\n",
    "  - Isolation Forest (contaminación=10%): **1,084** filas (10.0%) → patrón no lineal de anomalías combinadas.\n",
    "- Consenso entre métodos:\n",
    "  - Detectadas por los 3 métodos: **502** filas (casos altamente anómalos).\n",
    "  - Detectadas exactamente por 2 métodos: **731** filas (anómalas consistentes, revisar antes de decidir acción).\n",
    "\n",
    "**Variables más afectadas (IQR)**\n",
    "- `Reviews`: **1,925** outliers → distribución extremadamente sesgada; valores muy altos representan apps masivas (probablemente legítimos).\n",
    "- `Installs Numeric`: **828** outliers → escalas de descargas masivas (1e7–1e9).\n",
    "- `Price`: **800** outliers → pocos productos de precio elevado (≥ p75 + 1.5·IQR); revisar si son apps premium legítimas.\n",
    "- `Size`: **564** outliers → tamaños extremos (muy grandes o inusualmente pequeños).\n",
    "- `Rating`: **504** outliers → incluye valores extremos bajos y el caso inválido (`Rating=19`).\n",
    "\n",
    "**Interpretación y criterios**\n",
    "- Muchos outliers provienen de fenómenos de cola larga típicos (popularidad extrema o modelo freemium/premium).\n",
    "- No se recomienda eliminar masivamente outliers de `Reviews` o `Installs Numeric` sin antes transformar (`log1p`) o agrupar (binning), para no perder información sobre apps exitosas.\n",
    "- El valor inválido `Rating=19` debe normalizarse a `NaN` y excluirse de modelado. Otros ratings muy bajos pueden mantenerse (aportan contraste).\n",
    "- Outliers en `Price` podrían segmentarse: gratis (0), bajo costo (0 < p ≤ 10), premium (10 < p ≤ 50), ultra premium (>50).\n",
    "\n",
    "\n",
    "**Conclusión**\n",
    "El comportamiento extremo de `Reviews` e `Installs Numeric` refleja la naturaleza desigual del mercado (unas pocas apps concentran gran parte de la atención). Un manejo cuidadoso (transformaciones y flags) preservará información útil sin distorsionar el entrenamiento. Se prioriza limpieza puntual (ratings inválidos) sobre eliminación agresiva de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detección de outliers usando múltiples métodos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Método 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # Método 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # Método 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualización\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (Método IQR)')\n",
    "    ax1.set_ylabel('Número de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribución de outliers por método\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparación de Métodos de Detección')\n",
    "    ax2.set_ylabel('Número de Outliers Detectados')\n",
    "    ax2.set_xlabel('Método')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estadístico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detección de Anomalías:\n",
    "    \n",
    "    • Total de observaciones: {len(df):,}\n",
    "    • Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    • Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    • Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables más afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Investigar outliers antes de eliminar. \n",
    "    Pueden contener información valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('Análisis de Outliers y Anomalías', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "# Ejecutar la detección de outliers en el dataset de aplicaciones\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# 3. Transformación de Datos\n",
    "\n",
    "En base a todos los hallazgos del **Análisis Exploratorio de Datos (EDA)**, aplicaremos las siguientes técnicas de limpieza y transformación:\n",
    "\n",
    "## 3.1. Resumen de problemas detectados\n",
    "\n",
    "Durante el EDA identificamos:\n",
    "\n",
    "1. **Duplicados**: 483 filas duplicadas (~4.46%)\n",
    "2. **Valores imposibles**: Rating = 19 (fuera del rango 1-5)\n",
    "3. **Valores faltantes**: \n",
    "   - Size ≈ 15.6%\n",
    "   - Rating ≈ 13.6%\n",
    "   - Current Ver, Android Ver, Content Rating, Type, Price (<1%)\n",
    "4. **Outliers legítimos**: Distribuciones con colas largas en Reviews, Installs Numeric, Price, Size\n",
    "5. **Variables con distribuciones sesgadas**: Requieren transformaciones logarítmicas\n",
    "6. **Inconsistencias**: Necesidad de validar coherencia entre Type y Price\n",
    "\n",
    "## 3.2. Plan de transformación\n",
    "\n",
    "Aplicaremos las siguientes transformaciones en orden:\n",
    "\n",
    "1. **Eliminación de duplicados**\n",
    "2. **Corrección de valores imposibles**\n",
    "3. **Imputación de valores faltantes** (estrategia por variable)\n",
    "4. **Validación de consistencia** entre variables relacionadas\n",
    "5. **Transformaciones de variables numéricas** (log, binning)\n",
    "6. **Creación de variables derivadas** (features engineering básico)\n",
    "7. **Resumen final** del dataset limpio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 3.3.1. Eliminación de Duplicados\n",
    "\n",
    "Eliminamos las **483 filas duplicadas** detectadas en el EDA para evitar:\n",
    "- Sesgos en análisis estadísticos\n",
    "- Sobrepeso de ciertas apps en el modelado\n",
    "- Distorsión de métricas de evaluación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset para transformaciones\n",
    "df_clean = applications_data.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 1: ELIMINACIÓN DE DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado inicial\n",
    "print(f\"\\n📊 Registros antes de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"🔍 Duplicados encontrados: {df_clean.duplicated().sum():,} ({df_clean.duplicated().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de duplicados antes de eliminar\n",
    "if df_clean.duplicated().sum() > 0:\n",
    "    print(\"\\n📋 Ejemplos de aplicaciones duplicadas:\")\n",
    "    duplicated_apps = df_clean[df_clean.duplicated(keep=False)].sort_values('App')\n",
    "    display(duplicated_apps[['App', 'Category', 'Rating', 'Reviews', 'Installs']].head(10))\n",
    "\n",
    "# Eliminar duplicados (manteniendo la primera ocurrencia)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Estado final\n",
    "print(f\"\\n✅ Registros después de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"🗑️  Filas eliminadas: {len(applications_data) - len(df_clean):,}\")\n",
    "print(f\"📈 Reducción: {((len(applications_data) - len(df_clean))/len(applications_data)*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 3.3.2. Corrección de Valores Imposibles\n",
    "\n",
    "Corregimos valores que están fuera del rango válido:\n",
    "- **Rating = 19**: valor imposible (escala 1-5) → convertir a `NaN`\n",
    "- Cualquier Rating < 1 o > 5 → convertir a `NaN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 2: CORRECCIÓN DE VALORES IMPOSIBLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar valores de Rating fuera del rango [1, 5]\n",
    "invalid_ratings = df_clean['Rating'][(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5)]\n",
    "print(f\"\\n🔍 Ratings inválidos encontrados: {len(invalid_ratings)}\")\n",
    "\n",
    "if len(invalid_ratings) > 0:\n",
    "    print(\"\\n📋 Ejemplos de ratings inválidos:\")\n",
    "    invalid_apps = df_clean[df_clean['Rating'].isin(invalid_ratings)]\n",
    "    display(invalid_apps[['App', 'Category', 'Rating', 'Reviews']].head())\n",
    "    \n",
    "    # Mostrar distribución de valores inválidos\n",
    "    print(f\"\\n📊 Valores inválidos únicos: {sorted(invalid_ratings.dropna().unique())}\")\n",
    "    \n",
    "    # Corregir: convertir valores inválidos a NaN\n",
    "    df_clean.loc[(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5), 'Rating'] = np.nan\n",
    "    \n",
    "    print(f\"\\n✅ Valores inválidos corregidos (convertidos a NaN)\")\n",
    "    print(f\"📈 Total de NaN en Rating ahora: {df_clean['Rating'].isnull().sum()}\")\n",
    "else:\n",
    "    print(\"\\n✅ No se encontraron ratings inválidos\")\n",
    "\n",
    "# Verificar otros valores imposibles (negativos en columnas que no pueden serlo)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificando valores negativos en columnas numéricas:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numeric_cols = ['Reviews', 'Size', 'Price', 'Installs Numeric']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"⚠️  {col}: {negative_count} valores negativos encontrados\")\n",
    "            df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "            print(f\"   ✅ Corregidos a NaN\")\n",
    "        else:\n",
    "            print(f\"✅ {col}: Sin valores negativos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 3.3.3. Validación de Consistencia entre Variables\n",
    "\n",
    "Validamos y corregimos inconsistencias lógicas entre variables relacionadas:\n",
    "- **Type vs Price**: Si `Type = 'Free'`, entonces `Price` debe ser 0\n",
    "- **Type vs Price**: Si `Price > 0`, entonces `Type` debe ser 'Paid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 3: VALIDACIÓN DE CONSISTENCIA ENTRE VARIABLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validar consistencia Type vs Price\n",
    "print(\"\\n🔍 Validando consistencia entre Type y Price:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Casos inconsistentes: Type='Free' pero Price > 0\n",
    "free_but_paid = df_clean[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0)]\n",
    "print(f\"\\n⚠️  Apps marcadas como 'Free' pero con Price > 0: {len(free_but_paid)}\")\n",
    "if len(free_but_paid) > 0:\n",
    "    display(free_but_paid[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price > 0, cambiar Type a 'Paid'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    print(f\"   ✅ Corregido: Type cambiado a 'Paid'\")\n",
    "\n",
    "# Casos inconsistentes: Type='Paid' pero Price = 0\n",
    "paid_but_free = df_clean[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0)]\n",
    "print(f\"\\n⚠️  Apps marcadas como 'Paid' pero con Price = 0: {len(paid_but_free)}\")\n",
    "if len(paid_but_free) > 0:\n",
    "    display(paid_but_free[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price = 0, cambiar Type a 'Free'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    print(f\"   ✅ Corregido: Type cambiado a 'Free'\")\n",
    "\n",
    "# Inferir Type desde Price cuando Type es NaN\n",
    "type_missing = df_clean['Type'].isnull()\n",
    "if type_missing.sum() > 0:\n",
    "    print(f\"\\n🔍 Type faltante en {type_missing.sum()} registros\")\n",
    "    print(\"   ✅ Infiriendo Type desde Price...\")\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    remaining_missing = df_clean['Type'].isnull().sum()\n",
    "    print(f\"   ✅ Type inferido. Faltantes restantes: {remaining_missing}\")\n",
    "\n",
    "print(\"\\n✅ Validación de consistencia completada\")\n",
    "print(f\"📊 Distribución final de Type:\")\n",
    "print(df_clean['Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 3.3.4. Imputación de Valores Faltantes\n",
    "\n",
    "Aplicamos estrategias diferenciadas por variable según lo detectado en el EDA:\n",
    "\n",
    "**Estrategia por variable:**\n",
    "1. **Size** (~15.6% faltantes): imputar mediana por `Category × Type` + flag `size_missing`\n",
    "2. **Rating** (~13.6% faltantes): **NO imputar** (es el target); para modelado eliminaremos filas sin Rating\n",
    "3. **Content Rating** (<1%): imputar moda por `Category`\n",
    "4. **Android Ver** (~0.03%): imputar moda por `Category`\n",
    "5. **Current Ver** (~0.07%): imputar moda por `Category`\n",
    "6. **Price** (<1%): imputar 0 si Type='Free', mediana por Category si Type='Paid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: IMPUTACIÓN DE VALORES FALTANTES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Resumen inicial de valores faltantes\n",
    "print(\"\\n📊 Valores faltantes ANTES de imputación:\")\n",
    "missing_before = df_clean.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
    "for col, count in missing_before.items():\n",
    "    pct = count / len(df_clean) * 100\n",
    "    print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# 1. SIZE: Imputar mediana por Category × Type + crear flag\n",
    "print(\"\\n1️⃣  Imputando Size (mediana por Category × Type)...\")\n",
    "df_clean['size_missing'] = df_clean['Size'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    for app_type in df_clean['Type'].unique():\n",
    "        mask = (df_clean['Category'] == category) & (df_clean['Type'] == app_type) & df_clean['Size'].isnull()\n",
    "        if mask.sum() > 0:\n",
    "            # Calcular mediana del grupo\n",
    "            median_size = df_clean.loc[\n",
    "                (df_clean['Category'] == category) & (df_clean['Type'] == app_type), 'Size'\n",
    "            ].median()\n",
    "            \n",
    "            # Si no hay mediana en el grupo, usar mediana global\n",
    "            if pd.isna(median_size):\n",
    "                median_size = df_clean['Size'].median()\n",
    "            \n",
    "            df_clean.loc[mask, 'Size'] = median_size\n",
    "\n",
    "print(f\"   ✅ Size imputado. Faltantes restantes: {df_clean['Size'].isnull().sum()}\")\n",
    "print(f\"   📌 Flag 'size_missing' creado ({df_clean['size_missing'].sum()} registros marcados)\")\n",
    "\n",
    "# 2. CONTENT RATING: Imputar moda por Category\n",
    "print(\"\\n2️⃣  Imputando Content Rating (moda por Category)...\")\n",
    "df_clean['content_rating_missing'] = df_clean['Content Rating'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Content Rating'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_rating = df_clean.loc[df_clean['Category'] == category, 'Content Rating'].mode()\n",
    "        if len(mode_rating) > 0:\n",
    "            df_clean.loc[mask, 'Content Rating'] = mode_rating.iloc[0]\n",
    "        else:\n",
    "            # Si no hay moda en el grupo, usar moda global\n",
    "            df_clean.loc[mask, 'Content Rating'] = df_clean['Content Rating'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ✅ Content Rating imputado. Faltantes restantes: {df_clean['Content Rating'].isnull().sum()}\")\n",
    "\n",
    "# 3. ANDROID VER: Imputar moda por Category\n",
    "print(\"\\n3️⃣  Imputando Android Ver (moda por Category)...\")\n",
    "df_clean['android_ver_missing'] = df_clean['Android Ver'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Android Ver'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_ver = df_clean.loc[df_clean['Category'] == category, 'Android Ver'].mode()\n",
    "        if len(mode_ver) > 0:\n",
    "            df_clean.loc[mask, 'Android Ver'] = mode_ver.iloc[0]\n",
    "        else:\n",
    "            df_clean.loc[mask, 'Android Ver'] = df_clean['Android Ver'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ✅ Android Ver imputado. Faltantes restantes: {df_clean['Android Ver'].isnull().sum()}\")\n",
    "\n",
    "# 4. CURRENT VER: Imputar moda por Category\n",
    "print(\"\\n4️⃣  Imputando Current Ver (moda por Category)...\")\n",
    "df_clean['current_ver_missing'] = df_clean['Current Ver'].isnull().astype(int)\n",
    "\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & df_clean['Current Ver'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        mode_ver = df_clean.loc[df_clean['Category'] == category, 'Current Ver'].mode()\n",
    "        if len(mode_ver) > 0:\n",
    "            df_clean.loc[mask, 'Current Ver'] = mode_ver.iloc[0]\n",
    "        else:\n",
    "            df_clean.loc[mask, 'Current Ver'] = df_clean['Current Ver'].mode().iloc[0]\n",
    "\n",
    "print(f\"   ✅ Current Ver imputado. Faltantes restantes: {df_clean['Current Ver'].isnull().sum()}\")\n",
    "\n",
    "# 5. PRICE: Imputar según Type\n",
    "print(\"\\n5️⃣  Imputando Price (0 si Free, mediana por Category si Paid)...\")\n",
    "df_clean['price_missing'] = df_clean['Price'].isnull().astype(int)\n",
    "\n",
    "# Free apps → Price = 0\n",
    "mask_free = (df_clean['Type'] == 'Free') & df_clean['Price'].isnull()\n",
    "df_clean.loc[mask_free, 'Price'] = 0\n",
    "\n",
    "# Paid apps → mediana por Category\n",
    "for category in df_clean['Category'].unique():\n",
    "    mask = (df_clean['Category'] == category) & (df_clean['Type'] == 'Paid') & df_clean['Price'].isnull()\n",
    "    if mask.sum() > 0:\n",
    "        median_price = df_clean.loc[\n",
    "            (df_clean['Category'] == category) & (df_clean['Type'] == 'Paid'), 'Price'\n",
    "        ].median()\n",
    "        if pd.isna(median_price):\n",
    "            median_price = df_clean.loc[df_clean['Type'] == 'Paid', 'Price'].median()\n",
    "        df_clean.loc[mask, 'Price'] = median_price\n",
    "\n",
    "print(f\"   ✅ Price imputado. Faltantes restantes: {df_clean['Price'].isnull().sum()}\")\n",
    "\n",
    "# 6. RATING: NO IMPUTAR (es el target)\n",
    "print(\"\\n6️⃣  Rating: NO se imputa (es la variable objetivo)\")\n",
    "print(f\"   📌 Se mantendrán {df_clean['Rating'].isnull().sum()} registros con Rating faltante\")\n",
    "print(f\"   📌 Estos registros se eliminarán en la fase de preparación para modelado\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 Valores faltantes DESPUÉS de imputación:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
    "if len(missing_after) > 0:\n",
    "    for col, count in missing_after.items():\n",
    "        pct = count / len(df_clean) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   ✅ No quedan valores faltantes (excepto Rating, que es el target)\")\n",
    "\n",
    "print(\"\\n✅ Imputación completada exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 3.3.5. Transformaciones de Variables Numericas\n",
    "\n",
    "Aplicamos transformaciones para estabilizar distribuciones sesgadas:\n",
    "\n",
    "1. **Log-transformaciones**: Para variables con colas largas (Reviews, Installs Numeric)\n",
    "2. **Binning**: Crear versiones categoricas de variables numericas para analisis\n",
    "3. **Variables binarias**: Crear indicadores utiles (is_free, is_large_app, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 5: TRANSFORMACIONES DE VARIABLES NUMERICAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. LOG-TRANSFORMACIONES para variables con colas largas\n",
    "print(\"\\n1️⃣  Aplicando transformaciones logaritmicas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Reviews: log1p (log(1+x) para manejar 0s)\n",
    "df_clean['Reviews_log'] = np.log1p(df_clean['Reviews'])\n",
    "print(f\"   ✅ Reviews_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Reviews'].mean():.0f}, Mediana: {df_clean['Reviews'].median():.0f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Reviews_log'].mean():.2f}, Mediana: {df_clean['Reviews_log'].median():.2f}\")\n",
    "\n",
    "# Installs Numeric: log1p\n",
    "df_clean['Installs_log'] = np.log1p(df_clean['Installs Numeric'])\n",
    "print(f\"\\n   ✅ Installs_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Installs Numeric'].mean():.0f}, Mediana: {df_clean['Installs Numeric'].median():.0f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Installs_log'].mean():.2f}, Mediana: {df_clean['Installs_log'].median():.2f}\")\n",
    "\n",
    "# Size: log1p (opcional, menos sesgado que Reviews/Installs)\n",
    "df_clean['Size_log'] = np.log1p(df_clean['Size'])\n",
    "print(f\"\\n   ✅ Size_log creado (log1p)\")\n",
    "print(f\"      Original - Media: {df_clean['Size'].mean():.2f}, Mediana: {df_clean['Size'].median():.2f}\")\n",
    "print(f\"      Log      - Media: {df_clean['Size_log'].mean():.2f}, Mediana: {df_clean['Size_log'].median():.2f}\")\n",
    "\n",
    "# 2. BINNING de variables numéricas\n",
    "print(\"\\n\\n2️⃣  Creando bins para variables numéricas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Price bins\n",
    "df_clean['Price_bin'] = pd.cut(\n",
    "    df_clean['Price'], \n",
    "    bins=[-0.01, 0, 2.99, 9.99, 49.99, 500],\n",
    "    labels=['Free', 'Low ($0-3)', 'Mid ($3-10)', 'High ($10-50)', 'Premium ($50+)']\n",
    ")\n",
    "print(f\"   ✅ Price_bin creado\")\n",
    "print(f\"      Distribución:\\n{df_clean['Price_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# Size bins (en MB)\n",
    "df_clean['Size_bin'] = pd.cut(\n",
    "    df_clean['Size'],\n",
    "    bins=[0, 10, 25, 50, 100, 1000],\n",
    "    labels=['Small (<10MB)', 'Medium (10-25MB)', 'Large (25-50MB)', 'Very Large (50-100MB)', 'Huge (>100MB)']\n",
    ")\n",
    "print(f\"\\n   ✅ Size_bin creado\")\n",
    "print(f\"      Distribución:\\n{df_clean['Size_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# Installs bins (rangos más interpretables)\n",
    "df_clean['Installs_bin'] = pd.cut(\n",
    "    df_clean['Installs Numeric'],\n",
    "    bins=[0, 100, 1000, 10000, 100000, 1000000, 10000000, 1e10],\n",
    "    labels=['<100', '100-1K', '1K-10K', '10K-100K', '100K-1M', '1M-10M', '>10M']\n",
    ")\n",
    "print(f\"\\n   ✅ Installs_bin creado\")\n",
    "print(f\"      Distribución:\\n{df_clean['Installs_bin'].value_counts().to_string()}\")\n",
    "\n",
    "# 3. VARIABLES BINARIAS útiles\n",
    "print(\"\\n\\n3️⃣  Creando variables binarias (indicadores)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# is_free\n",
    "df_clean['is_free'] = (df_clean['Type'] == 'Free').astype(int)\n",
    "print(f\"   ✅ is_free creado: {df_clean['is_free'].sum()} apps gratuitas ({df_clean['is_free'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_large_app (>50MB)\n",
    "df_clean['is_large_app'] = (df_clean['Size'] > 50).astype(int)\n",
    "print(f\"   ✅ is_large_app creado: {df_clean['is_large_app'].sum()} apps grandes ({df_clean['is_large_app'].mean()*100:.1f}%)\")\n",
    "\n",
    "# has_high_installs (>1M)\n",
    "df_clean['has_high_installs'] = (df_clean['Installs Numeric'] > 1000000).astype(int)\n",
    "print(f\"   ✅ has_high_installs creado: {df_clean['has_high_installs'].sum()} apps populares ({df_clean['has_high_installs'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_top_category (pertenece a FAMILY o GAME)\n",
    "df_clean['is_top_category'] = df_clean['Category'].isin(['FAMILY', 'GAME']).astype(int)\n",
    "print(f\"   ✅ is_top_category creado: {df_clean['is_top_category'].sum()} apps en categorías principales ({df_clean['is_top_category'].mean()*100:.1f}%)\")\n",
    "\n",
    "# is_everyone_rated (Content Rating = Everyone)\n",
    "df_clean['is_everyone_rated'] = (df_clean['Content Rating'] == 'Everyone').astype(int)\n",
    "print(f\"   ✅ is_everyone_rated creado: {df_clean['is_everyone_rated'].sum()} apps para todos ({df_clean['is_everyone_rated'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Transformaciones numéricas completadas exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 3.3.6. Creacion de Variables Derivadas (Feature Engineering Basico)\n",
    "\n",
    "Creamos nuevas variables combinando informacion existente para capturar patrones mas complejos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 6: FEATURE ENGINEERING (VARIABLES DERIVADAS)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Review Rate: Reviews por instalación (engagement)\n",
    "print(\"\\n1️⃣  Calculando Review Rate (engagement)...\")\n",
    "df_clean['review_rate'] = df_clean['Reviews'] / (df_clean['Installs Numeric'] + 1)  # +1 para evitar división por 0\n",
    "print(f\"   ✅ review_rate creado\")\n",
    "print(f\"      Media: {df_clean['review_rate'].mean():.6f}, Mediana: {df_clean['review_rate'].median():.6f}\")\n",
    "print(f\"      Interpretación: proporción de usuarios que dejan reseña\")\n",
    "\n",
    "# 2. Genres Main: Extraer primer género de la lista de géneros\n",
    "print(\"\\n2️⃣  Extrayendo género principal...\")\n",
    "df_clean['Genres Main'] = df_clean['Genres'].str.split(';').str[0]\n",
    "print(f\"   ✅ Genres Main creado\")\n",
    "print(f\"      Géneros únicos: {df_clean['Genres Main'].nunique()}\")\n",
    "print(f\"      Top 5:\\n{df_clean['Genres Main'].value_counts().head().to_string()}\")\n",
    "\n",
    "# 3. Days Since Update: Días desde última actualización (requiere parsear fecha)\n",
    "print(\"\\n3️⃣  Calculando días desde última actualización...\")\n",
    "try:\n",
    "    df_clean['Last Updated Parsed'] = pd.to_datetime(df_clean['Last Updated'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2025-10-02')  # Fecha actual del proyecto\n",
    "    df_clean['days_since_update'] = (reference_date - df_clean['Last Updated Parsed']).dt.days\n",
    "    \n",
    "    print(f\"   ✅ days_since_update creado\")\n",
    "    print(f\"      Media: {df_clean['days_since_update'].mean():.0f} días\")\n",
    "    print(f\"      Mediana: {df_clean['days_since_update'].median():.0f} días\")\n",
    "    print(f\"      Rango: {df_clean['days_since_update'].min():.0f} - {df_clean['days_since_update'].max():.0f} días\")\n",
    "    \n",
    "    # Crear bins de actualización\n",
    "    df_clean['update_recency'] = pd.cut(\n",
    "        df_clean['days_since_update'],\n",
    "        bins=[-1, 30, 90, 180, 365, 730, 10000],\n",
    "        labels=['<1 month', '1-3 months', '3-6 months', '6-12 months', '1-2 years', '>2 years']\n",
    "    )\n",
    "    print(f\"\\n   ✅ update_recency creado\")\n",
    "    print(f\"      Distribución:\\n{df_clean['update_recency'].value_counts().to_string()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Error calculando days_since_update: {e}\")\n",
    "\n",
    "# 4. Size per Install: Tamaño promedio por instalación (eficiencia)\n",
    "print(\"\\n4️⃣  Calculando tamaño por instalación...\")\n",
    "df_clean['size_per_install'] = df_clean['Size'] / (df_clean['Installs Numeric'] + 1)\n",
    "print(f\"   ✅ size_per_install creado\")\n",
    "print(f\"      Media: {df_clean['size_per_install'].mean():.8f}\")\n",
    "\n",
    "# 5. Price Category: Categorización más simple de precio\n",
    "print(\"\\n5️⃣  Creando categoría simple de precio...\")\n",
    "df_clean['price_category'] = 'Free'\n",
    "df_clean.loc[df_clean['Price'] > 0, 'price_category'] = 'Paid'\n",
    "print(f\"   ✅ price_category creado\")\n",
    "print(f\"      Distribución:\\n{df_clean['price_category'].value_counts().to_string()}\")\n",
    "\n",
    "# 6. Popularity Score: Score compuesto simple\n",
    "print(\"\\n6️⃣  Calculando popularity score...\")\n",
    "# Normalizar componentes a [0,1] usando min-max\n",
    "installs_norm = (df_clean['Installs Numeric'] - df_clean['Installs Numeric'].min()) / (df_clean['Installs Numeric'].max() - df_clean['Installs Numeric'].min())\n",
    "reviews_norm = (df_clean['Reviews'] - df_clean['Reviews'].min()) / (df_clean['Reviews'].max() - df_clean['Reviews'].min())\n",
    "\n",
    "df_clean['popularity_score'] = (installs_norm * 0.7 + reviews_norm * 0.3) * 100  # Escala 0-100\n",
    "print(f\"   ✅ popularity_score creado (0-100)\")\n",
    "print(f\"      Media: {df_clean['popularity_score'].mean():.2f}\")\n",
    "print(f\"      Mediana: {df_clean['popularity_score'].median():.2f}\")\n",
    "\n",
    "print(\"\\n✅ Feature Engineering completado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 3.3.7. Resumen Final del Dataset Limpio\n",
    "\n",
    "Comparacion del dataset original vs limpio y resumen de todas las transformaciones aplicadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN FINAL: DATASET LIMPIO\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"🔹\" * 40)\n",
    "print(\"COMPARACIÓN: ORIGINAL vs LIMPIO\")\n",
    "print(\"🔹\" * 40)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Métrica': [\n",
    "        'Total de registros',\n",
    "        'Total de columnas',\n",
    "        'Duplicados',\n",
    "        'Rating faltantes',\n",
    "        'Size faltantes',\n",
    "        'Price faltantes',\n",
    "        'Type faltantes',\n",
    "        'Ratings inválidos (>5 o <1)',\n",
    "        'Memoria (MB)'\n",
    "    ],\n",
    "    'Original': [\n",
    "        f\"{len(applications_data):,}\",\n",
    "        f\"{len(applications_data.columns)}\",\n",
    "        f\"{applications_data.duplicated().sum():,}\",\n",
    "        f\"{applications_data['Rating'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Size'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Price'].isnull().sum():,}\",\n",
    "        f\"{applications_data['Type'].isnull().sum():,}\",\n",
    "        f\"{((applications_data['Rating'] > 5) | (applications_data['Rating'] < 1)).sum():,}\",\n",
    "        f\"{applications_data.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ],\n",
    "    'Limpio': [\n",
    "        f\"{len(df_clean):,}\",\n",
    "        f\"{len(df_clean.columns)}\",\n",
    "        f\"{df_clean.duplicated().sum():,}\",\n",
    "        f\"{df_clean['Rating'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Size'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Price'].isnull().sum():,}\",\n",
    "        f\"{df_clean['Type'].isnull().sum():,}\",\n",
    "        f\"{((df_clean['Rating'] > 5) | (df_clean['Rating'] < 1)).sum():,}\",\n",
    "        f\"{df_clean.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\n\" + \"🔹\" * 40)\n",
    "print(\"NUEVAS VARIABLES CREADAS\")\n",
    "print(\"🔹\" * 40)\n",
    "\n",
    "new_features = [\n",
    "    'Reviews_log', 'Installs_log', 'Size_log',\n",
    "    'Price_bin', 'Size_bin', 'Installs_bin',\n",
    "    'is_free', 'is_large_app', 'has_high_installs', 'is_top_category', 'is_everyone_rated',\n",
    "    'review_rate', 'Genres Main', 'days_since_update', 'update_recency',\n",
    "    'size_per_install', 'price_category', 'popularity_score',\n",
    "    'size_missing', 'content_rating_missing', 'android_ver_missing', 'current_ver_missing', 'price_missing'\n",
    "]\n",
    "\n",
    "available_features = [f for f in new_features if f in df_clean.columns]\n",
    "print(f\"\\nTotal de nuevas variables: {len(available_features)}\")\n",
    "print(\"\\nListado:\")\n",
    "for i, feat in enumerate(available_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"🔹\" * 40)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS (Variables numéricas principales)\")\n",
    "print(\"🔹\" * 40)\n",
    "\n",
    "key_numeric = ['Rating', 'Reviews', 'Reviews_log', 'Size', 'Price', 'Installs Numeric', \n",
    "               'Installs_log', 'popularity_score', 'review_rate']\n",
    "available_numeric = [col for col in key_numeric if col in df_clean.columns]\n",
    "\n",
    "display(df_clean[available_numeric].describe().round(3).T)\n",
    "\n",
    "print(\"\\n\" + \"🔹\" * 40)\n",
    "print(\"DISTRIBUCIÓN DE VARIABLES CATEGÓRICAS CLAVE\")\n",
    "print(\"🔹\" * 40)\n",
    "\n",
    "print(\"\\n📊 Category (Top 10):\")\n",
    "print(df_clean['Category'].value_counts().head(10))\n",
    "\n",
    "print(\"\\n📊 Type:\")\n",
    "print(df_clean['Type'].value_counts())\n",
    "\n",
    "print(\"\\n📊 Content Rating:\")\n",
    "print(df_clean['Content Rating'].value_counts())\n",
    "\n",
    "print(\"\\n📊 Price Category:\")\n",
    "print(df_clean['price_category'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ TRANSFORMACIÓN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n📁 Dataset limpio disponible en: df_clean\")\n",
    "print(f\"📊 Dimensiones finales: {df_clean.shape[0]:,} filas × {df_clean.shape[1]} columnas\")\n",
    "print(f\"💾 Memoria utilizada: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\n🎯 Siguiente paso: Preparar datos para modelado (eliminar filas sin Rating)\")\n",
    "print(f\"   → df_model = df_clean.dropna(subset=['Rating'])\")\n",
    "print(f\"   → Filas para modelado: {df_clean['Rating'].notna().sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 3.3.8. Visualizacion Comparativa: Antes vs Despues\n",
    "\n",
    "Visualizamos el impacto de las transformaciones aplicadas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Reviews: Original vs Log\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(applications_data['Reviews'].dropna(), bins=50, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax1.set_xlabel('Reviews (escala original)')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.set_title('Reviews - Original (cola larga)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(df_clean['Reviews_log'].dropna(), bins=50, alpha=0.6, label='Log-transformado', color='green', edgecolor='black')\n",
    "ax2.set_xlabel('Reviews (log1p)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Reviews - Después de log1p (estabilizado)')\n",
    "ax2.legend()\n",
    "\n",
    "# 2. Valores faltantes: Original vs Limpio\n",
    "ax3 = axes[0, 2]\n",
    "missing_orig = applications_data.isnull().sum().sort_values(ascending=False).head(6)\n",
    "missing_clean = df_clean[missing_orig.index].isnull().sum()\n",
    "\n",
    "x = np.arange(len(missing_orig))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, missing_orig.values, width, label='Original', alpha=0.7, color='coral')\n",
    "ax3.bar(x + width/2, missing_clean.values, width, label='Limpio', alpha=0.7, color='lightgreen')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(missing_orig.index, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Valores Faltantes')\n",
    "ax3.set_title('Valores Faltantes: Original vs Limpio')\n",
    "ax3.legend()\n",
    "\n",
    "# 3. Rating distribution (solo válidos)\n",
    "ax4 = axes[1, 0]\n",
    "valid_ratings_orig = applications_data['Rating'][(applications_data['Rating'] >= 1) & (applications_data['Rating'] <= 5)]\n",
    "valid_ratings_clean = df_clean['Rating'].dropna()\n",
    "ax4.hist(valid_ratings_orig, bins=30, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax4.hist(valid_ratings_clean, bins=30, alpha=0.6, label='Limpio', color='orange', edgecolor='black')\n",
    "ax4.set_xlabel('Rating')\n",
    "ax4.set_ylabel('Frecuencia')\n",
    "ax4.set_title('Distribución de Rating (valores válidos)')\n",
    "ax4.legend()\n",
    "\n",
    "# 4. Installs: Original vs Log\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(applications_data['Installs Numeric'].dropna(), bins=50, alpha=0.6, label='Original', edgecolor='black')\n",
    "ax5.set_xlabel('Installs (escala original)')\n",
    "ax5.set_ylabel('Frecuencia')\n",
    "ax5.set_title('Installs - Original (muy sesgado)')\n",
    "ax5.legend()\n",
    "ax5.set_xlim(0, applications_data['Installs Numeric'].quantile(0.95))  # Truncar para visualización\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "ax6.hist(df_clean['Installs_log'].dropna(), bins=50, alpha=0.6, label='Log-transformado', color='purple', edgecolor='black')\n",
    "ax6.set_xlabel('Installs (log1p)')\n",
    "ax6.set_ylabel('Frecuencia')\n",
    "ax6.set_title('Installs - Después de log1p')\n",
    "ax6.legend()\n",
    "\n",
    "plt.suptitle('Impacto de las Transformaciones: Antes vs Después', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualización comparativa completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playstore-kgOgXMXp-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
