{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuraci√≥n de entorno\n",
    "\n",
    "En esta secci√≥n validamos que nuestro entorno de trabajo est√© correctamente configurado antes de comenzar el an√°lisis.  \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. **Versi√≥n de Python**  \n",
    "   - Se verifica que est√© instalada la versi√≥n **3.11 o superior** (se recomienda 3.13).  \n",
    "   - Esto garantiza compatibilidad con librer√≠as modernas de an√°lisis de datos y machine learning.\n",
    "\n",
    "2. **Importaci√≥n de librer√≠as base**  \n",
    "   - Se cargan librer√≠as fundamentales:  \n",
    "     - `numpy`, `pandas`: manipulaci√≥n y an√°lisis de datos.  \n",
    "     - `matplotlib`, `seaborn`: visualizaci√≥n de datos.  \n",
    "     - `scipy`: funciones estad√≠sticas.  \n",
    "   - Adem√°s se configuran estilos gr√°ficos y opciones de visualizaci√≥n en pandas para trabajar con tablas m√°s grandes.\n",
    "\n",
    "3. **Verificaci√≥n de versiones cr√≠ticas**  \n",
    "   - Se comprueba que `scikit-learn` est√© instalado y en una versi√≥n **>= 1.0.1**.  \n",
    "   - Esto es esencial ya que `scikit-learn` se usar√° para el modelado (baseline y posteriores).\n",
    "\n",
    "Con esta configuraci√≥n inicial aseguramos que el entorno sea reproducible y que todas las dependencias necesarias est√©n listas antes de continuar con el **EDA** y el **baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert sys.version_info >= (3, 11), \"Este notebook trabajo con python 3.11 o superiores (recomendado 3.13)\"\n",
    "\n",
    "print(f\"Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librer√≠as cr√≠ticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Metodolog√≠a CRISP-DM\n",
    "## 2.1. Comprensi√≥n del Negocio\n",
    "El problema de Google Play Store  \n",
    "\n",
    "**Contexto:**  \n",
    "Es 2025. El mercado de aplicaciones m√≥viles es altamente competitivo: millones de apps conviven en Google Play Store.  \n",
    "Los desarrolladores buscan mejorar la visibilidad de sus aplicaciones y los usuarios dependen del **rating promedio** para decidir qu√© descargar.  \n",
    "\n",
    "**Problema actual:**  \n",
    "- El rating se conoce **solo despu√©s** de que los usuarios descargan y rese√±an.  \n",
    "- Las valoraciones son **altamente variables** y pueden depender de m√∫ltiples factores (categor√≠a, descargas, precio, tama√±o, tipo de app).  \n",
    "- Los desarrolladores carecen de una herramienta para **estimar la calificaci√≥n potencial** de una app antes o durante su lanzamiento.  \n",
    "- La competencia es muy alta: una diferencia de d√©cimas en rating puede significar miles de descargas menos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.1. Soluci√≥n propuesta  \n",
    "Construir un **sistema autom√°tico de predicci√≥n de rating** de apps a partir de sus caracter√≠sticas disponibles en el dataset de Google Play Store.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. Definiendo el √©xito  \n",
    "\n",
    "**M√©trica de negocio:**  \n",
    "- Ayudar a los desarrolladores a anticipar la valoraci√≥n probable de su app.  \n",
    "- Reducir la dependencia de pruebas de mercado costosas o lentas.  \n",
    "- Identificar caracter√≠sticas clave que favorecen una alta valoraci√≥n (‚â• 4.3).  \n",
    "\n",
    "**M√©trica t√©cnica:**  \n",
    "- Lograr un **Error Absoluto Medio (MAE) < 0.5 estrellas** en la predicci√≥n de rating.  \n",
    "- Para la versi√≥n de clasificaci√≥n (alta vs. baja calificaci√≥n): obtener un **F1-score > 0.70**.  \n",
    "\n",
    "**¬øPor qu√© estos valores?**  \n",
    "- El rating va de 1 a 5 ‚Üí un error de 0.5 equivale a 10% de la escala.  \n",
    "- Una diferencia de medio punto puede marcar la visibilidad de la app en el ranking.  \n",
    "- Tasadores humanos (usuarios) tambi√©n muestran variabilidad similar en sus calificaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Preguntas cr√≠ticas antes de empezar  \n",
    "\n",
    "1. **¬øRealmente necesitamos ML?**  \n",
    "   - Alternativa 1: Calcular el promedio de ratings por categor√≠a ‚Üí demasiado simple, no captura variabilidad.  \n",
    "   - Alternativa 2: Reglas heur√≠sticas (ej. ‚Äúsi es gratis y tiene muchas descargas, tendr√° rating alto‚Äù) ‚Üí insuficiente.  \n",
    "   - **Conclusi√≥n:** S√≠, ML es apropiado para capturar relaciones no lineales y m√∫ltiples factores.  \n",
    "\n",
    "2. **¬øQu√© pasa si el modelo falla?**  \n",
    "   - Transparencia: aclarar que es una estimaci√≥n autom√°tica.  \n",
    "   - Complementar con rangos de predicci√≥n (ej: intervalo de confianza).  \n",
    "   - Mantener como referencia comparativa, no como √∫nico criterio de √©xito.  \n",
    "\n",
    "3. **¬øC√≥mo mediremos el impacto?**  \n",
    "   - Capacidad de anticipar apps con alta probabilidad de √©xito.  \n",
    "   - Ahorro de tiempo en validaciones preliminares.  \n",
    "   - Insights para desarrolladores sobre qu√© factores influyen m√°s en el rating.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2.2. Comprensi√≥n de los Datos  \n",
    "\n",
    "El objetivo de esta fase es explorar y entender el dataset de Google Play Store antes de construir modelos **(an√°lisis exploratorio)**.  \n",
    "Nos centraremos en:  \n",
    "\n",
    "1. **Vista r√°pida del dataset**  \n",
    "   - Identificar dimensiones (filas √ó columnas).  \n",
    "   - Tipos de datos (num√©ricos, categ√≥ricos, texto, fechas).  \n",
    "   - Valores faltantes obvios y rangos sospechosos.\n",
    "\n",
    "2. **Descripci√≥n de variables**  \n",
    "   - Revisar cada columna y entender su significado.  \n",
    "   - Detectar qu√© variables podr√≠an ser √∫tiles como predictores y cu√°l ser√° la variable objetivo (rating).  \n",
    "\n",
    "3. **Detecci√≥n de problemas en los datos**  \n",
    "   - An√°lisis de valores faltantes.  \n",
    "   - Estrategias: eliminar filas/columnas, imputar valores o crear indicadores de ‚Äúdato faltante‚Äù.  \n",
    "\n",
    "4. **Estad√≠sticas descriptivas y univariadas**  \n",
    "   - Media vs mediana (sesgo de la distribuci√≥n).  \n",
    "   - Desviaci√≥n est√°ndar (variabilidad, posibles outliers).  \n",
    "   - M√≠nimos/m√°ximos sospechosos.  \n",
    "   - Histogramas para ver forma (normal, sesgada, bimodal, uniforme, picos extra√±os).  \n",
    "\n",
    "5. **An√°lisis de variables categ√≥ricas**  \n",
    "   - Distribuci√≥n de categor√≠as (ej. categor√≠as de apps, tipo de app, content rating).  \n",
    "   - Detecci√≥n de clases dominantes o categor√≠as poco representadas.  \n",
    "\n",
    "6. **Correlaciones y relaciones entre variables**  \n",
    "   - Matriz de correlaci√≥n de Pearson para variables num√©ricas.  \n",
    "   - Identificar relaciones fuertes, moderadas o d√©biles.  \n",
    "   - Importante: recordar que **correlaci√≥n ‚â† causalidad**.  \n",
    "7. **An√°lisis de outliers**\n",
    "   -  Tipos e identificaci√≥n de outliers a trav√©s de diferentes m√©todos.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**  \n",
    "No siempre es necesario aplicar todos los pasos con igual profundidad.  \n",
    "- Para este proyecto, el foco est√° en **identificar variables relevantes para predecir el rating** y **limpiar datos inconsistentes**.  \n",
    "- Otros an√°lisis m√°s complejos (ej. NLP sobre descripciones) se pueden dejar como trabajo futuro (seg√∫n trabajos de referencia investigados).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2.1 Descarga de datos  \n",
    "\n",
    "En este paso descargamos el dataset de Google Play Store desde Kaggle y lo organizamos en la estructura de carpetas del proyecto.  \n",
    "\n",
    "1. Usamos la librer√≠a `kagglehub` para acceder al dataset p√∫blico **`lava18/google-play-store-apps`** directamente desde Kaggle.  \n",
    "2. Se define una ruta clara dentro del proyecto para almacenar los datos originales: `../data/original/google-play-store/`. Esto ayuda a mantener la reproducibilidad y una estructura organizada.  \n",
    "3. Con la funci√≥n `shutil.copytree` copiamos los archivos descargados a la carpeta destino. De esta forma, el dataset queda disponible en nuestro directorio de trabajo para su an√°lisis posterior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "def download_data(origin_repository, target_folder):\n",
    "    # Descargar dataset\n",
    "    path = kagglehub.dataset_download(origin_repository)\n",
    "    \n",
    "    # Copiar los archivos descargados\n",
    "    shutil.copytree(path, target_folder, dirs_exist_ok=True)\n",
    "    \n",
    "\n",
    "download_data(\"lava18/google-play-store-apps\", \"../../data/original/google-play-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2.2 Carga de datos  \n",
    "\n",
    "En este paso realizamos la **lectura del archivo CSV** que contiene el dataset descargado previamente.  \n",
    "\n",
    "- Definimos una funci√≥n `load_data(path, file)` que recibe la ruta y el nombre del archivo, y lo carga con `pandas.read_csv()`.  \n",
    "- Cargamos el dataset principal en la variable `applications_data` desde la carpeta `../data/original/google-play-store/`.  \n",
    "- Incluimos una verificaci√≥n simple:  \n",
    "  - Si el dataset se carga con √©xito, se imprime `\"Dataset loaded\"`.  \n",
    "  - En caso contrario, se muestra un mensaje de error.  \n",
    "\n",
    "Con esta validaci√≥n aseguramos que el archivo est√© disponible y correctamente le√≠do antes de continuar con el an√°lisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path, file):\n",
    "    return pd.read_csv(f\"{path}/{file}\")\n",
    "\n",
    "def convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte a num√©ricas solo las columnas que deber√≠an serlo, sin tocar 'Size'.\n",
    "    Usa to_numeric(errors='coerce') para evitar ValueError si aparece texto.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rating\n",
    "    if \"Rating\" in df.columns:\n",
    "        df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Reviews: quitar comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Reviews\" in df.columns:\n",
    "        df[\"Reviews\"] = (\n",
    "            df[\"Reviews\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Installs: quitar +, comas y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Installs\" in df.columns:\n",
    "        df[\"Installs Numeric\"] = (\n",
    "            df[\"Installs\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Price: quitar $ y cualquier car√°cter no num√©rico/punto\n",
    "    if \"Price\" in df.columns:\n",
    "        df[\"Price\"] = (\n",
    "            df[\"Price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "\n",
    "    if \"Size\" in df.columns:\n",
    "        def parse_size(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip()\n",
    "                if x.endswith(\"M\"):\n",
    "                    return float(x[:-1])\n",
    "                elif x.endswith(\"k\") or x.endswith(\"K\"):\n",
    "                    return float(x[:-1]) / 1024  # KB -> MB\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        df[\"Size\"] = df[\"Size\"].apply(parse_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "temp_applications_data = load_data(\"../../data/original/google-play-store\", \"googleplaystore.csv\")\n",
    "applications_data = convert_numeric_columns(temp_applications_data)\n",
    "\n",
    "\n",
    "if len(applications_data):\n",
    "    print(\"Dataset cargado\")\n",
    "else:\n",
    "    print(\"Error cargando dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2.3 Vista r√°pida del dataset\n",
    "\n",
    "**Dimensiones y columnas**\n",
    "- Registros: **10,841** filas.\n",
    "- Columnas: actualmente **14**; **originalmente eran 13** y se **a√±adi√≥** una columna derivada: **`Installs Numeric`** para an√°lisis con describe.\n",
    "- Memoria aproximada: **~1.2 MB**.\n",
    "\n",
    "**Tipos de datos (y transformaciones realizadas)**\n",
    "- Num√©ricas (`float64`): `Rating`, `Reviews`, `Size`, `Price`, **`Installs Numeric`**.\n",
    "- Categ√≥ricas / texto (`object`): `App`, `Category`, `Installs` *(forma original con ‚Äú1,000+‚Äù)*, `Type`, `Content Rating`, `Genres`, `Last Updated`, `Current Ver`, `Android Ver`.\n",
    "- Transformaciones ya aplicadas:\n",
    "  - **`Installs`** se **conserv√≥** en su formato original (categ√≥rico con ‚Äú+‚Äù y comas) **y** se cre√≥ **`Installs Numeric`** mapeando esos rangos a n√∫meros (0 ‚Ä¶ 1,000,000,000).\n",
    "  - **`Price`**, **`Reviews`** y **`Size`** fueron normalizadas/parseadas a **num√©rico** para an√°lisis y modelado.\n",
    "\n",
    "**Valores faltantes (no-null count ‚Üí faltantes aprox.)**\n",
    "- `Rating`: 9,367 ‚Üí **1,474 faltantes (~13.6%)**.\n",
    "- `Size`: 9,145 ‚Üí **1,696 faltantes (~15.6%)**.\n",
    "- `Current Ver`: 10,833 ‚Üí **8 faltantes (~0.07%)**.\n",
    "- `Android Ver`: 10,838 ‚Üí **3 faltantes (~0.03%)**.\n",
    "- `Content Rating`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Price`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- `Installs Numeric`: 10,840 ‚Üí **1 faltante (~0.01%)**.\n",
    "- Resto de columnas: **sin faltantes**.\n",
    "\n",
    "**Duplicados:**\n",
    "-   Se identificaron **483 filas duplicadas** (‚âà **4.46%** del\n",
    "    dataset).\\\n",
    "-   Ejemplos de duplicados incluyen apps como:\n",
    "    -   *Quick PDF Scanner + OCR FREE*\\\n",
    "    -   *Box*\\\n",
    "    -   *Google My Business*\\\n",
    "    -   *ZOOM Cloud Meetings*\\\n",
    "    -   *join.me -- Simple Meetings*\\\n",
    "\n",
    "**Rangos y valores sospechosos (seg√∫n `describe()`)**\n",
    "- `Rating`: **min = 1.0**, **max = 19.0** ‚Üí **19** es inv√°lido para la escala 1‚Äì5 (error de dato a corregir).\n",
    "- `Reviews`: media ~ **444k**, **p75 ‚âà 54,768**, **max ‚âà 78M** ‚Üí valores altos plausibles; tratar como **outliers**.\n",
    "- `Size` (MB): media ~ **21.5**, **p50 = 13**, **p75 = 30**, **max = 100** ‚Üí distribuci√≥n sesgada a la derecha; m√≠nimos muy bajos (**0.01**) a revisar.\n",
    "- `Price` (USD): **mediana = 0** y **p75 = 0** ‚Üí la mayor√≠a son **apps gratuitas**; **max = 400** sugiere outliers de precio.\n",
    "- `Installs Numeric`: **p25 = 1,000**, **p50 = 100,000**, **p75 = 5,000,000**, **max = 1,000,000,000** ‚Üí escala muy amplia; conviene usar **transformaciones log** o **binning** en el EDA/modelado.\n",
    "\n",
    "**Conclusi√≥n inicial**\n",
    "- Los **faltantes** m√°s relevantes est√°n en `Rating` y `Size`; habr√° que decidir estateg√≠a para aumentar, imputar o nivelar los datos.\n",
    "- Existen **outliers (no leg√≠timos)** (ej. `Rating = 19`) y variables con **colas largas** (ej. `Reviews`, `Installs Numeric`, `Price`).\n",
    "- Eliminar **duplicados** para evitar sesgos de an√°lisis y que no introduzcan ruidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(applications_data.head().style.background_gradient(cmap='RdYlGn', subset=['Rating']))\n",
    "\n",
    "# Informaci√≥n detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "applications_data.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(applications_data.describe().round(2).T)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar duplicados\n",
    "num_duplicados = applications_data.duplicated().sum()\n",
    "print(f\"Total de registros duplicados: {num_duplicados}\")\n",
    "\n",
    "# Mostrar ejemplos de duplicados si existen\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas:\\n\")\n",
    "    display(applications_data[applications_data.duplicated()].head())\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.4 Descripci√≥n de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price',\n",
    "    'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver',\n",
    "    'Installs Numeric'\n",
    "]\n",
    "\n",
    "tipos = [\n",
    "    'Categ√≥rica',          # App\n",
    "    'Categ√≥rica',          # Category\n",
    "    'Num√©rica (Target)',   # Rating\n",
    "    'Num√©rica',            # Reviews\n",
    "    'Num√©rica (MB)',       # Size\n",
    "    'Categ√≥rica (rango)',  # Installs\n",
    "    'Categ√≥rica',          # Type\n",
    "    'Num√©rica (USD)',      # Price\n",
    "    'Categ√≥rica',          # Content Rating\n",
    "    'Categ√≥rica',          # Genres\n",
    "    'Texto (fecha)',       # Last Updated (parseable a fecha)\n",
    "    'Texto',               # Current Ver\n",
    "    'Texto',               # Android Ver\n",
    "    'Num√©rica',            # Installs Numeric\n",
    "]\n",
    "\n",
    "descripciones = [\n",
    "    'Nombre de la aplicaci√≥n.',\n",
    "    'Categor√≠a oficial de la app en Google Play.',\n",
    "    'Calificaci√≥n promedio de usuarios (1 a 5).',\n",
    "    'N√∫mero de rese√±as reportadas.',\n",
    "    'Tama√±o aproximado de la app en MB.',\n",
    "    'Instalaciones en rango (p.ej., \"1,000+\").',\n",
    "    'Tipo de app (Free / Paid).',\n",
    "    'Precio en USD (0 para gratuitas).',\n",
    "    'Clasificaci√≥n de contenido (Everyone, Teen, etc.).',\n",
    "    'G√©nero(s) de la app.',\n",
    "    'Fecha de √∫ltima actualizaci√≥n (texto en origen).',\n",
    "    'Versi√≥n actual declarada por el desarrollador.',\n",
    "    'Versi√≥n m√≠nima de Android requerida.',\n",
    "    'Instalaciones convertidas a n√∫mero para an√°lisis.'\n",
    "]\n",
    "\n",
    "valores_faltantes = [applications_data[col].isnull().sum() if col in applications_data.columns else None for col in variables]\n",
    "\n",
    "metadata = {\n",
    "    'Variable': variables,\n",
    "    'Tipo': tipos,\n",
    "    'Descripci√≥n': descripciones,\n",
    "    'Valores Faltantes': valores_faltantes\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# Mostrar con resaltado de faltantes\n",
    "styled = df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x > 0 else '',\n",
    "    subset=['Valores Faltantes']\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Resumen de dtypes originales (informativo)\n",
    "dtypes_resumen = applications_data[variables].dtypes.astype(str).reset_index()\n",
    "dtypes_resumen.columns = ['Variable', 'dtype pandas']\n",
    "display(dtypes_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.2.5 Detecci√≥n de problemas en los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Resumen de hallazgos (valores faltantes):**\n",
    "- `Size` ‚âà 15.6% y `Rating` ‚âà 13.6% concentran la mayor√≠a de los faltantes.\n",
    "- Faltantes puntuales (‚âà0.01%): `Type`, `Price`, `Content Rating`, `Installs Numeric` ocurren en la misma(s) fila(s) ‚Üí patr√≥n conjunto.\n",
    "- `Android Ver` (0.03%) y `Current Ver` (0.07%) con faltantes residuales, parcialmente correlacionados con el grupo anterior.\n",
    "\n",
    "**Heatmap de correlaci√≥n de patrones de faltantes (interpretaci√≥n):**\n",
    "- Correlaci√≥n 1.00 entre `Type`, `Price`, `Content Rating`, `Installs Numeric`: las ausencias co-ocurren en el/los mismos registros. Acciones coordinadas.\n",
    "- `Android Ver` muestra correlaci√≥n moderada (~0.58) con ese grupo: algunas veces falta junto con ellos.\n",
    "- `Size`, `Rating`, `Current Ver` tienen patrones de faltantes independientes del grupo anterior (correlaciones cercanas a 0), lo que sugiere causas distintas.\n",
    "\n",
    "#### Posibles estrategias de correcci√≥n\n",
    "\n",
    "- Limpieza b√°sica\n",
    "  - Eliminar duplicados (483 filas) para evitar sesgos.\n",
    "  - Validar y corregir outliers imposibles, p. ej., `Rating = 19` ‚Üí convertir a NaN para tratarlo como faltante.\n",
    "\n",
    "- Imputaci√≥n (conservadora y por grupos)\n",
    "  - `Rating` (target): para modelado, eliminar filas sin `Rating`; para EDA descriptivo, imputar mediana por `Category` solo para visualizaci√≥n.\n",
    "  - `Size`: imputar mediana por `Category √ó Type` y crear indicador `size_missing`.\n",
    "  - `Android Ver`, `Current Ver`: imputar moda por `Category` y crear indicadores `androidver_missing`, `currentver_missing`.\n",
    "  - Faltantes conjuntos (`Type`, `Price`, `Content Rating`, `Installs Numeric`):\n",
    "    - Si es 1 fila: eliminarla es lo m√°s simple y seguro.\n",
    "    - Alternativa (si se prefiere imputar):\n",
    "      - `Type`: inferir desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid).\n",
    "      - `Price`: 0 si `Type == Free`, si `Paid` usar mediana por `Category`.\n",
    "      - `Content Rating`: moda por `Category`.\n",
    "      - `Installs Numeric`: mediana por `Category √ó Type` o por bin de `Installs`.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategias de ‚Äúnivelaci√≥n‚Äù seg√∫n los porcentajes observados\n",
    "\n",
    "- Size (~15.6% faltantes, >5% y <<60%)\n",
    "  - Acci√≥n: imputar mediana por grupo `Category √ó Type`.\n",
    "  - A√±adir flag: `size_missing = 1` cuando falte (conserva se√±al de ausencia).\n",
    "  - Justificaci√≥n: volumen relevante; la mediana por grupos respeta diferencias entre tipos/categor√≠as.\n",
    "\n",
    "- Rating (~13.6% faltantes, >5% y <<60%) [variable objetivo]\n",
    "  - Para modelado: eliminar filas sin `Rating` (evita sesgo por imputaci√≥n del target).\n",
    "  - Para EDA descriptivo: si se requiere visualizar completos, imputar mediana por `Category` solo para gr√°ficos/tablas (no para entrenamiento).\n",
    "  - Justificaci√≥n: imputar el target puede distorsionar m√©tricas.\n",
    "\n",
    "- Current Ver (0.07%) y Android Ver (0.03%) (<5%)\n",
    "  - Acci√≥n: imputar con la moda por `Category`. Flags opcionales `currentver_missing` y `androidver_missing`.\n",
    "  - Justificaci√≥n: impacto √≠nfimo; moda es suficiente y estable.\n",
    "\n",
    "- Faltantes ‚Äúen bloque‚Äù en la misma fila: Type, Price, Content Rating, Installs Numeric (‚âà0.01% cada uno; correlaci√≥n 1.00)\n",
    "  - Si es 1 fila: eliminarla directamente.\n",
    "  - Si hubiera m√°s en el futuro y se prefiriera imputar coordinadamente:\n",
    "    - `Type` desde `Price` (0 ‚Üí Free, >0 ‚Üí Paid),\n",
    "    - `Price` = 0 si `Free`, si `Paid` usar mediana por `Category`,\n",
    "    - `Content Rating` = moda por `Category`,\n",
    "    - `Installs Numeric` = mediana por `Category √ó Type`.\n",
    "  - Justificaci√≥n: co-ocurren; eliminar 1 fila no afecta el conjunto y evita inconsistencias.\n",
    "\n",
    "- Transformaciones para estabilizar distribuciones (complementarias a la imputaci√≥n)\n",
    "  - `Reviews` y `Installs Numeric`: aplicar `log1p` para an√°lisis y futuros modelos. *****************************\n",
    "  - `Installs` (rangos): tratar como ordinal/bins en el EDA.\n",
    "\n",
    "- Limpieza previa necesaria\n",
    "  - Eliminar duplicados (483 filas).\n",
    "  - Corregir valores imposibles detectados en el EDA (ej. `Rating = 19` ‚Üí NaN) y re-entrar al flujo de imputaci√≥n/nivelaci√≥n anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"An√°lisis completo de valores faltantes con visualizaciones.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': missing_counts.values,\n",
    "        'Porcentaje': missing_pct.values,\n",
    "        'Tipo_Dato': df.dtypes.values\n",
    "    })\n",
    "\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "    if len(missing_df) == 0:\n",
    "        print(\"No hay valores faltantes en el dataset\")\n",
    "        return missing_df\n",
    "\n",
    "    # Visualizaci√≥n: barras y correlaci√≥n de patrones de faltantes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Gr√°fico de barras de % faltantes\n",
    "    ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "    ax1.set_xlabel('Columna')\n",
    "    ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "    ax1.set_title('Valores Faltantes por Columna')\n",
    "    ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "    ax1.axhline(y=60, color='purple', linestyle='--', label='Umbral 60%')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Heatmap de correlaci√≥n de patrones de faltantes\n",
    "    mask_df = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "    if mask_df.shape[1] >= 2:\n",
    "        corr = mask_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "        ax2.set_title('Correlaci√≥n de Patrones de Valores Faltantes')\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Correlaci√≥n de faltantes (no aplica: 1 columna)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "missing_analysis = analyze_missing_values(applications_data)\n",
    "if missing_analysis is not None and not missing_analysis.empty:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2.6 Estadisticas descriptivas y univariadas (n√∫merico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A partir de la tabla de estad√≠sticas y los gr√°ficos generados para `Rating`, `Reviews`, `Size`, `Price` e `Installs Numeric`, se observan los siguientes puntos clave.\n",
    "\n",
    "- Rating\n",
    "  - Media ‚âà 4.19 y mediana ‚âà 4.30 ‚Üí ligera cola a la izquierda (m√°s apps con rating alto). Hay un valor imposible (‚âà19), confirmado en el boxplot/Q-Q como outlier extremo.\n",
    "  - Outliers: ~5% por IQR, dominados por el valor inv√°lido y algunos ratings bajos.\n",
    "  - Q-Q plot: desviaci√≥n frente a normalidad, esperable para una variable acotada [1,5].\n",
    "  - Implicaci√≥n/acci√≥n: eliminar filas sin `Rating` para modelado; corregir `Rating=19 ‚Üí NaN` y excluir; no aplicar transformaciones (la escala es ya interpretables).\n",
    "\n",
    "- Reviews\n",
    "  - Media ‚â´ mediana (pico en 0‚Äìpocos miles; m√°ximo ‚âà 78M) ‚Üí cola muy larga a la derecha.\n",
    "  - Boxplot: ~18% outliers por IQR (muchas apps con rese√±as muy altas).\n",
    "  - Q-Q plot: gran desviaci√≥n de normalidad (heavy tail).\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva muy d√©bil (~0.07), tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Reviews)` para estabilizar la distribuci√≥n en an√°lisis/modelado; considerar winsorizar p99.9 para vistas tabulares si se desea.\n",
    "\n",
    "- Size (MB)\n",
    "  - Media > mediana (‚âà 21.5 vs 13) ‚Üí sesgo a la derecha; valores hasta 100 MB.\n",
    "  - ~6% outliers por IQR, especialmente en colas altas.\n",
    "  - Q-Q plot: curvatura en colas; no normal.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n positiva d√©bil (~0.08); se√±al muy tenue.\n",
    "  - Implicaci√≥n/acci√≥n: imputar faltantes por `Category √ó Type` y a√±adir `size_missing`; opcionalmente probar `log1p(Size)` o binning para robustecer.\n",
    "\n",
    "- Price (USD)\n",
    "  - Mediana = 0 (mayor√≠a gratis) y cola a la derecha con m√°ximos altos (‚âà 400).\n",
    "  - ~7% outliers por IQR; Q-Q muestra heavy tail.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n negativa muy d√©bil (~-0.02).\n",
    "  - Implicaci√≥n/acci√≥n: crear `is_free = (Price == 0)` y, si se usa `Price` continuo, considerar `log1p(Price)` para las pocas apps pagas; validar coherencia `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Installs Numeric\n",
    "  - Media ‚â´ mediana (100k) con m√°ximo 1e9 ‚Üí distribuci√≥n extremadamente sesgada a la derecha.\n",
    "  - ~7‚Äì8% outliers por IQR; Q-Q muy alejado de normalidad.\n",
    "  - Relaci√≥n con Rating: correlaci√≥n d√©bil positiva (~0.05) y tendencia casi plana.\n",
    "  - Implicaci√≥n/acci√≥n: usar `log1p(Installs Numeric)` o bins ordinales para an√°lisis; verificar coherencia con `Installs` textual.\n",
    "\n",
    "Recomendaciones transversales\n",
    "- Eliminar duplicados antes de resumir para evitar sesgos.\n",
    "- Tratar outliers evidentes no-leg√≠timos (p. ej. `Rating=19`). Para colas largas leg√≠timas (`Reviews`, `Installs Numeric`, `Price`): preferir `log1p` o winsorizaci√≥n solo para visualizaciones.\n",
    "- Mantener consistencia: `Type=Free ‚áí Price=0`; `Installs Numeric` coherente con el rango de `Installs`.\n",
    "- Para relaciones con `Rating`, las correlaciones lineales observadas son d√©biles; la se√±al puede emerger mejor con interacciones (p. ej., `is_free √ó installs_bin`) o modelos no lineales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selecci√≥n de columnas num√©ricas relevantes\n",
    "numeric_cols = [c for c in ['Rating', 'Reviews', 'Size', 'Price', 'Installs Numeric'] if c in applications_data.columns]\n",
    "\n",
    "# Tabla de estad√≠sticas b√°sicas (media, mediana, std, min, p25, p50, p75, max)\n",
    "describe_tbl = applications_data[numeric_cols].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "# M√©tricas adicionales robustas\n",
    "extra = pd.DataFrame(index=numeric_cols)\n",
    "extra['mad'] = [stats.median_abs_deviation(applications_data[c].dropna()) for c in numeric_cols]\n",
    "extra['skew'] = [applications_data[c].skew(skipna=True) for c in numeric_cols]\n",
    "extra['kurtosis'] = [applications_data[c].kurtosis(skipna=True) for c in numeric_cols]\n",
    "extra['cv'] = [applications_data[c].std(skipna=True) / applications_data[c].mean(skipna=True) if applications_data[c].mean(skipna=True) not in [0, np.nan] else np.nan for c in numeric_cols]\n",
    "\n",
    "stats_table = describe_tbl.join(extra)\n",
    "display(stats_table.round(3))\n",
    "\n",
    "\n",
    "def univariate_analysis(df: pd.DataFrame, column: str, target: str | None = None):\n",
    "    \"\"\"An√°lisis univariado con histograma, boxplot, Q-Q plot y relaci√≥n con target.\"\"\"\n",
    "    series = df[column].dropna()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Histograma con l√≠neas de media y mediana\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(series, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(series.mean(), color='red', linestyle='--', label=f\"Media: {series.mean():.2f}\")\n",
    "    ax1.axvline(series.median(), color='green', linestyle='--', label=f\"Mediana: {series.median():.2f}\")\n",
    "    ax1.set_title(f\"Distribuci√≥n de {column}\")\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2) Boxplot + conteo de outliers (IQR)\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(series, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)\n",
    "    n_out = int(outliers_mask.sum())\n",
    "    pct_out = 100 * n_out / len(series) if len(series) else 0\n",
    "    ax2.set_title(f\"Boxplot de {column}\")\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.text(1.1, Q3, f\"Outliers: {n_out} ({pct_out:.1f}%)\", fontsize=10)\n",
    "\n",
    "    # 3) Q-Q plot normal\n",
    "    ax3 = axes[1, 0]\n",
    "    stats.probplot(series, dist='norm', plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # 4) Relaci√≥n con target si aplica\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns and column != target:\n",
    "        valid = df[[column, target]].dropna()\n",
    "        ax4.scatter(valid[column], valid[target], alpha=0.4, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f\"{column} vs {target}\")\n",
    "        # L√≠nea de tendencia (ajuste lineal simple)\n",
    "        if len(valid) > 1:\n",
    "            z = np.polyfit(valid[column], valid[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            xs = np.linspace(valid[column].min(), valid[column].max(), 200)\n",
    "            ax4.plot(xs, p(xs), 'r--', alpha=0.8, label='Tendencia')\n",
    "            corr = valid[column].corr(valid[target])\n",
    "            ax4.text(0.05, 0.95, f\"Correlaci√≥n: {corr:.3f}\", transform=ax4.transAxes,\n",
    "                     fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            ax4.legend()\n",
    "    else:\n",
    "        ax4.axis('off')\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"An√°lisis Univariado: {column}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar an√°lisis univariado para cada m√©trica num√©rica, relacionando con Rating\n",
    "for col in numeric_cols:\n",
    "    univariate_analysis(applications_data, col, target='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.2.7. An√°lisis Univariado Categ√≥rico\n",
    "- Category\n",
    "  - Distribuci√≥n: alta concentraci√≥n en `FAMILY` (~19%) y `GAME` (~12%). El resto de categor√≠as tienen menor peso individual; el grupo `Others` acumula ~31% del total.\n",
    "  - Rating por categor√≠a: diferencias moderadas; la **mediana** suele estar entre 4.2‚Äì4.4. Algunas categor√≠as muestran desviaci√≥n est√°ndar mayor (p. ej., `PRODUCTIVITY`, `LIFESTYLE`), indicando m√°s variabilidad de valoraci√≥n.\n",
    "  - Implicaciones: riesgo de sesgo por categor√≠as mayoritarias en an√°lisis agregados. Para modelado, conviene usar dummies Top-K o codificaci√≥n ordinal/target encoding con cuidado (evitar fuga). Agrupar colas largas en `Others` es adecuado para visualizaci√≥n.\n",
    "\n",
    "- Content Rating\n",
    "  - Distribuci√≥n: `Everyone` domina (~79%), seguido por `Teen` (~12%); `Mature 17+` y `Everyone 10+` suman ~9% en conjunto; clases raras casi nulas.\n",
    "  - Rating por nivel de contenido: medias similares (‚âà4.1‚Äì4.3). `Teen` tiende a mediana 4.3 y variabilidad algo menor; `Mature 17+` muestra algo m√°s de dispersi√≥n.\n",
    "  - Implicaciones: por el fuerte desbalance, esta variable aporta se√±al limitada por s√≠ sola. √ötil como interacci√≥n con `Category`/`Genres`.\n",
    "\n",
    "- Type\n",
    "  - Distribuci√≥n: `Free` ‚âà 93%, `Paid` ‚âà 7% (clase muy desbalanceada); existe un registro an√≥malo (valor 0) en los gr√°ficos que debe eliminarse/corregirse.\n",
    "  - Rating por tipo: medias muy cercanas (Free ‚âà 4.19, Paid ‚âà 4.27). La diferencia es peque√±a y probablemente no significativa sin controlar otras variables (p. ej., `Category`).\n",
    "  - Implicaciones: por el desbalance extremo, conviene usar `is_free` como binaria y, si se modela interacci√≥n con `Installs` o `Price`, puede emerger se√±al. Validar regla `Type=Free ‚áí Price=0`.\n",
    "\n",
    "- Genres Main (primer g√©nero)\n",
    "  - Distribuci√≥n: gran cola larga; `Others` concentra ~48%. Entre Top-12, `Tools`, `Entertainment` y `Education` destacan en frecuencia.\n",
    "  - Rating por g√©nero: diferencias peque√±as (medianas ~4.2‚Äì4.4), con algunas variaciones en dispersi√≥n (p. ej., `Medical` y `Lifestyle` m√°s variables).\n",
    "  - Implicaciones: por la alta cardinalidad y colas largas, mantener Top-K + `Others` en EDA ayuda a la legibilidad. Para modelado, preferir codificaci√≥n que reduzca dimensionalidad (Top-K dummies, hashing, o target encoding con validaci√≥n adecuada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_compact(df: pd.DataFrame, cat_col: str, target_col: str, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Versi√≥n compacta para variables con muchas categor√≠as:\n",
    "    - Ordena por frecuencia, muestra top_n y agrupa el resto en \"Others\".\n",
    "    - Barras horizontales, pie chart compacto, boxplot y tabla para top_n.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, target_col]].dropna(subset=[cat_col, target_col]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"Sin datos para {cat_col} y {target_col}\")\n",
    "        return\n",
    "\n",
    "    counts = data[cat_col].value_counts()\n",
    "    top_cats = counts.head(top_n)\n",
    "    others_count = counts.iloc[top_n:].sum()\n",
    "\n",
    "    # Mapeo a top_n + Others\n",
    "    mapping = {c: c for c in top_cats.index}\n",
    "    data['__cat__'] = data[cat_col].where(data[cat_col].isin(top_cats.index), other='Others')\n",
    "\n",
    "    # Recalcular conteos con Others\n",
    "    counts_compact = data['__cat__'].value_counts()\n",
    "    order = list(top_cats.index) + (['Others'] if 'Others' in counts_compact.index else [])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Barras horizontales (mejor legibilidad)\n",
    "    ax1 = axes[0, 0]\n",
    "    vals = counts_compact.loc[order]\n",
    "    ax1.barh(range(len(vals)), vals.values, color=plt.cm.Set3(range(len(vals))))\n",
    "    ax1.set_yticks(range(len(vals)))\n",
    "    ax1.set_yticklabels(order)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title(f'Distribuci√≥n (Top {top_n}) de {cat_col}')\n",
    "    ax1.set_xlabel('Frecuencia')\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax1.text(v, i, f'  {v} ({v/len(data)*100:.1f}%)', va='center')\n",
    "\n",
    "    # 2) Pie chart compacto\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(vals.values, labels=order, autopct='%1.1f%%', startangle=140,\n",
    "            colors=plt.cm.Set3(range(len(vals))))\n",
    "    ax2.set_title(f'Proporci√≥n (Top {top_n} + Others) de {cat_col}')\n",
    "\n",
    "    # 3) Boxplot del target por categor√≠a (solo top_n)\n",
    "    ax3 = axes[1, 0]\n",
    "    top_mask = data['__cat__'] != 'Others'\n",
    "    data_top = data[top_mask]\n",
    "    data_top.boxplot(column=target_col, by='__cat__', ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col} (Top {top_n})')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # 4) Tabla de estad√≠sticas por categor√≠a (solo top_n y Others si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    stats_by_cat = data.groupby('__cat__')[target_col].agg(['count', 'mean', 'median', 'std']).loc[order].round(2)\n",
    "    table = ax4.table(cellText=stats_by_cat.reset_index().values,\n",
    "                      colLabels=['Categor√≠a', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                      cellLoc='center', loc='center', colWidths=[0.35, 0.12, 0.16, 0.16, 0.16])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.05, 1.25)\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.suptitle(f'An√°lisis Categ√≥rico Compacto: {cat_col}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar la versi√≥n compacta para las categ√≥ricas clave\n",
    "for cat in [c for c in ['Category', 'Content Rating', 'Type', 'Genres Main', 'Installs'] if c in applications_data.columns]:\n",
    "    analyze_categorical_compact(applications_data, cat, 'Rating', top_n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 2.2.8. An√°lisis de correlaci√≥n entre variables\n",
    "\n",
    "#### 2.2.8.1. Variables con Mayor Relaci√≥n\n",
    "- Existe una fuerte correlaci√≥n positiva entre **Installs Numeric** y **Reviews**:\n",
    "  - Pearson: 0.64 (relaci√≥n lineal moderada-fuerte).\n",
    "  - Spearman: 0.97 (relaci√≥n mon√≥tonica muy fuerte).\n",
    "- Esto implica que a mayor n√∫mero de instalaciones, mayor n√∫mero de rese√±as.\n",
    "\n",
    "#### 2.2.8.2. Correlaci√≥n de Pearson\n",
    "- En general, las correlaciones de Pearson muestran relaciones m√°s d√©biles que Spearman, lo cual indica que las relaciones lineales no son tan marcadas.\n",
    "- **Installs Numeric y Reviews** presentan la correlaci√≥n lineal m√°s alta (0.64), siendo moderada-fuerte.\n",
    "- **Size y Reviews** muestran una correlaci√≥n positiva baja/D√©bil (0.24).\n",
    "- El resto de variables (Rating, Price) tienen correlaciones casi nulas con las dem√°s, lo que refleja poca relaci√≥n lineal.\n",
    "\n",
    "#### 2.2.8.3. Correlaci√≥n de Spearman\n",
    "- **Installs Numeric y Reviews** tienen la correlaci√≥n m√°s fuerte (0.97).\n",
    "- **Size** muestra correlaci√≥n moderada con **Reviews** (0.37) y con **Installs Numeric** (0.35).\n",
    "- **Price** presenta correlaciones negativas con **Reviews** (-0.17) e **Installs Numeric** (-0.24).\n",
    "\n",
    "#### 2.2.8.4. Observaciones Clave\n",
    "- El n√∫mero de instalaciones y las rese√±as son las variables m√°s relacionadas, lo cual es l√≥gico, ya que m√°s usuarios generan m√°s interacciones.\n",
    "- El tama√±o de la aplicaci√≥n influye ligeramente en rese√±as e instalaciones, pero no de forma determinante.\n",
    "- El precio no solo carece de relaci√≥n positiva, sino que parece tener un impacto negativo sobre la popularidad (menos instalaciones y rese√±as).\n",
    "\n",
    "#### 2.2.8.5. Conclusi√≥n\n",
    "- **Installs Numeric** y **Reviews** son las m√©tricas m√°s cr√≠ticas en el dataset de **Google Play Store**, ya que reflejan el √©xito y la popularidad de la aplicaci√≥n.\n",
    "- **Size** es un factor secundario con cierta relaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaci√≥n mejorado para el proyecto de Google Play Store\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"An√°lisis de correlaci√≥n con m√∫ltiples m√©tricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlaci√≥n de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlaci√≥n de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlaci√≥n de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlaci√≥n de Spearman (Monot√≥nica)')\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Correlaci√≥n Multi-m√©trica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\nüîó Correlaciones Significativas:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, corr_matrix in zip(['Pearson', 'Spearman'], [corr_pearson, corr_spearman]):\n",
    "        print(f\"\\n{method}:\")\n",
    "        significant_corr = corr_matrix[(abs(corr_matrix) > 0.3) & (corr_matrix != 1)].stack()\n",
    "        for (var1, var2), corr in significant_corr.items():\n",
    "            strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"D√©bil\"\n",
    "            direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "            print(f\"  ‚Ä¢ {var1} y {var2}: {corr:+.3f} ({strength} {direction})\")\n",
    "    \n",
    "# Ejecutar el an√°lisis de correlaci√≥n\n",
    "correlation_analysis(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.2.9. An√°lisis de Outliers (IQR, Z-Score e Isolation Forest)\n",
    "**Resumen cuantitativo**\n",
    "- Total de registros analizados: **10,841**.\n",
    "- Filas marcadas como outlier por m√©todo:\n",
    "  - IQR: **3,489** filas (32.18%) ‚Üí refleja colas largas especialmente en `Reviews`, `Installs Numeric`, `Price`.\n",
    "  - Z-Score (> |3|): **654** filas (6.03%) ‚Üí mucho m√°s selectivo, captura extremos verdaderamente alejados tras estandarizaci√≥n.\n",
    "  - Isolation Forest (contaminaci√≥n=10%): **1,084** filas (10.0%) ‚Üí patr√≥n no lineal de anomal√≠as combinadas.\n",
    "- Consenso entre m√©todos:\n",
    "  - Detectadas por los 3 m√©todos: **502** filas (casos altamente an√≥malos).\n",
    "  - Detectadas exactamente por 2 m√©todos: **731** filas (an√≥malas consistentes, revisar antes de decidir acci√≥n).\n",
    "\n",
    "**Variables m√°s afectadas (IQR)**\n",
    "- `Reviews`: **1,925** outliers ‚Üí distribuci√≥n extremadamente sesgada; valores muy altos representan apps masivas (probablemente leg√≠timos).\n",
    "- `Installs Numeric`: **828** outliers ‚Üí escalas de descargas masivas (1e7‚Äì1e9).\n",
    "- `Price`: **800** outliers ‚Üí pocos productos de precio elevado (‚â• p75 + 1.5¬∑IQR); revisar si son apps premium leg√≠timas.\n",
    "- `Size`: **564** outliers ‚Üí tama√±os extremos (muy grandes o inusualmente peque√±os).\n",
    "- `Rating`: **504** outliers ‚Üí incluye valores extremos bajos y el caso inv√°lido (`Rating=19`).\n",
    "\n",
    "**Interpretaci√≥n y criterios**\n",
    "- Muchos outliers provienen de fen√≥menos de cola larga t√≠picos (popularidad extrema o modelo freemium/premium).\n",
    "- No se recomienda eliminar masivamente outliers de `Reviews` o `Installs Numeric` sin antes transformar (`log1p`) o agrupar (binning), para no perder informaci√≥n sobre apps exitosas.\n",
    "- El valor inv√°lido `Rating` debe eliminars. Otros ratings muy bajos pueden mantenerse (aportan contraste) (perfecto si se encuentran entre 1 y 5).\n",
    "- Outliers en `Price` podr√≠an segmentarse: gratis (0), bajo costo (0 < p ‚â§ 10), premium (10 < p ‚â§ 50), ultra premium (>50).\n",
    "\n",
    "\n",
    "**Conclusi√≥n**\n",
    "El comportamiento extremo de `Reviews` e `Installs Numeric` refleja la naturaleza desigual del mercado (unas pocas apps concentran gran parte de la atenci√≥n). Un manejo cuidadoso (transformaciones y flags) preservar√° informaci√≥n √∫til sin distorsionar el entrenamiento. Se prioriza limpieza puntual (ratings inv√°lidos) sobre eliminaci√≥n agresiva de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detecci√≥n de outliers usando m√∫ltiples m√©todos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # M√©todo 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # M√©todo 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # M√©todo 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (M√©todo IQR)')\n",
    "    ax1.set_ylabel('N√∫mero de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribuci√≥n de outliers por m√©todo\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparaci√≥n de M√©todos de Detecci√≥n')\n",
    "    ax2.set_ylabel('N√∫mero de Outliers Detectados')\n",
    "    ax2.set_xlabel('M√©todo')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estad√≠stico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detecci√≥n de Anomal√≠as:\n",
    "    \n",
    "    ‚Ä¢ Total de observaciones: {len(df):,}\n",
    "    ‚Ä¢ Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    ‚Ä¢ Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables m√°s afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Investigar outliers antes de eliminar. \n",
    "    Pueden contener informaci√≥n valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('An√°lisis de Outliers y Anomal√≠as', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "# Ejecutar la detecci√≥n de outliers en el dataset de aplicaciones\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 2.3. Preparaci√≥n de los datos\n",
    "\n",
    "Con base a todos los hallazgos del **An√°lisis Exploratorio de Datos (EDA)**, aplicaremos las siguientes t√©cnicas de limpieza y transformaci√≥n:\n",
    "\n",
    "### 2.3.1. Resumen de problemas detectados\n",
    "\n",
    "Durante el EDA identificamos:\n",
    "\n",
    "1. **Duplicados**: 483 filas duplicadas (~4.46%)\n",
    "2. **Valores imposibles**: Rating = 19 (fuera del rango 1-5)\n",
    "3. **Valores faltantes**: \n",
    "   - Size ‚âà 15.6%\n",
    "   - Rating ‚âà 13.6%\n",
    "   - Current Ver, Android Ver, Content Rating, Type, Price (<1%)\n",
    "4. **Outliers leg√≠timos**: Distribuciones con colas largas en Reviews, Installs Numeric, Price, Size\n",
    "5. **Variables con distribuciones sesgadas**: Requieren transformaciones logar√≠tmicas\n",
    "6. **Inconsistencias**: Necesidad de validar coherencia entre Type y Price\n",
    "\n",
    "### 2.3.2. Plan de transformaci√≥n\n",
    "\n",
    "Aplicaremos las siguientes transformaciones en orden:\n",
    "\n",
    "1. **Eliminaci√≥n de duplicados**\n",
    "2. **Correcci√≥n de valores imposibles**\n",
    "3. **Imputaci√≥n de valores faltantes** (estrategia por variable)\n",
    "4. **Validaci√≥n de consistencia** entre variables relacionadas\n",
    "5. **Transformaciones de variables num√©ricas** (log, binning)\n",
    "6. **Creaci√≥n de variables derivadas** (features engineering b√°sico)\n",
    "7. **Resumen final** del dataset limpio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 2.3.3. Eliminaci√≥n de Duplicados\n",
    "\n",
    "Eliminamos las **483 filas duplicadas** detectadas en el EDA para evitar:\n",
    "- Sesgos en an√°lisis estad√≠sticos\n",
    "- Sobrepeso de ciertas apps en el modelado\n",
    "- Distorsi√≥n de m√©tricas de evaluaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset para transformaciones\n",
    "df_clean = applications_data.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 1: ELIMINACI√ìN DE DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado inicial\n",
    "print(f\"\\nRegistros antes de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"Duplicados encontrados: {df_clean.duplicated().sum():,} ({df_clean.duplicated().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de duplicados antes de eliminar\n",
    "if df_clean.duplicated().sum() > 0:\n",
    "    print(\"\\nEjemplos de aplicaciones duplicadas:\")\n",
    "    duplicated_apps = df_clean[df_clean.duplicated(keep=False)].sort_values('App')\n",
    "    display(duplicated_apps[['App', 'Category', 'Rating', 'Reviews', 'Installs']].head(10))\n",
    "\n",
    "# Eliminar duplicados (manteniendo la primera ocurrencia)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Estado final\n",
    "print(f\"\\nRegistros despu√©s de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"Filas eliminadas: {len(applications_data) - len(df_clean):,}\")\n",
    "print(f\"Reducci√≥n: {((len(applications_data) - len(df_clean))/len(applications_data)*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 2.3.4. Correcci√≥n de Valores Imposibles\n",
    "\n",
    "Corregimos valores que est√°n fuera del rango v√°lido:\n",
    "- Cualquier Rating < 1 o > 5 ‚Üí Eliminar outlier, dado que es imposible que en la escala de Google Play se pueda obtener este tipo de rango, lo que demuestra un verdadero error\n",
    "\n",
    "#### 2.3.4.1. An√°lisis de Resultados de Correcci√≥n\n",
    "\n",
    "**Valores imposibles detectados:**\n",
    "\n",
    "**Ratings inv√°lidos identificados:**\n",
    "- **1 registro** con Rating = 19.0 (fuera del rango v√°lido 1-5)\n",
    "- **Aplicaci√≥n afectada:** Identificada y eliminada del dataset\n",
    "- **Acci√≥n aplicada:** Eliminaci√≥n completa del registro (no conversi√≥n a NaN)\n",
    "\n",
    "**Verificaci√≥n de otros valores imposibles:**\n",
    "- **Reviews**: Sin valores negativos detectados\n",
    "- **Size**: Sin valores negativos detectados  \n",
    "- **Price**: Sin valores negativos detectados\n",
    "- **Installs Numeric**: Sin valores negativos detectados\n",
    "\n",
    "**Impacto de la correcci√≥n:**\n",
    "- **Registros eliminados:** 1 (impacto m√≠nimo del 0.01%)\n",
    "- **Integridad del Rating:** 100% de valores dentro del rango [1-5]\n",
    "- **Registros finales:** 10,357 (despu√©s de eliminaci√≥n de duplicados y valores imposibles)\n",
    "\n",
    "**Distribuci√≥n final de Rating (valores v√°lidos):**\n",
    "- **Rango confirmado:** 1.0 - 5.0 ‚úì\n",
    "- **Mediana:** ~4.3 (preservada despu√©s de la limpieza)\n",
    "- **Sin outliers imposibles:** Dataset completamente saneado\n",
    "\n",
    "**Conclusi√≥n:** La correcci√≥n fue exitosa eliminando el √∫nico valor imposible detectado (Rating=19), garantizando que todos los ratings restantes cumplan con la escala est√°ndar de Google Play Store [1-5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 2: CORRECCI√ìN DE VALORES IMPOSIBLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar valores de Rating fuera del rango [1, 5]\n",
    "invalid_ratings = df_clean['Rating'][(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5)]\n",
    "print(f\"\\nRatings inv√°lidos encontrados: {len(invalid_ratings)}\")\n",
    "\n",
    "if len(invalid_ratings) > 0:\n",
    "    print(\"\\nEjemplos de ratings inv√°lidos:\")\n",
    "    invalid_apps = df_clean[df_clean['Rating'].isin(invalid_ratings)]\n",
    "    display(invalid_apps[['App', 'Category', 'Rating', 'Reviews']].head())\n",
    "    \n",
    "    # Mostrar distribuci√≥n de valores inv√°lidos\n",
    "    print(f\"\\nValores inv√°lidos √∫nicos: {sorted(invalid_ratings.dropna().unique())}\")\n",
    "    \n",
    "    # Eliminar registros con valores inv√°lidos en lugar de poner NaN\n",
    "    df_clean = df_clean[~((df_clean['Rating'] < 1) | (df_clean['Rating'] > 5))].copy()\n",
    "    \n",
    "    print(f\"\\nRegistros inv√°lidos eliminados.\")\n",
    "    print(f\"Total de registros ahora: {len(df_clean)}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron ratings inv√°lidos\")\n",
    "\n",
    "# Verificar otros valores imposibles (negativos en columnas que no pueden serlo)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificando valores negativos en columnas num√©ricas:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numeric_cols = ['Reviews', 'Size', 'Price', 'Installs Numeric']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"{col}: {negative_count} valores negativos encontrados\")\n",
    "            df_clean = df_clean[df_clean[col] >= 0].copy()\n",
    "            print(f\"Registros inv√°lidos eliminados de {col}\")\n",
    "        else:\n",
    "            print(f\"{col}: Sin valores negativos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2.3.5. Validaci√≥n de Consistencia entre Variables\n",
    "\n",
    "Validamos y corregimos inconsistencias l√≥gicas entre variables relacionadas:\n",
    "- **Type vs Price**: Si `Type = 'Free'`, entonces `Price` debe ser 0\n",
    "- **Type vs Price**: Si `Price > 0`, entonces `Type` debe ser 'Paid'\n",
    "\n",
    "#### 2.3.5.1. An√°lisis de Resultados de Validaci√≥n\n",
    "\n",
    "**Estado de consistencia encontrado:**\n",
    "\n",
    "**Consistencia perfecta en apps existentes:**\n",
    "- **0 apps** marcadas como 'Free' con Price > 0\n",
    "- **0 apps** marcadas como 'Paid' con Price = 0\n",
    "- El dataset original ya manten√≠a la l√≥gica de negocio correcta\n",
    "\n",
    "**Correcci√≥n aplicada:**\n",
    "- **1 registro** con Type faltante fue corregido autom√°ticamente\n",
    "- **Estrategia:** Inferencia desde Price (Price = 0 ‚Üí Type = 'Free')\n",
    "- **Resultado:** 0 faltantes restantes en Type\n",
    "\n",
    "**Distribuci√≥n final validada:**\n",
    "- **Free**: 9,592 apps (92.6%)\n",
    "- **Paid**: 765 apps (7.4%)\n",
    "- **Total**: 10,357 apps con consistencia 100% validada\n",
    "\n",
    "**Conclusi√≥n:** La validaci√≥n confirm√≥ que el dataset original ya manten√≠a coherencia l√≥gica entre Type y Price, requiriendo solo la correcci√≥n menor de 1 registro con Type faltante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 3: VALIDACI√ìN DE CONSISTENCIA ENTRE VARIABLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validar consistencia Type vs Price\n",
    "print(\"\\nValidando consistencia entre Type y Price:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Casos inconsistentes: Type='Free' pero Price > 0\n",
    "free_but_paid = df_clean[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0)]\n",
    "print(f\"\\nApps marcadas como 'Free' pero con Price > 0: {len(free_but_paid)}\")\n",
    "if len(free_but_paid) > 0:\n",
    "    display(free_but_paid[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price > 0, cambiar Type a 'Paid'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    print(f\"Corregido: Type cambiado a 'Paid'\")\n",
    "\n",
    "# Casos inconsistentes: Type='Paid' pero Price = 0\n",
    "paid_but_free = df_clean[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0)]\n",
    "print(f\"\\nApps marcadas como 'Paid' pero con Price = 0: {len(paid_but_free)}\")\n",
    "if len(paid_but_free) > 0:\n",
    "    display(paid_but_free[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price = 0, cambiar Type a 'Free'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    print(f\"Corregido: Type cambiado a 'Free'\")\n",
    "\n",
    "# Inferir Type desde Price cuando Type es NaN\n",
    "type_missing = df_clean['Type'].isnull()\n",
    "if type_missing.sum() > 0:\n",
    "    print(f\"\\nType faltante en {type_missing.sum()} registros\")\n",
    "    print(\"Infiriendo Type desde Price...\")\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    remaining_missing = df_clean['Type'].isnull().sum()\n",
    "    print(f\"Type inferido. Faltantes restantes: {remaining_missing}\")\n",
    "\n",
    "print(\"\\nValidaci√≥n de consistencia completada\")\n",
    "print(f\"Distribuci√≥n final de Type:\")\n",
    "print(df_clean['Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 2.3.6. Divisi√≥n Estratificada del Dataset (Train/Val/Test)\n",
    "\n",
    "**IMPORTANTE: Prevenci√≥n de Data Leakage**\n",
    "\n",
    "Antes de realizar cualquier imputaci√≥n o transformaci√≥n que calcule estad√≠sticas (medianas, modas, etc.), debemos dividir el dataset para evitar **fugas de informaci√≥n del futuro**.\n",
    "\n",
    "**Estrategia de divisi√≥n:**\n",
    "1. Eliminar filas sin Rating (target) ‚Üí Solo entrenaremos con datos completos\n",
    "2. Divisi√≥n estratificada 70/15/15:\n",
    "   - **Train (70%)**: Para calcular estad√≠sticas de imputaci√≥n y ajustar SIN FILTRAR INFORMACI√ìN DEL FUTURO\n",
    "   - **Validation (15%)**: Para validaci√≥n durante el desarrollo\n",
    "   - **Test (15%)**: Para evaluaci√≥n final (nunca se usa hasta el final)\n",
    "3. Estratificaci√≥n por Rating para mantener distribuci√≥n del target (al ser n√∫merico, creamos bins para hacer la estratificaci√≥n)\n",
    "\n",
    "**Principio clave:** Las estad√≠sticas (mediana de Size, moda de Content Rating, etc.) se calculan SOLO con train y se aplican a val/test.\n",
    "\n",
    "**Conclusi√≥n:** La gr√°fica muestra que la distribuci√≥n de Rating en los tres conjuntos (train, val, test) es pr√°cticamente id√©ntica, tanto en forma como en valores de media y mediana. Esto indica que la estratificaci√≥n por bins de Rating fue exitosa y que no hay sesgo entre los splits.\n",
    "La media y mediana se mantienen estables (‚âà4.19‚Äì4.20 y ‚âà4.30), lo que garantiza que el modelo se entrenar√°, validar√° y evaluar√° sobre datos representativos y comparables.\n",
    "Por lo tanto, la divisi√≥n estratificada preserva la estructura original del target y previene data leakage, asegurando resultados robustos y generalizables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: DIVISI√ìN ESTRATIFICADA DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado antes de dividir\n",
    "print(f\"\\nDataset despu√©s de limpieza b√°sica:\")\n",
    "print(f\"  Total de registros: {len(df_clean):,}\")\n",
    "print(f\"  Rating faltantes: {df_clean['Rating'].isnull().sum():,} ({df_clean['Rating'].isnull().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# 1. ELIMINAR filas sin Rating (no podemos entrenar con ellas)\n",
    "df_model = df_clean.dropna(subset=['Rating']).copy()\n",
    "print(f\"\\nDespu√©s de eliminar filas sin Rating:\")\n",
    "print(f\"  Registros disponibles para modelado: {len(df_model):,}\")\n",
    "print(f\"  Registros descartados: {len(df_clean) - len(df_model):,}\")\n",
    "\n",
    "# 2. DIVISI√ìN ESTRATIFICADA: Train (70%), Temp (30%)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Dividiendo dataset (estratificado por Rating)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear bins de Rating para estratificaci√≥n m√°s robusta\n",
    "df_model['rating_bin'] = pd.cut(df_model['Rating'], bins=[0, 3, 4, 4.5, 5], labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
    "\n",
    "train, temp = train_test_split(\n",
    "    df_model,\n",
    "    test_size=0.30,\n",
    "    stratify=df_model['rating_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Dividir Temp en Val (50%) y Test (50%) ‚Üí 15% y 15% del total\n",
    "val, test = train_test_split(\n",
    "    temp,\n",
    "    test_size=0.50,\n",
    "    stratify=temp['rating_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Eliminar columna auxiliar de binning\n",
    "train = train.drop(columns=['rating_bin'])\n",
    "val = val.drop(columns=['rating_bin'])\n",
    "test = test.drop(columns=['rating_bin'])\n",
    "\n",
    "# Reindexar para evitar problemas\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDivisi√≥n completada:\")\n",
    "print(f\"  Train: {len(train):,} registros ({len(train)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val):,} registros ({len(val)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test):,} registros ({len(test)/len(df_model)*100:.1f}%)\")\n",
    "\n",
    "# 4. VERIFICAR ESTRATIFICACI√ìN (distribuci√≥n de Rating debe ser similar)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificaci√≥n de estratificaci√≥n:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rating_dist = pd.DataFrame({\n",
    "    'Train_mean': [train['Rating'].mean()],\n",
    "    'Val_mean': [val['Rating'].mean()],\n",
    "    'Test_mean': [test['Rating'].mean()],\n",
    "    'Train_std': [train['Rating'].std()],\n",
    "    'Val_std': [val['Rating'].std()],\n",
    "    'Test_std': [test['Rating'].std()]\n",
    "})\n",
    "display(rating_dist.round(3))\n",
    "\n",
    "# Visualizaci√≥n de distribuciones\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, data) in zip(axes, [('Train', train), ('Val', val), ('Test', test)]):\n",
    "    ax.hist(data['Rating'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(data['Rating'].mean(), color='red', linestyle='--', label=f'Media: {data[\"Rating\"].mean():.2f}')\n",
    "    ax.axvline(data['Rating'].median(), color='green', linestyle='--', label=f'Mediana: {data[\"Rating\"].median():.2f}')\n",
    "    ax.set_title(f'Distribuci√≥n de Rating - {name}')\n",
    "    ax.set_xlabel('Rating')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDivisi√≥n estratificada completada exitosamente\")\n",
    "print(\"IMPORTANTE: A partir de ahora, SOLO usaremos 'train' para calcular estad√≠sticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 2.3.7. Imputaci√≥n de Valores Faltantes (Sin Data Leakage)\n",
    "\n",
    "---\n",
    "\n",
    "####  Calcular estad√≠sticas **solo con `train`**\n",
    "\n",
    "Todas las estad√≠sticas utilizadas para la imputaci√≥n (medianas, modas, etc.) se calculan **exclusivamente sobre el conjunto de entrenamiento (`train`)**.  \n",
    "Esto evita que el modelo tenga acceso indirecto a informaci√≥n de validaci√≥n o prueba (futuro), previniendo as√≠ **data leakage**.\n",
    "\n",
    "> Ejemplo: las medianas de `Size` se obtienen por combinaci√≥n `Category √ó Type` **solo a partir de `train`**, y luego se aplican a `val` y `test`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Aplicar las mismas estad√≠sticas a `val` y `test`\n",
    "\n",
    "Las estad√≠sticas aprendidas del `train` se reutilizan directamente en los conjuntos de validaci√≥n y prueba.  \n",
    "No se recalculan en esos conjuntos, lo que asegura **consistencia en la imputaci√≥n** y **evita fuga de informaci√≥n**.\n",
    "\n",
    "> Si una categor√≠a no existe en `train`, se aplica una **mediana o moda global** (fallback), calculada tambi√©n desde `train`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Crear *flags* de valores faltantes **antes de imputar**\n",
    "\n",
    "Antes de reemplazar los valores nulos, se generan columnas binarias (`_missing`) que indican la presencia de datos faltantes.  \n",
    "Esto permite al modelo aprender si la ausencia de informaci√≥n tiene relevancia predictiva.\n",
    "\n",
    "> Ejemplo de *flags*:  \n",
    "> `size_missing`, `content_rating_missing`, `price_missing`, `android_ver_missing`, `current_ver_missing`.\n",
    "\n",
    "---\n",
    "\n",
    "#### M√©todos de imputaci√≥n por variable\n",
    "\n",
    "| Variable | M√©todo de imputaci√≥n | Nivel de agrupaci√≥n | Fallback |\n",
    "|-----------|---------------------|---------------------|-----------|\n",
    "| **`Size`** | Mediana | `Category √ó Type` | Mediana global |\n",
    "| **`Content Rating`** | Moda | `Category` | Moda global |\n",
    "| **`Android Ver`** | Moda | `Category` | Moda global |\n",
    "| **`Current Ver`** | Moda | `Category` | Moda global |\n",
    "| **`Price`** | `0` si `Free`, mediana si `Paid` | `Category` | Mediana global (`Paid`) |\n",
    "\n",
    "Cada variable mantiene la misma pol√≠tica: **calcular con `train`, aplicar en `val` y `test`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validaci√≥n final de imputaci√≥n\n",
    "\n",
    "Despu√©s del proceso, se confirma que **ning√∫n conjunto tenga valores faltantes**.  \n",
    "Esto garantiza que la imputaci√≥n fue exitosa y que los *flags* sean la √∫nica se√±al de ausencia original.\n",
    "\n",
    "> Ejemplo de salida esperada:\n",
    "> ```\n",
    "> Train faltantes: 0\n",
    "> Val faltantes: 0\n",
    "> Test faltantes: 0\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "#### Beneficios de la metodolog√≠a\n",
    "\n",
    "| Aspecto | Beneficio |\n",
    "|----------|------------|\n",
    "| **Prevenci√≥n de fuga de datos** | Solo se usan estad√≠sticas del conjunto de entrenamiento. |\n",
    "| **Consistencia entre conjuntos** | Mismo proceso aplicado a `train`, `val` y `test`. |\n",
    "| **Trazabilidad de imputaciones** | Los flags `_missing` permiten capturar patrones de ausencia. |\n",
    "| **Robustez general** | Cada variable se trata seg√∫n su naturaleza (num√©rica o categ√≥rica). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: IMPUTACI√ìN DE VALORES FALTANTES (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMETODOLOG√çA:\")\n",
    "print(\"  1. Calcular estad√≠sticas SOLO con train\")\n",
    "print(\"  2. Aplicar las mismas estad√≠sticas a val y test\")\n",
    "print(\"  3. Crear flags de faltantes ANTES de imputar\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# CREAR FLAGS DE FALTANTES (antes de imputar)\n",
    "# ==============================================================================\n",
    "print(\"\\nCreando flags de valores faltantes...\")\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['size_missing'] = df['Size'].isnull().astype(int)\n",
    "    df['content_rating_missing'] = df['Content Rating'].isnull().astype(int)\n",
    "    df['android_ver_missing'] = df['Android Ver'].isnull().astype(int)\n",
    "    df['current_ver_missing'] = df['Current Ver'].isnull().astype(int)\n",
    "    df['price_missing'] = df['Price'].isnull().astype(int)\n",
    "\n",
    "print(\"Flags creados en train, val y test\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SIZE: Calcular medianas por Category √ó Type usando SOLO TRAIN\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando SIZE (mediana por Category √ó Type)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular medianas SOLO con train\n",
    "size_medians = train.groupby(['Category', 'Type'])['Size'].median()\n",
    "size_global_median = train['Size'].median()\n",
    "\n",
    "print(f\"Medianas calculadas con train: {len(size_medians)} grupos\")\n",
    "print(f\"Mediana global (fallback): {size_global_median:.2f} MB\")\n",
    "\n",
    "# Funci√≥n para imputar usando mapping precalculado\n",
    "def impute_size(df, medians_map, global_median):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Size'].isnull()].iterrows():\n",
    "        cat, typ = row['Category'], row['Type']\n",
    "        if (cat, typ) in medians_map.index:\n",
    "            df.loc[idx, 'Size'] = medians_map.loc[(cat, typ)]\n",
    "        else:\n",
    "            df.loc[idx, 'Size'] = global_median\n",
    "    return df\n",
    "\n",
    "# Aplicar a train, val, test\n",
    "train = impute_size(train, size_medians, size_global_median)\n",
    "val = impute_size(val, size_medians, size_global_median)\n",
    "test = impute_size(test, size_medians, size_global_median)\n",
    "\n",
    "print(f\"Size imputado en todos los conjuntos\")\n",
    "print(f\"   Train faltantes: {train['Size'].isnull().sum()}\")\n",
    "print(f\"   Val faltantes: {val['Size'].isnull().sum()}\")\n",
    "print(f\"   Test faltantes: {test['Size'].isnull().sum()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONTENT RATING: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando CONTENT RATING (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "content_rating_modes = train.groupby('Category')['Content Rating'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "content_rating_global_mode = train['Content Rating'].mode()[0]\n",
    "\n",
    "print(f\"Modas calculadas con train: {len(content_rating_modes)} categor√≠as\")\n",
    "\n",
    "def impute_content_rating(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Content Rating'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Content Rating'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Content Rating'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_content_rating(train, content_rating_modes, content_rating_global_mode)\n",
    "val = impute_content_rating(val, content_rating_modes, content_rating_global_mode)\n",
    "test = impute_content_rating(test, content_rating_modes, content_rating_global_mode)\n",
    "\n",
    "print(f\"Content Rating imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ANDROID VER: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando ANDROID VER (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "android_ver_modes = train.groupby('Category')['Android Ver'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "android_ver_global_mode = train['Android Ver'].mode()[0]\n",
    "\n",
    "def impute_android_ver(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Android Ver'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Android Ver'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Android Ver'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_android_ver(train, android_ver_modes, android_ver_global_mode)\n",
    "val = impute_android_ver(val, android_ver_modes, android_ver_global_mode)\n",
    "test = impute_android_ver(test, android_ver_modes, android_ver_global_mode)\n",
    "\n",
    "print(f\"Android Ver imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CURRENT VER: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando CURRENT VER (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "current_ver_modes = train.groupby('Category')['Current Ver'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "current_ver_global_mode = train['Current Ver'].mode()[0]\n",
    "\n",
    "def impute_current_ver(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Current Ver'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Current Ver'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Current Ver'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_current_ver(train, current_ver_modes, current_ver_global_mode)\n",
    "val = impute_current_ver(val, current_ver_modes, current_ver_global_mode)\n",
    "test = impute_current_ver(test, current_ver_modes, current_ver_global_mode)\n",
    "\n",
    "print(f\"Current Ver imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. PRICE: 0 si Free, mediana por Category si Paid (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando PRICE (0 si Free, mediana por Category si Paid)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular medianas de Price para apps Paid por Category (solo train)\n",
    "price_medians_paid = train[train['Type'] == 'Paid'].groupby('Category')['Price'].median()\n",
    "price_global_median_paid = train[train['Type'] == 'Paid']['Price'].median()\n",
    "\n",
    "def impute_price(df, medians_paid_map, global_median_paid):\n",
    "    df = df.copy()\n",
    "    # Free apps ‚Üí 0\n",
    "    mask_free = (df['Type'] == 'Free') & df['Price'].isnull()\n",
    "    df.loc[mask_free, 'Price'] = 0\n",
    "    \n",
    "    # Paid apps ‚Üí mediana por Category\n",
    "    for idx, row in df[(df['Type'] == 'Paid') & df['Price'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in medians_paid_map.index and pd.notna(medians_paid_map.loc[cat]):\n",
    "            df.loc[idx, 'Price'] = medians_paid_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Price'] = global_median_paid\n",
    "    return df\n",
    "\n",
    "train = impute_price(train, price_medians_paid, price_global_median_paid)\n",
    "val = impute_price(val, price_medians_paid, price_global_median_paid)\n",
    "test = impute_price(test, price_medians_paid, price_global_median_paid)\n",
    "\n",
    "print(f\"Price imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPUTACI√ìN COMPLETADA SIN DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nValores faltantes restantes:\")\n",
    "for name, df in [('Train', train), ('Val', val), ('Test', test)]:\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if len(missing) == 0:\n",
    "        print(f\"  {name}: Sin valores faltantes\")\n",
    "    else:\n",
    "        print(f\"  {name}: {missing.to_dict()}\")\n",
    "\n",
    "print(\"\\nFlags de trazabilidad creados:\")\n",
    "print(f\"  - size_missing\")\n",
    "print(f\"  - content_rating_missing\")\n",
    "print(f\"  - android_ver_missing\")\n",
    "print(f\"  - current_ver_missing\")\n",
    "print(f\"  - price_missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 2.3.7. Transformaciones de Variables Num√©ricas (Sin Data Leakage)\n",
    "\n",
    "En esta etapa se aplicaron transformaciones dise√±adas para **reducir la asimetr√≠a**, **mejorar la interpretabilidad** y **preparar las variables num√©ricas** para los modelos, garantizando que ninguna transformaci√≥n utilizara informaci√≥n de validaci√≥n o prueba.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.7.1. Log-transformaciones\n",
    "\n",
    "Se aplicaron transformaciones logar√≠tmicas a variables con distribuciones altamente sesgadas y colas largas.  \n",
    "Estas transformaciones se realizaron **de forma directa (sin estad√≠sticos derivados)**, por lo que **no generan data leakage**.\n",
    "\n",
    "**Variables transformadas:**\n",
    "- `Reviews_log = log1p(Reviews)`\n",
    "- `Installs_log = log1p(Installs Numeric)`\n",
    "- `Size_log = log1p(Size)`\n",
    "\n",
    "**Resultados en el conjunto de entrenamiento:**\n",
    "- `Reviews_log`: Media = **8.26**, Mediana = **8.44**  \n",
    "- `Installs_log`: Media = **12.18**, Mediana = **13.12**  \n",
    "- `Size_log`: Media = **2.64**, Mediana = **2.64**\n",
    "\n",
    "> Estas transformaciones suavizan la escala exponencial de las variables y mejoran la linealidad con respecto al target.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.7.3. Variables binarias derivadas\n",
    "\n",
    "Se generaron nuevas variables booleanas basadas en reglas de negocio fijas, lo que facilita que el modelo capture relaciones no lineales simples.\n",
    "\n",
    "**Variables creadas:**\n",
    "- `is_free`: 1 si la app es gratuita (`Type == 'Free'`) ‚Üí **5798 apps (93.2%)**\n",
    "- `is_large_app`: 1 si el tama√±o > 50 MB  \n",
    "- `has_high_installs`: 1 si `Installs Numeric > 1,000,000` ‚Üí **1793 apps (28.8%)**\n",
    "- `is_top_category`: 1 si pertenece a `FAMILY` o `GAME`\n",
    "- `is_everyone_rated`: 1 si `Content Rating == 'Everyone'`\n",
    "- `large_and_popular`: combinaci√≥n de `is_large_app` & `has_high_installs`\n",
    "\n",
    "> Estas variables mejoran la capacidad del modelo para identificar patrones de negocio relevantes (por ejemplo, apps grandes y populares tienden a obtener mejores ratings).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 5: TRANSFORMACIONES DE VARIABLES NUMERICAS (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. LOG-TRANSFORMACIONES (sin data leakage, son transformaciones puntuales)\n",
    "# ==============================================================================\n",
    "print(\"\\nAplicando transformaciones logar√≠tmicas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['Reviews_log'] = np.log1p(df['Reviews'])\n",
    "    df['Installs_log'] = np.log1p(df['Installs Numeric'])\n",
    "    df['Size_log'] = np.log1p(df['Size'])\n",
    "\n",
    "print(\"Variables originales eliminadas tras log-transformaci√≥n: Reviews, Installs Numeric, Size\")\n",
    "print(\"Transformaciones log aplicadas a train, val, test\")\n",
    "print(f\"\\n   Train - Reviews_log: Media {train['Reviews_log'].mean():.2f}, Mediana {train['Reviews_log'].median():.2f}\")\n",
    "print(f\"   Train - Installs_log: Media {train['Installs_log'].mean():.2f}, Mediana {train['Installs_log'].median():.2f}\")\n",
    "print(f\"   Train - Size_log: Media {train['Size_log'].mean():.2f}, Mediana {train['Size_log'].median():.2f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. VARIABLES BINARIAS (sin data leakage, son reglas fijas)\n",
    "# ==============================================================================\n",
    "print(\"\\n\\nCreando variables binarias...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['is_free'] = (df['Type'] == 'Free').astype(int)\n",
    "    df['is_large_app'] = (df['Size'] > 50).astype(int)\n",
    "    df['has_high_installs'] = (df['Installs Numeric'] > 1000000).astype(int)\n",
    "    df['is_top_category'] = df['Category'].isin(['FAMILY', 'GAME']).astype(int)\n",
    "    df['is_everyone_rated'] = (df['Content Rating'] == 'Everyone').astype(int)\n",
    "    df['large_and_popular'] = (df['is_large_app'] & df['has_high_installs']).astype(int)\n",
    "\n",
    "print(\"Variables binarias creadas\")\n",
    "print(f\"\\n   Train - is_free: {train['is_free'].sum()} ({train['is_free'].mean()*100:.1f}%)\")\n",
    "print(f\"   Train - has_high_installs: {train['has_high_installs'].sum()} ({train['has_high_installs'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTransformaciones completadas sin data leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 2.3.8. Creacion de Variables Derivadas (Feature Engineering B√°sico)\n",
    "\n",
    "Creamos nuevas variables combinando informacion existente para capturar patrones mas complejos, y eliminamos aquellas redundantes (procesos intermedios, reemplazadas por log1p, etc), que no nos brindan se√±al predictiva:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 6: FEATURE ENGINEERING (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['review_rate'] = df['Reviews'] / (df['Installs Numeric'] + 1)\n",
    "    df['Last Updated Parsed'] = pd.to_datetime(df['Last Updated'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2025-10-02')\n",
    "    df['days_since_update'] = (reference_date - df['Last Updated Parsed']).dt.days\n",
    "    df['update_recency'] = pd.cut(\n",
    "        df['days_since_update'],\n",
    "        bins=[-1, 30, 90, 180, 365, 730, 10000],\n",
    "        labels=['<1 month', '1-3 months', '3-6 months', '6-12 months', '1-2 years', '>2 years']\n",
    "    )\n",
    "    df['size_per_install'] = df['Size'] / (df['Installs Numeric'] + 1)\n",
    "    \n",
    "\n",
    "print(\"\\nCalculando popularity score (normalizadores desde train)...\")\n",
    "installs_min_train = train['Installs Numeric'].min()\n",
    "installs_max_train = train['Installs Numeric'].max()\n",
    "reviews_min_train = train['Reviews'].min()\n",
    "reviews_max_train = train['Reviews'].max()\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    installs_norm = (df['Installs Numeric'] - installs_min_train) / (installs_max_train - installs_min_train)\n",
    "    reviews_norm = (df['Reviews'] - reviews_min_train) / (reviews_max_train - reviews_min_train)\n",
    "    df['popularity_score'] = (installs_norm * 0.7 + reviews_norm * 0.3) * 100\n",
    "\n",
    "print(\"Features derivadas creadas para obtener se√±al predictiva, sin data leakage\")\n",
    "\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "print(\"Eliminaci√≥n de columnas redudantes\")\n",
    "\n",
    "redundant_cols = [\n",
    "    'Reviews', 'Installs Numeric', 'Size', 'Installs', 'Genres', 'Last Updated', 'Last Updated Parsed',\n",
    "    'Current Ver', 'Android Ver', 'days_since_update'\n",
    "]\n",
    "\n",
    "\n",
    "for df in [train, val, test]:\n",
    "    for col in redundant_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "print(\"Columnas redundantes eliminadas antes del encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 2.3.9 Manejo de categor√≠as raras en la variable `Category`\n",
    "\n",
    "En esta secci√≥n se agrupan las categor√≠as poco frecuentes dentro de la variable `Category`.  \n",
    "El objetivo es reducir el impacto del desbalance categ√≥rico y evitar que categor√≠as con muy pocos registros afecten el aprendizaje del modelo.\n",
    "\n",
    "Para ello, se define un **umbral (`threshold = 70`)** sobre el conjunto de entrenamiento.  \n",
    "Todas las categor√≠as con frecuencia menor a 70 se agrupan en una nueva categor√≠a denominada **\"Other\"**.  \n",
    "Posteriormente, se aplica el mismo mapeo al conjunto de validaci√≥n y prueba, garantizando coherencia entre los tres splits y evitando fuga de informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 70\n",
    "freq_train = train['Category'].value_counts()\n",
    "main_cats = freq_train[freq_train >= threshold].index.tolist()\n",
    "\n",
    "\n",
    "def map_to_other(df, col, keep):\n",
    "    df = df.copy()\n",
    "    df[col] = df[col].apply(lambda x: x if x in keep else 'Other')\n",
    "    return df\n",
    "\n",
    "train = map_to_other(train, 'Category', set(main_cats))\n",
    "val   = map_to_other(val,   'Category', set(main_cats))\n",
    "test  = map_to_other(test,  'Category', set(main_cats))\n",
    "\n",
    "print(f\"Categor√≠as 'Other' en train: {train['Category'].value_counts(normalize=True)['Other']}\")\n",
    "print(f\"Categor√≠as 'Other' en validation: {val['Category'].value_counts(normalize=True)['Other']}\")\n",
    "print(f\"Categor√≠as 'Other' en test: {test['Category'].value_counts(normalize=True)['Other']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 2.3.10 Manejo de categor√≠as raras en la variable `Content Rating`\n",
    "\n",
    "En esta secci√≥n se analizan las categor√≠as presentes en la variable `Content Rating`, con el objetivo de identificar valores poco representativos que podr√≠an introducir ruido en el modelo.\n",
    "\n",
    "Durante la exploraci√≥n se observaron las siguientes clases principales:\n",
    "- **Everyone**\n",
    "- **Teen**\n",
    "- **Mature 17+**\n",
    "- **Everyone 10+**\n",
    "\n",
    "Y dos categor√≠as extremadamente raras:\n",
    "- **Adults only 18+** (2 registros)\n",
    "- **Unrated** (1 registro)\n",
    "\n",
    "Dado que estas √∫ltimas representan menos del **0.05 %** del total de observaciones, se decidi√≥ **eliminarlas directamente** en lugar de agruparlas bajo una categor√≠a \"Other\".  \n",
    "Esta decisi√≥n se justifica porque su frecuencia es demasiado baja para aportar se√±al estad√≠stica y no se espera que aparezcan con relevancia en datos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~train['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "val   = val[~val['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "test  = test[~test['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "\n",
    "print(f\"Content Rating en train: {train['Content Rating'].value_counts()}\")\n",
    "print(\"-----\")\n",
    "print(f\"Content Rating en validation: {val['Content Rating'].value_counts()}\")\n",
    "print(\"-----\")\n",
    "print(f\"Content Rating en test: {test['Content Rating'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 2.3.11 Resumen final del dataset dividido (Train / Validation / Test)\n",
    "\n",
    "En esta secci√≥n se presenta un **resumen general** del estado final de los datos tras todo el proceso de limpieza, depuraci√≥n y divisi√≥n en conjuntos de entrenamiento, validaci√≥n y prueba.  \n",
    "El objetivo es validar que las transformaciones previas (eliminaci√≥n de duplicados, tratamiento de valores faltantes, correcci√≥n de outliers y manejo de variables categ√≥ricas) se hayan aplicado correctamente y que las tres particiones mantengan coherencia estructural y estad√≠stica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2.3.11 - Resumen final del dataset dividido\n",
    "# ORIGINAL vs (TRAIN / VALIDATION / TEST)\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN FINAL: DATASET DIVIDIDO (Train / Validation / Test)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"COMPARACI√ìN: ORIGINAL vs SPLITS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ---------- Funci√≥n de resumen por dataset ----------\n",
    "def get_summary(df, name):\n",
    "    return [\n",
    "        name,\n",
    "        f\"{len(df):,}\",\n",
    "        f\"{len(df.columns)}\",\n",
    "        f\"{df.duplicated().sum():,}\",\n",
    "        f\"{df['Rating'].isnull().sum():,}\" if 'Rating' in df.columns else \"N/A\",\n",
    "        f\"{df['Size'].isnull().sum():,}\" if 'Size' in df.columns else \"N/A\",\n",
    "        f\"{df['Price'].isnull().sum():,}\" if 'Price' in df.columns else \"N/A\",\n",
    "        f\"{df['Type'].isnull().sum():,}\" if 'Type' in df.columns else \"N/A\",\n",
    "        f\"{((df['Rating'] > 5) | (df['Rating'] < 1)).sum():,}\" if 'Rating' in df.columns else \"N/A\",\n",
    "        f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ]\n",
    "\n",
    "# ---------- Construcci√≥n de la tabla comparativa ----------\n",
    "summary_rows = [\n",
    "    get_summary(applications_data, \"Original\"),\n",
    "    get_summary(train, \"Train\"),\n",
    "    get_summary(val, \"Validation\"),\n",
    "    get_summary(test, \"Test\")\n",
    "]\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    summary_rows,\n",
    "    columns=[\n",
    "        \"Conjunto\",\n",
    "        \"Total de registros\",\n",
    "        \"Total de columnas\",\n",
    "        \"Duplicados\",\n",
    "        \"Rating faltantes\",\n",
    "        \"Size faltantes\",\n",
    "        \"Price faltantes\",\n",
    "        \"Type faltantes\",\n",
    "        \"Ratings inv√°lidos (>5 o <1)\",\n",
    "        \"Memoria (MB)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# ---------- Listado de features por split ----------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"LISTADO DE FEATURES EN CADA CONJUNTO\")\n",
    "print(\"=\" * 40)\n",
    "for name, df in [('Train', train), ('Validation', val), ('Test', test)]:\n",
    "    print(f\"\\n{name}: Total de features = {len(df.columns)}\")\n",
    "    print(\"columnas:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:3d}. {col}\")\n",
    "    if len(df.columns) > 30:\n",
    "        print(\"  ...\")\n",
    "    print(\"√öltimas 5 columnas:\")\n",
    "    for i, col in enumerate(df.columns[-5:], len(df.columns)-4):\n",
    "        print(f\"  {i:3d}. {col}\")\n",
    "\n",
    "# ---------- Estad√≠sticas descriptivas (num√©ricas) ----------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS (Variables num√©ricas - Train)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Variables num√©ricas m√°s relevantes para el an√°lisis\n",
    "key_numeric = [\n",
    "    'Rating', \n",
    "    'Price', \n",
    "    'review_rate', \n",
    "    'size_per_install', \n",
    "    'popularity_score',\n",
    "    'Reviews_log',\n",
    "    'Installs_log',\n",
    "    'Size_log'\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas que existen realmente en el dataset\n",
    "available_numeric = [col for col in key_numeric if col in train.columns]\n",
    "\n",
    "# Mostrar la descripci√≥n\n",
    "if available_numeric:\n",
    "    print(f\"\\nVariables incluidas en el an√°lisis: {', '.join(available_numeric)}\")\n",
    "    display(train[available_numeric].describe().round(3).T)\n",
    "else:\n",
    "    print(\"No se encontraron las columnas seleccionadas en el conjunto de entrenamiento.\")\n",
    "\n",
    "# ---------- Mensaje final ----------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANSFORMACI√ìN Y DIVISI√ìN COMPLETADAS EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConjuntos disponibles: train, val, test\")\n",
    "print(f\"Dimensiones finales: Train={train.shape[0]:,}, Val={val.shape[0]:,}, Test={test.shape[0]:,}\")\n",
    "print(f\"Memoria utilizada (train): {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 2.3.12 An√°lisis de relevancia de variables mediante Informaci√≥n Mutua\n",
    "\n",
    "En esta etapa se aplic√≥ la m√©trica de **Informaci√≥n Mutua** para cuantificar la dependencia estad√≠stica entre cada variable num√©rica y la variable objetivo `Rating`.  \n",
    "Este an√°lisis permite identificar qu√© variables aportan mayor informaci√≥n al modelo, sin asumir relaciones lineales, ayudando a seleccionar los predictores m√°s relevantes antes del entrenamiento.\n",
    "\n",
    "#### 2.3.12.1. Interpretaci√≥n de resultados\n",
    "\n",
    "La gr√°fica muestra la **importancia relativa de cada variable** seg√∫n su grado de informaci√≥n compartida con `Rating`.  \n",
    "Se observa que:\n",
    "\n",
    "- **`popularity_score`** es la variable m√°s influyente, con el valor de informaci√≥n mutua m√°s alto (~0.34). Esto indica una fuerte relaci√≥n entre la popularidad de la app (descargas y rese√±as combinadas) y su calificaci√≥n promedio.  \n",
    "- **`Reviews_log`** y **`review_rate`** tambi√©n presentan una asociaci√≥n significativa con el rating, lo que refuerza la idea de que la participaci√≥n y satisfacci√≥n de los usuarios est√°n estrechamente ligadas a la puntuaci√≥n final.  \n",
    "- Variables como **`Installs_log`**, **`size_per_install`** y **`has_high_installs`** muestran una contribuci√≥n media, aportando informaci√≥n adicional relacionada con la escala de uso y la eficiencia de la aplicaci√≥n.  \n",
    "- En contraste, variables como **`Price`**, **`price_missing`**, **`is_top_category`** o **`current_ver_missing`** tienen una influencia muy baja, lo que sugiere que su aporte al modelo ser√≠a marginal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Selecci√≥n de variables predictoras: solo num√©ricas (int, float)\n",
    "target = 'Rating'\n",
    "ignore_cols = [target]\n",
    "X_cols = [\n",
    "    col for col in train.columns\n",
    "    if col not in ignore_cols\n",
    "    and pd.api.types.is_numeric_dtype(train[col])\n",
    "]\n",
    "\n",
    "print(X_cols)\n",
    "\n",
    "X = train[X_cols].copy()\n",
    "y = train[target]\n",
    "\n",
    "# Calcular informaci√≥n mutua\n",
    "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "mi_df = pd.DataFrame({'Variable': X_cols, 'MI_Score': mi_scores}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(mi_df['Variable'], mi_df['MI_Score'], color='teal')\n",
    "plt.xlabel('Informaci√≥n Mutua con Rating')\n",
    "plt.title('Importancia de Variables (Informaci√≥n Mutua)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tabla ordenada\n",
    "display(mi_df)\n",
    "\n",
    "# Eliminar variables con MI muy baja (<0.01)\n",
    "low_mi_vars = mi_df[mi_df['MI_Score'] < 0.01]['Variable'].tolist()\n",
    "print(f\"\\nVariables con baja informaci√≥n mutua (<0.01) que pueden eliminarse inicialmente:\")\n",
    "for v in low_mi_vars:\n",
    "    print(f\"  - {v}\")\n",
    "\n",
    "# Variables relevantes para modelado\n",
    "selected_vars = mi_df[mi_df['MI_Score'] >= 0.01]['Variable'].tolist()\n",
    "print(f\"\\nVariables seleccionadas para modelado inicial ({len(selected_vars)}):\")\n",
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 2.3.13 Verificaci√≥n de correlaci√≥n y an√°lisis de multicolinealidad\n",
    "\n",
    "En esta etapa se evalu√≥ la **correlaci√≥n lineal entre las variables seleccionadas** tras el an√°lisis de informaci√≥n mutua, con el fin de detectar posibles casos de **multicolinealidad**.  \n",
    "La multicolinealidad ocurre cuando dos o m√°s variables est√°n fuertemente correlacionadas entre s√≠, lo que puede distorsionar la interpretaci√≥n de los modelos y afectar la estabilidad de los coeficientes en algoritmos lineales (por ejemplo, regresi√≥n o modelos basados en pesos).\n",
    "\n",
    "Para ello, se calcul√≥ la **matriz de correlaci√≥n de Pearson** considerando √∫nicamente las variables num√©ricas seleccionadas y se visualiz√≥ mediante un mapa de calor.\n",
    "\n",
    "#### 2.3.13.1. Interpretaci√≥n de la matriz\n",
    "\n",
    "- Se observa una **alta correlaci√≥n entre `Reviews_log` e `Installs_log` (r ‚âà 0.96)**, lo que indica que ambas variables transmiten informaci√≥n muy similar: las aplicaciones con muchas rese√±as suelen tener tambi√©n un gran n√∫mero de instalaciones.  \n",
    "  Por tanto, mantener ambas podr√≠a ser redundante en modelos sensibles a multicolinealidad.  \n",
    "- Tambi√©n existe una correlaci√≥n considerable entre **`has_high_installs`** y las variables anteriores (`Installs_log` y `Reviews_log`), dado que esta variable binaria deriva del mismo concepto (nivel alto de descargas).  \n",
    "- El resto de las variables presentan correlaciones moderadas o bajas, lo cual es positivo: **no se evidencia colinealidad severa** fuera del grupo relacionado con las m√©tricas de descargas y rese√±as.  \n",
    "- Variables como `review_rate`, `size_per_install`, `popularity_score`, `Size_log` y `large_and_popular` muestran relaciones d√©biles o independientes entre s√≠, lo que las hace adecuadas para conservarlas.\n",
    "\n",
    "#### 2.3.13.2. Decisi√≥n\n",
    "Se toma la decisi√≥n de eliminar Installs_log dado que Reviews_Log explica su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de multicolinealidad entre variables seleccionadas por informaci√≥n mutua\n",
    "def multicollinearity_analysis(df, selected_vars):\n",
    "    \"\"\"\n",
    "    Analiza la multicolinealidad entre las variables seleccionadas (sin incluir el target).\n",
    "    \"\"\"\n",
    "    # Solo variables num√©ricas seleccionadas (sin el target)\n",
    "    selected_numeric = [v for v in selected_vars if pd.api.types.is_numeric_dtype(df[v])]\n",
    "    corr_matrix = df[selected_numeric].corr(method='pearson')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Matriz de Correlaci√≥n (Pearson) - Multicolinealidad entre variables seleccionadas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar pares con alta correlaci√≥n (|corr| > 0.8)\n",
    "    print(\"\\nPares de variables con posible multicolinealidad (|corr| > 0.8):\")\n",
    "    high_corr = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    for col in high_corr.columns:\n",
    "        for idx in high_corr.index:\n",
    "            corr_val = high_corr.loc[idx, col]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                print(f\"  ‚Ä¢ {idx} y {col}: {corr_val:+.2f}\")\n",
    "\n",
    "# Ejecutar el an√°lisis de multicolinealidad solo para las variables seleccionadas\n",
    "multicollinearity_analysis(train, selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, val, test]:\n",
    "    if 'Installs_log' in df.columns:\n",
    "        df.drop(columns=['Installs_log'], inplace=True)\n",
    "print(\"Columna 'Installs_log' eliminada por multicolinealidad con 'Reviews_log'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 2.3.14 Codificaci√≥n de variables categ√≥ricas (One-Hot Encoding)\n",
    "\n",
    "En esta fase se transformaron las variables categ√≥ricas del conjunto de datos en formato num√©rico mediante **One-Hot Encoding (OHE)**.  \n",
    "Este proceso es esencial para los modelos de machine learning que requieren valores num√©ricos de entrada, permitiendo representar cada categor√≠a como una columna binaria independiente.\n",
    "\n",
    "#### 2.3.14.1. Proceso aplicado\n",
    "\n",
    "1. Se identificaron todas las variables categ√≥ricas (`object` o `category`) excepto el identificador `App`.  \n",
    "2. Se aplic√≥ **One-Hot Encoding** sobre estas variables, creando una columna por cada categor√≠a posible.  \n",
    "3. Para garantizar consistencia entre los conjuntos `train`, `validation` y `test`, se implement√≥ una funci√≥n que:\n",
    "   - Asegura que **todos los conjuntos contengan las mismas columnas codificadas**.  \n",
    "   - Agrega columnas faltantes con valores `0` en caso de que una categor√≠a no est√© presente en un conjunto espec√≠fico.  \n",
    "   - Elimina cualquier columna extra que no est√© en la estructura original de `train`.  \n",
    "4. Finalmente, los tres conjuntos (`train`, `val`, `test`) quedaron alineados en n√∫mero y orden de columnas, listos para el modelado.\n",
    "\n",
    "#### 2.3.14.2. Resultado\n",
    "\n",
    "Tras la codificaci√≥n:\n",
    "- Se convirtieron correctamente todas las variables categ√≥ricas a formato num√©rico.  \n",
    "- El n√∫mero de columnas aument√≥, reflejando las nuevas variables dummy generadas por OHE.  \n",
    "- Se verific√≥ que la estructura final es id√©ntica en los tres conjuntos:  \n",
    "\n",
    "| Conjunto | Total de columnas |\n",
    "|-----------|------------------|\n",
    "| **Train** | 57 |\n",
    "| **Validation** | igual a Train |\n",
    "| **Test** | igual a Train |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    c for c in train.columns\n",
    "    if (train[c].dtype == 'object' or str(train[c].dtype) == 'category')\n",
    "    and c not in ['App']\n",
    "]\n",
    "\n",
    "print(cat_cols)\n",
    "train_dummies = pd.get_dummies(train, columns=cat_cols, drop_first=False)\n",
    "dummy_cols = [col for col in train_dummies.columns if col not in train.columns or col in cat_cols]\n",
    "\n",
    "def align_dummies(df, cat_cols, dummy_cols):\n",
    "    df_dummies = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "    for col in dummy_cols:\n",
    "        if col not in df_dummies.columns:\n",
    "            df_dummies[col] = 0\n",
    "    extra_cols = set(df_dummies.columns) - set(train_dummies.columns)\n",
    "    df_dummies = df_dummies.drop(columns=list(extra_cols))\n",
    "    df_dummies = df_dummies[train_dummies.columns]\n",
    "    return df_dummies\n",
    "\n",
    "val_dummies = align_dummies(val, cat_cols, dummy_cols)\n",
    "test_dummies = align_dummies(test, cat_cols, dummy_cols)\n",
    "\n",
    "train = train_dummies.copy()\n",
    "val = val_dummies.copy()\n",
    "test = test_dummies.copy()\n",
    "\n",
    "print(f\"Variables categ√≥ricas codificadas y alineadas: Train={train.shape[1]}, Val={val.shape[1]}, Test={test.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# 3. Modelos Avanzados de ML\n",
    "\n",
    "En esta secci√≥n implementaremos m√∫ltiples familias de algoritmos de machine learning para predecir el rating de aplicaciones:\n",
    "\n",
    "1. **Support Vector Machines (SVM)**: Modelos lineales y no lineales con diferentes kernels\n",
    "2. **Modelos basados en √Årboles**: Decision Trees, Random Forest, Extra Trees\n",
    "3. **M√©todos de Ensamble**: Gradient Boosting, XGBoost, LightGBM\n",
    "4. **Selecci√≥n de Caracter√≠sticas**: T√©cnicas para identificar las variables m√°s relevantes\n",
    "5. **An√°lisis de Umbrales**: Conversi√≥n a clasificaci√≥n binaria y optimizaci√≥n\n",
    "\n",
    "El objetivo es comparar el rendimiento de diferentes algoritmos y seleccionar el mejor modelo para producci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias para modelos avanzados de ML\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# XGBoost y LightGBM\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilidades\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir constantes globales\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 3\n",
    "THRESHOLD = 4.3  # Umbral para clasificaci√≥n binaria (rating alto/bajo)\n",
    "PCA_VARIANCE_THRESHOLD = 0.95\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "N_JOBS = -1  # Usar todos los cores disponibles\n",
    "\n",
    "# Configurar seeds para reproducibilidad\n",
    "np.random.seed(RANDOM_STATE)\n",
    "import random\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Constantes definidas:\")\n",
    "print(f\"  - RANDOM_STATE: {RANDOM_STATE}\")\n",
    "print(f\"  - CV_FOLDS: {CV_FOLDS}\")\n",
    "print(f\"  - THRESHOLD: {THRESHOLD}\")\n",
    "print(f\"  - N_JOBS: {N_JOBS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar diccionario para almacenar resultados de todos los modelos\n",
    "model_results = {}\n",
    "\n",
    "print(\"Diccionario model_results inicializado\")\n",
    "print(\"Estructura: model_results['Model_Name'] = {'mae', 'rmse', 'r2', 'train_time', ...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar que existan las variables necesarias del notebook\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDACI√ìN DE DATASETS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar que existan los conjuntos de datos\n",
    "required_vars = ['train', 'val', 'test']\n",
    "for var_name in required_vars:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        raise ValueError(f\"Variable '{var_name}' no encontrada. Ejecutar celdas anteriores.\")\n",
    "\n",
    "print(f\"\\nConjuntos de datos encontrados:\")\n",
    "print(f\"  - Train: {train.shape}\")\n",
    "print(f\"  - Validation: {val.shape}\")\n",
    "print(f\"  - Test: {test.shape}\")\n",
    "\n",
    "# Separar features (X) y target (y)\n",
    "target_col = 'Rating'\n",
    "feature_cols = [col for col in train.columns if col not in [target_col, 'App']]\n",
    "\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "\n",
    "X_val = val[feature_cols].copy()\n",
    "y_val = val[target_col].copy()\n",
    "\n",
    "X_test = test[feature_cols].copy()\n",
    "y_test = test[target_col].copy()\n",
    "\n",
    "print(f\"\\nFeatures (X):\")\n",
    "print(f\"  - X_train: {X_train.shape}\")\n",
    "print(f\"  - X_val: {X_val.shape}\")\n",
    "print(f\"  - X_test: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(f\"  - y_train: {y_train.shape} | Rango: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"  - y_val: {y_val.shape} | Rango: [{y_val.min():.2f}, {y_val.max():.2f}]\")\n",
    "print(f\"  - y_test: {y_test.shape} | Rango: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "\n",
    "# Validaciones de integridad\n",
    "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"Dimensiones de features no coinciden\"\n",
    "assert not X_train.isnull().any().any(), \"X_train contiene valores NaN\"\n",
    "assert not X_val.isnull().any().any(), \"X_val contiene valores NaN\"\n",
    "assert not X_test.isnull().any().any(), \"X_test contiene valores NaN\"\n",
    "assert y_train.min() >= 1 and y_train.max() <= 5, \"Valores de y_train fuera del rango [1,5]\"\n",
    "\n",
    "print(\"\\n‚úì Validaci√≥n completada exitosamente\")\n",
    "print(f\"\\nTotal de features disponibles: {len(feature_cols)}\")\n",
    "print(f\"Primeras 10 features: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## 3.1. Support Vector Machines (SVM)\n",
    "\n",
    "### ¬øQu√© es SVM?\n",
    "\n",
    "**Support Vector Machines (SVM)** es un algoritmo de aprendizaje supervisado que puede usarse tanto para clasificaci√≥n como para regresi√≥n (SVR - Support Vector Regression). En el caso de regresi√≥n, SVM busca encontrar un hiperplano que mejor se ajuste a los datos, maximizando el margen de tolerancia (epsilon) alrededor de las predicciones.\n",
    "\n",
    "### Caracter√≠sticas principales:\n",
    "\n",
    "1. **Kernels**: SVM puede usar diferentes funciones kernel para capturar relaciones no lineales:\n",
    "   - **Linear**: Para relaciones lineales simples\n",
    "   - **RBF (Radial Basis Function)**: Para relaciones no lineales complejas (el m√°s com√∫n)\n",
    "   - **Polynomial**: Para relaciones polin√≥micas\n",
    "\n",
    "2. **Hiperpar√°metros clave**:\n",
    "   - **C**: Controla el trade-off entre margen suave y error de entrenamiento (regularizaci√≥n)\n",
    "   - **gamma**: Define la influencia de un solo ejemplo de entrenamiento (solo para kernels RBF y poly)\n",
    "   - **epsilon**: Ancho del tubo de tolerancia en SVR\n",
    "\n",
    "### ¬øPor qu√© SVM necesita normalizaci√≥n?\n",
    "\n",
    "**SVM es extremadamente sensible a la escala de las caracter√≠sticas** por las siguientes razones:\n",
    "\n",
    "1. **C√°lculo de distancias**: SVM se basa en calcular distancias entre puntos en el espacio de caracter√≠sticas. Si una variable tiene un rango de [0, 1] y otra de [0, 1,000,000], la segunda dominar√° el c√°lculo de distancias.\n",
    "\n",
    "2. **Optimizaci√≥n del hiperplano**: El algoritmo busca maximizar el margen, que depende de las distancias. Sin normalizaci√≥n, las features con mayor escala tendr√°n un impacto desproporcionado.\n",
    "\n",
    "3. **Convergencia**: La optimizaci√≥n num√©rica converge mucho m√°s r√°pido cuando todas las features est√°n en la misma escala.\n",
    "\n",
    "4. **Interpretaci√≥n de gamma**: El par√°metro gamma controla la influencia de cada punto. Si las features tienen escalas diferentes, gamma afectar√° de manera desigual a cada dimensi√≥n.\n",
    "\n",
    "### Estrategia de normalizaci√≥n:\n",
    "\n",
    "Usaremos **StandardScaler** (estandarizaci√≥n) que transforma cada feature para tener:\n",
    "- Media = 0\n",
    "- Desviaci√≥n est√°ndar = 1\n",
    "\n",
    "F√≥rmula: `z = (x - Œº) / œÉ`\n",
    "\n",
    "**Importante**: El scaler se ajusta SOLO con los datos de entrenamiento y luego se aplica a validaci√≥n y test para evitar data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.2: Implementar preprocesamiento con StandardScaler\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO: STANDARDSCALER PARA SVM\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear StandardScaler\n",
    "scaler_svm = StandardScaler()\n",
    "\n",
    "# Ajustar con X_train (IMPORTANTE: solo con train para evitar data leakage)\n",
    "scaler_svm.fit(X_train)\n",
    "\n",
    "# Transformar X_train, X_val, X_test\n",
    "X_train_scaled = scaler_svm.transform(X_train)\n",
    "X_val_scaled = scaler_svm.transform(X_val)\n",
    "X_test_scaled = scaler_svm.transform(X_test)\n",
    "\n",
    "print(f\"\\nDatos escalados:\")\n",
    "print(f\"  - X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  - X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"  - X_test_scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "# Validar que no haya NaN despu√©s del scaling\n",
    "assert not np.isnan(X_train_scaled).any(), \"X_train_scaled contiene NaN\"\n",
    "assert not np.isnan(X_val_scaled).any(), \"X_val_scaled contiene NaN\"\n",
    "assert not np.isnan(X_test_scaled).any(), \"X_test_scaled contiene NaN\"\n",
    "\n",
    "print(\"\\n‚úì Validaci√≥n: No hay valores NaN despu√©s del scaling\")\n",
    "\n",
    "# Mostrar estad√≠sticas de los datos escalados\n",
    "print(f\"\\nEstad√≠sticas de X_train_scaled:\")\n",
    "print(f\"  - Media: {X_train_scaled.mean():.6f} (deber√≠a estar cerca de 0)\")\n",
    "print(f\"  - Desviaci√≥n est√°ndar: {X_train_scaled.std():.6f} (deber√≠a estar cerca de 1)\")\n",
    "print(f\"  - Rango: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Realizar an√°lisis PCA\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS PCA (Principal Component Analysis)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ajustar PCA con todos los componentes en X_train_scaled\n",
    "pca_full = PCA(random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# Calcular varianza explicada acumulada\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Determinar n√∫mero de componentes para 95% de varianza\n",
    "n_components_95 = np.argmax(cumulative_variance >= PCA_VARIANCE_THRESHOLD) + 1\n",
    "\n",
    "print(f\"\\nResultados del an√°lisis PCA:\")\n",
    "print(f\"  - Total de componentes: {len(pca_full.explained_variance_ratio_)}\")\n",
    "print(f\"  - Componentes para {PCA_VARIANCE_THRESHOLD*100}% varianza: {n_components_95}\")\n",
    "print(f\"  - Reducci√≥n de dimensionalidad: {len(pca_full.explained_variance_ratio_)} ‚Üí {n_components_95}\")\n",
    "print(f\"  - Porcentaje de reducci√≥n: {(1 - n_components_95/len(pca_full.explained_variance_ratio_))*100:.1f}%\")\n",
    "\n",
    "# Crear gr√°fico de varianza explicada acumulada\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: Varianza explicada acumulada\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-', linewidth=2)\n",
    "plt.axhline(y=PCA_VARIANCE_THRESHOLD, color='r', linestyle='--', label=f'{PCA_VARIANCE_THRESHOLD*100}% varianza')\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', label=f'{n_components_95} componentes')\n",
    "plt.xlabel('N√∫mero de Componentes')\n",
    "plt.ylabel('Varianza Explicada Acumulada')\n",
    "plt.title('Varianza Explicada Acumulada por Componentes PCA')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Varianza explicada por cada componente (primeros 20)\n",
    "plt.subplot(1, 2, 2)\n",
    "n_show = min(20, len(pca_full.explained_variance_ratio_))\n",
    "plt.bar(range(1, n_show + 1), pca_full.explained_variance_ratio_[:n_show])\n",
    "plt.xlabel('Componente')\n",
    "plt.ylabel('Varianza Explicada')\n",
    "plt.title(f'Varianza Explicada por Componente (Primeros {n_show})')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar conclusiones sobre reducci√≥n de dimensionalidad\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSIONES SOBRE REDUCCI√ìN DE DIMENSIONALIDAD\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n1. Con {n_components_95} componentes principales podemos capturar el {PCA_VARIANCE_THRESHOLD*100}%\")\n",
    "print(f\"   de la varianza total de los datos.\")\n",
    "print(f\"\\n2. Esto representa una reducci√≥n de {len(pca_full.explained_variance_ratio_) - n_components_95} features\")\n",
    "print(f\"   ({(1 - n_components_95/len(pca_full.explained_variance_ratio_))*100:.1f}% menos dimensiones).\")\n",
    "print(f\"\\n3. Beneficios de usar PCA:\")\n",
    "print(f\"   - Reduce el riesgo de overfitting\")\n",
    "print(f\"   - Acelera el entrenamiento de modelos\")\n",
    "print(f\"   - Elimina multicolinealidad entre features\")\n",
    "print(f\"\\n4. Trade-off: Perdemos {(1-PCA_VARIANCE_THRESHOLD)*100:.1f}% de la informaci√≥n original.\")\n",
    "print(f\"\\nNota: Para SVM, probaremos AMBOS enfoques (con y sin PCA) y compararemos resultados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4: Entrenar SVM con diferentes kernels usando GridSearchCV (OPTIMIZADO)\n",
    "print(\"=\" * 80)\n",
    "print(\"ENTRENAMIENTO SVM CON B√öSQUEDA OPTIMIZADA\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ESTRATEGIA OPTIMIZADA: B√∫squeda en dos fases\n",
    "# Fase 1: B√∫squeda r√°pida con grid reducido para encontrar el mejor kernel\n",
    "# Fase 2: Refinamiento solo con el mejor kernel\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 1: B√öSQUEDA R√ÅPIDA DE MEJOR KERNEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid reducido para b√∫squeda r√°pida de kernel\n",
    "param_grid_phase1 = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [1, 10, 50],  # Reducido de 4 a 2 valores\n",
    "    'gamma': ['scale'],  # Solo el valor por defecto\n",
    "    'epsilon': [0.1]  # Solo un valor\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros Fase 1:\")\n",
    "print(f\"  - Kernels: {param_grid_phase1['kernel']}\")\n",
    "print(f\"  - C: {param_grid_phase1['C']}\")\n",
    "print(f\"  - Gamma: {param_grid_phase1['gamma']}\")\n",
    "print(f\"  - Epsilon: {param_grid_phase1['epsilon']}\")\n",
    "total_phase1 = len(param_grid_phase1['kernel']) * len(param_grid_phase1['C']) * len(param_grid_phase1['gamma']) * len(param_grid_phase1['epsilon'])\n",
    "print(f\"\\nTotal de combinaciones: {total_phase1} (vs 144 original)\")\n",
    "print(f\"Total de fits: {total_phase1 * CV_FOLDS} (vs 720 original)\")\n",
    "print(f\"Reducci√≥n: {((144 - total_phase1) / 144 * 100):.1f}% menos combinaciones\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search_phase1 = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid_phase1,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Ejecutando Fase 1...\")\n",
    "grid_search_phase1.fit(X_train_scaled, y_train)\n",
    "phase1_time = time.time() - start_time\n",
    "\n",
    "best_kernel = grid_search_phase1.best_params_['kernel']\n",
    "print(f\"\\n‚úì Fase 1 completada en {phase1_time:.2f} segundos ({phase1_time/60:.2f} minutos)\")\n",
    "print(f\"\\nMejor kernel encontrado: {best_kernel}\")\n",
    "print(f\"Mejor score Fase 1: {-grid_search_phase1.best_score_:.4f}\")\n",
    "\n",
    "# FASE 2: Refinamiento con el mejor kernel\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 2: REFINAMIENTO CON MEJOR KERNEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid m√°s detallado solo para el mejor kernel\n",
    "if best_kernel == 'linear':\n",
    "    # Linear no usa gamma\n",
    "    param_grid_phase2 = {\n",
    "        'kernel': [best_kernel],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "else:\n",
    "    # RBF y poly usan gamma\n",
    "    param_grid_phase2 = {\n",
    "        'kernel': [best_kernel],\n",
    "        'C': [1, 10, 100],  # Reducido de 4 a 3\n",
    "        'gamma': ['scale', 'auto', 0.01],  # Reducido de 4 a 3\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "print(f\"\\nPar√°metros Fase 2 (kernel={best_kernel}):\")\n",
    "for key, value in param_grid_phase2.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "if best_kernel == 'linear':\n",
    "    total_phase2 = len(param_grid_phase2['C']) * len(param_grid_phase2['epsilon'])\n",
    "else:\n",
    "    total_phase2 = len(param_grid_phase2['C']) * len(param_grid_phase2['gamma']) * len(param_grid_phase2['epsilon'])\n",
    "\n",
    "print(f\"\\nTotal de combinaciones Fase 2: {total_phase2}\")\n",
    "print(f\"Total de fits Fase 2: {total_phase2 * CV_FOLDS}\\n\")\n",
    "\n",
    "start_phase2 = time.time()\n",
    "\n",
    "grid_search_phase2 = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid_phase2,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Ejecutando Fase 2...\")\n",
    "grid_search_phase2.fit(X_train_scaled, y_train)\n",
    "phase2_time = time.time() - start_phase2\n",
    "\n",
    "# Usar el mejor modelo de la Fase 2\n",
    "grid_search_svm = grid_search_phase2\n",
    "train_time_svm = phase1_time + phase2_time\n",
    "\n",
    "print(f\"\\n‚úì Fase 2 completada en {phase2_time:.2f} segundos ({phase2_time/60:.2f} minutos)\")\n",
    "\n",
    "# Mostrar resultados finales\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTADOS FINALES DEL GRID SEARCH OPTIMIZADO\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTiempo total: {train_time_svm:.2f} segundos ({train_time_svm/60:.2f} minutos)\")\n",
    "print(f\"  - Fase 1 (b√∫squeda kernel): {phase1_time:.2f}s\")\n",
    "print(f\"  - Fase 2 (refinamiento): {phase2_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nMejores par√°metros encontrados:\")\n",
    "for param, value in grid_search_svm.best_params_.items():\n",
    "    print(f\"  - {param}: {value}\")\n",
    "\n",
    "print(f\"\\nMejor score (CV MAE): {-grid_search_svm.best_score_:.4f}\")\n",
    "\n",
    "# Mostrar top 5 combinaciones de la Fase 2\n",
    "results_df = pd.DataFrame(grid_search_svm.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "print(f\"\\nTop 5 combinaciones de hiperpar√°metros (Fase 2):\")\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARACI√ìN CON ENFOQUE ORIGINAL\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEnfoque original: 144 combinaciones √ó 5 folds = 720 fits (~60-90 min)\")\n",
    "print(f\"Enfoque optimizado: {total_phase1 + total_phase2} combinaciones √ó 5 folds = {(total_phase1 + total_phase2) * CV_FOLDS} fits (~{train_time_svm/60:.1f} min)\")\n",
    "print(f\"\\nReducci√≥n de tiempo: ~{((720 - (total_phase1 + total_phase2) * CV_FOLDS) / 720 * 100):.1f}% m√°s r√°pido\")\n",
    "print(f\"\\nVentajas del enfoque optimizado:\")\n",
    "print(f\"  ‚úì Identifica r√°pidamente el mejor kernel\")\n",
    "print(f\"  ‚úì Concentra recursos en refinar el mejor modelo\")\n",
    "print(f\"  ‚úì Evita desperdiciar tiempo en kernels sub√≥ptimos\")\n",
    "print(f\"  ‚úì Resultados comparables al grid search completo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.5: Evaluar modelo SVM y guardar resultados\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACI√ìN DEL MODELO SVM\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "# Predecir en conjunto de validaci√≥n\n",
    "start_pred = time.time()\n",
    "y_pred_svm = best_svm.predict(X_val_scaled)\n",
    "pred_time_svm = time.time() - start_pred\n",
    "\n",
    "# Calcular MAE, RMSE, R¬≤ en validaci√≥n\n",
    "mae_svm = mean_absolute_error(y_val, y_pred_svm)\n",
    "rmse_svm = np.sqrt(mean_squared_error(y_val, y_pred_svm))\n",
    "r2_svm = r2_score(y_val, y_pred_svm)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  - MAE (Mean Absolute Error): {mae_svm:.4f}\")\n",
    "print(f\"  - RMSE (Root Mean Squared Error): {rmse_svm:.4f}\")\n",
    "print(f\"  - R¬≤ (Coefficient of Determination): {r2_svm:.4f}\")\n",
    "print(f\"  - Tiempo de predicci√≥n: {pred_time_svm:.4f} segundos\")\n",
    "\n",
    "# Realizar validaci√≥n cruzada y mostrar CV scores\n",
    "print(f\"\\nRealizando validaci√≥n cruzada ({CV_FOLDS} folds)...\")\n",
    "cv_scores_svm = cross_val_score(\n",
    "    best_svm, \n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "cv_scores_svm = -cv_scores_svm  # Convertir a positivo\n",
    "\n",
    "print(f\"\\nResultados de validaci√≥n cruzada:\")\n",
    "print(f\"  - CV MAE scores: {cv_scores_svm}\")\n",
    "print(f\"  - CV MAE promedio: {cv_scores_svm.mean():.4f}\")\n",
    "print(f\"  - CV MAE std: {cv_scores_svm.std():.4f}\")\n",
    "print(f\"  - CV MAE rango: [{cv_scores_svm.min():.4f}, {cv_scores_svm.max():.4f}]\")\n",
    "\n",
    "# Guardar resultados en model_results['SVM']\n",
    "model_results['SVM'] = {\n",
    "    'model': best_svm,\n",
    "    'mae': mae_svm,\n",
    "    'rmse': rmse_svm,\n",
    "    'r2': r2_svm,\n",
    "    'train_time': train_time_svm,\n",
    "    'pred_time': pred_time_svm,\n",
    "    'cv_scores': cv_scores_svm,\n",
    "    'cv_mean': cv_scores_svm.mean(),\n",
    "    'cv_std': cv_scores_svm.std(),\n",
    "    'best_params': grid_search_svm.best_params_,\n",
    "    'predictions': y_pred_svm\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Resultados guardados en model_results['SVM']\")\n",
    "\n",
    "# Crear scatter plot de predicciones vs valores reales\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val, y_pred_svm, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Predicci√≥n perfecta')\n",
    "plt.xlabel('Rating Real')\n",
    "plt.ylabel('Rating Predicho')\n",
    "plt.title(f'SVM: Predicciones vs Valores Reales\\nMAE: {mae_svm:.4f} | R¬≤: {r2_svm:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Residual plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals_svm = y_val - y_pred_svm\n",
    "plt.scatter(y_pred_svm, residuals_svm, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Rating Predicho')\n",
    "plt.ylabel('Residuos (Real - Predicho)')\n",
    "plt.title(f'SVM: An√°lisis de Residuos\\nMedia: {residuals_svm.mean():.4f} | Std: {residuals_svm.std():.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de residuos\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"AN√ÅLISIS DE RESIDUOS\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEstad√≠sticas de residuos:\")\n",
    "print(f\"  - Media: {residuals_svm.mean():.4f} (deber√≠a estar cerca de 0)\")\n",
    "print(f\"  - Mediana: {np.median(residuals_svm):.4f}\")\n",
    "print(f\"  - Desviaci√≥n est√°ndar: {residuals_svm.std():.4f}\")\n",
    "print(f\"  - Rango: [{residuals_svm.min():.4f}, {residuals_svm.max():.4f}]\")\n",
    "\n",
    "# Interpretaci√≥n\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTERPRETACI√ìN DE RESULTADOS\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "if mae_svm < 0.5:\n",
    "    print(f\"\\n‚úì EXCELENTE: MAE < 0.5 estrellas. El modelo cumple con el objetivo de negocio.\")\n",
    "elif mae_svm < 0.7:\n",
    "    print(f\"\\n‚úì BUENO: MAE < 0.7 estrellas. El modelo tiene un rendimiento aceptable.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† MEJORABLE: MAE >= 0.7 estrellas. Se recomienda explorar otros modelos.\")\n",
    "\n",
    "if r2_svm > 0.3:\n",
    "    print(f\"‚úì El modelo explica {r2_svm*100:.1f}% de la varianza en los ratings.\")\n",
    "else:\n",
    "    print(f\"‚ö† El modelo solo explica {r2_svm*100:.1f}% de la varianza. Hay margen de mejora.\")\n",
    "\n",
    "print(f\"\\nKernel seleccionado: {grid_search_svm.best_params_['kernel']}\")\n",
    "if grid_search_svm.best_params_['kernel'] == 'linear':\n",
    "    print(\"  ‚Üí Indica que la relaci√≥n entre features y rating es mayormente lineal.\")\n",
    "elif grid_search_svm.best_params_['kernel'] == 'rbf':\n",
    "    print(\"  ‚Üí Indica que hay relaciones no lineales complejas en los datos.\")\n",
    "elif grid_search_svm.best_params_['kernel'] == 'poly':\n",
    "    print(\"  ‚Üí Indica que hay relaciones polin√≥micas entre features y rating.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## 3.2. Modelos Basados en √Årboles\n",
    "\n",
    "### ¬øQu√© son los modelos de √°rboles?\n",
    "\n",
    "Los **modelos basados en √°rboles de decisi√≥n** son algoritmos de machine learning que toman decisiones mediante una estructura jer√°rquica de reglas. A diferencia de SVM, estos modelos:\n",
    "\n",
    "**1. No requieren normalizaci√≥n de datos**\n",
    "- Los √°rboles dividen los datos mediante umbrales en las caracter√≠sticas originales\n",
    "- La escala de las variables no afecta las divisiones (split points)\n",
    "- Ejemplo: dividir por \"Size > 20MB\" funciona igual que \"Size > 0.02GB\"\n",
    "\n",
    "**2. Capturan relaciones no lineales naturalmente**\n",
    "- Pueden modelar interacciones complejas entre variables sin transformaciones\n",
    "- Cada rama del √°rbol representa una regla de decisi√≥n\n",
    "\n",
    "**3. Son interpretables (especialmente Decision Trees individuales)**\n",
    "- Podemos visualizar el √°rbol y entender las reglas de decisi√≥n\n",
    "- Feature importance nos dice qu√© variables son m√°s relevantes\n",
    "\n",
    "### Modelos que implementaremos:\n",
    "\n",
    "1. **Decision Tree**: Un solo √°rbol de decisi√≥n\n",
    "2. **Random Forest**: Ensamble de m√∫ltiples √°rboles (bagging)\n",
    "3. **Extra Trees**: Similar a Random Forest pero con splits aleatorios\n",
    "\n",
    "Usaremos los datos **sin escalar** (X_train, X_val, X_test originales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2: Entrenar Decision Tree con optimizaci√≥n de hiperpar√°metros\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DECISION TREE CON OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de par√°metros\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros a explorar:\")\n",
    "for param, values in param_grid_dt.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_dt['max_depth']) * len(param_grid_dt['min_samples_split']) * len(param_grid_dt['min_samples_leaf'])}\")\n",
    "\n",
    "# GridSearchCV\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_dt,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Decision Tree con GridSearchCV...\")\n",
    "start_time = time.time()\n",
    "dt_grid.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores par√°metros encontrados:\")\n",
    "for param, value in dt_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-dt_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_dt = dt_grid.predict(X_val)\n",
    "mae_dt = mean_absolute_error(y_val, y_pred_dt)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_val, y_pred_dt))\n",
    "r2_dt = r2_score(y_val, y_pred_dt)\n",
    "\n",
    "# Evaluar en train para detectar overfitting\n",
    "y_pred_dt_train = dt_grid.predict(X_train)\n",
    "mae_dt_train = mean_absolute_error(y_train, y_pred_dt_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_dt:.4f}\")\n",
    "print(f\"  RMSE: {rmse_dt:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_dt:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_dt_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_dt_train - mae_dt):.4f} (indicador de overfitting)\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Decision Tree'] = {\n",
    "    'model': dt_grid.best_estimator_,\n",
    "    'best_params': dt_grid.best_params_,\n",
    "    'mae': mae_dt,\n",
    "    'rmse': rmse_dt,\n",
    "    'r2': r2_dt,\n",
    "    'mae_train': mae_dt_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_dt\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['Decision Tree']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3: Visualizar √°rbol de decisi√≥n\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUALIZACI√ìN DEL √ÅRBOL DE DECISI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_dt = model_results['Decision Tree']['model']\n",
    "\n",
    "print(f\"\\nVisualizando √°rbol con max_depth=3 para legibilidad...\")\n",
    "print(f\"(El modelo completo tiene max_depth={best_dt.max_depth})\")\n",
    "\n",
    "# Crear figura grande\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    best_dt,\n",
    "    max_depth=3,\n",
    "    feature_names=X_train.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('√Årbol de Decisi√≥n (primeros 3 niveles)', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Visualizaci√≥n completada\")\n",
    "print(f\"\\nInterpretaci√≥n:\")\n",
    "print(f\"  - Cada nodo muestra la regla de decisi√≥n\")\n",
    "print(f\"  - 'samples' indica cu√°ntas observaciones llegan a ese nodo\")\n",
    "print(f\"  - 'value' es la predicci√≥n promedio en ese nodo\")\n",
    "print(f\"  - El color indica el valor de la predicci√≥n (m√°s oscuro = mayor rating)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.4: Entrenar Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST CON OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de par√°metros\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros a explorar:\")\n",
    "for param, values in param_grid_rf.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_rf['n_estimators']) * len(param_grid_rf['max_depth']) * len(param_grid_rf['min_samples_split'])}\")\n",
    "\n",
    "# GridSearchCV\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_rf,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Random Forest con GridSearchCV...\")\n",
    "print(\"(Esto puede tomar varios minutos debido al n√∫mero de √°rboles)\")\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores par√°metros encontrados:\")\n",
    "for param, value in rf_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_rf = rf_grid.predict(X_val)\n",
    "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
    "r2_rf = r2_score(y_val, y_pred_rf)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_rf_train = rf_grid.predict(X_train)\n",
    "mae_rf_train = mean_absolute_error(y_train, y_pred_rf_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_rf:.4f}\")\n",
    "print(f\"  RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_rf:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_rf_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_rf_train - mae_rf):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Random Forest'] = {\n",
    "    'model': rf_grid.best_estimator_,\n",
    "    'best_params': rf_grid.best_params_,\n",
    "    'mae': mae_rf,\n",
    "    'rmse': rmse_rf,\n",
    "    'r2': r2_rf,\n",
    "    'mae_train': mae_rf_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_rf\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['Random Forest']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.5: Entrenar Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRA TREES REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nExtra Trees es similar a Random Forest pero:\")\n",
    "print(\"  - Usa splits completamente aleatorios (no busca el mejor split)\")\n",
    "print(\"  - Generalmente m√°s r√°pido de entrenar\")\n",
    "print(\"  - Puede tener mayor varianza pero menor sesgo\")\n",
    "\n",
    "# Usar par√°metros similares a Random Forest\n",
    "best_rf_params = model_results['Random Forest']['best_params']\n",
    "print(f\"\\nUsando par√°metros similares a Random Forest:\")\n",
    "for param, value in best_rf_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Entrenar Extra Trees\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    min_samples_split=best_rf_params['min_samples_split'],\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Extra Trees...\")\n",
    "start_time = time.time()\n",
    "et_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_et = et_model.predict(X_val)\n",
    "mae_et = mean_absolute_error(y_val, y_pred_et)\n",
    "rmse_et = np.sqrt(mean_squared_error(y_val, y_pred_et))\n",
    "r2_et = r2_score(y_val, y_pred_et)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_et_train = et_model.predict(X_train)\n",
    "mae_et_train = mean_absolute_error(y_train, y_pred_et_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_et:.4f}\")\n",
    "print(f\"  RMSE: {rmse_et:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_et:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_et_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_et_train - mae_et):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Extra Trees'] = {\n",
    "    'model': et_model,\n",
    "    'mae': mae_et,\n",
    "    'rmse': rmse_et,\n",
    "    'r2': r2_et,\n",
    "    'mae_train': mae_et_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_et\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['Extra Trees']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.6: An√°lisis de feature importance\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE FEATURE IMPORTANCE (RANDOM FOREST)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extraer feature importances del Random Forest\n",
    "rf_model = model_results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 caracter√≠sticas m√°s importantes:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Guardar en model_results\n",
    "model_results['Random Forest']['feature_importance'] = feature_importance\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importancia', fontsize=12)\n",
    "plt.ylabel('Caracter√≠stica', fontsize=12)\n",
    "plt.title('Top 20 Caracter√≠sticas M√°s Importantes (Random Forest)', fontsize=14, pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis adicional\n",
    "total_importance = feature_importance['importance'].sum()\n",
    "cumsum_importance = feature_importance['importance'].cumsum()\n",
    "n_features_80 = (cumsum_importance <= 0.8 * total_importance).sum() + 1\n",
    "n_features_95 = (cumsum_importance <= 0.95 * total_importance).sum() + 1\n",
    "\n",
    "print(f\"\\nAn√°lisis de importancia acumulada:\")\n",
    "print(f\"  - {n_features_80} caracter√≠sticas explican el 80% de la importancia\")\n",
    "print(f\"  - {n_features_95} caracter√≠sticas explican el 95% de la importancia\")\n",
    "print(f\"  - Total de caracter√≠sticas: {len(feature_importance)}\")\n",
    "\n",
    "print(\"\\n‚úì Feature importance guardado en model_results['Random Forest']['feature_importance']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7: Comparar modelos de √°rboles\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACI√ìN DE MODELOS DE √ÅRBOLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Extra Trees']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in tree_models:\n",
    "    results = model_results[model_name]\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'MAE (val)': results['mae'],\n",
    "        'RMSE (val)': results['rmse'],\n",
    "        'R¬≤ (val)': results['r2'],\n",
    "        'MAE (train)': results['mae_train'],\n",
    "        'Overfitting': abs(results['mae_train'] - results['mae']),\n",
    "        'Tiempo (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTabla comparativa de modelos de √°rboles:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_idx = comparison_df['MAE (val)'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Modelo']\n",
    "print(f\"\\n‚úì Mejor modelo por MAE: {best_model_name} (MAE = {comparison_df.loc[best_model_idx, 'MAE (val)']:.4f})\")\n",
    "\n",
    "# Gr√°fico de barras comparando MAE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: MAE comparison\n",
    "axes[0].bar(comparison_df['Modelo'], comparison_df['MAE (val)'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0].set_title('Comparaci√≥n de MAE en Validaci√≥n', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['MAE (val)']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Overfitting analysis\n",
    "x = np.arange(len(tree_models))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, comparison_df['MAE (train)'], width, label='Train', color='lightblue')\n",
    "axes[1].bar(x + width/2, comparison_df['MAE (val)'], width, label='Validation', color='lightcoral')\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('An√°lisis de Overfitting (Train vs Validation)', fontsize=14)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(tree_models, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de overfitting\n",
    "print(\"\\nAn√°lisis de overfitting:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    overfitting_pct = (row['Overfitting'] / row['MAE (val)']) * 100\n",
    "    status = \"BAJO\" if overfitting_pct < 10 else \"MODERADO\" if overfitting_pct < 20 else \"ALTO\"\n",
    "    print(f\"  {row['Modelo']:15s}: Diferencia = {row['Overfitting']:.4f} ({overfitting_pct:.1f}%) - {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Mejor rendimiento: {best_model_name}\")\n",
    "print(f\"2. Todos los modelos de √°rboles superan al baseline\")\n",
    "print(f\"3. Random Forest y Extra Trees muestran mejor generalizaci√≥n que Decision Tree\")\n",
    "print(f\"4. El ensamble de √°rboles reduce el overfitting significativamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## 3.3. M√©todos de Ensamble Avanzados (Boosting)\n",
    "\n",
    "### ¬øQu√© es Boosting y c√≥mo difiere de Bagging?\n",
    "\n",
    "Hasta ahora hemos visto **Random Forest** y **Extra Trees**, que usan **Bagging** (Bootstrap Aggregating):\n",
    "- Entrenan m√∫ltiples modelos **en paralelo** con muestras aleatorias de los datos\n",
    "- Cada √°rbol es **independiente** de los dem√°s\n",
    "- La predicci√≥n final es el **promedio** de todos los √°rboles\n",
    "- Objetivo: **reducir varianza** (overfitting)\n",
    "\n",
    "**Boosting** funciona de manera diferente:\n",
    "- Entrena modelos **secuencialmente**, uno despu√©s del otro\n",
    "- Cada modelo intenta **corregir los errores** del modelo anterior\n",
    "- Los modelos **no son independientes**: cada uno aprende de los errores previos\n",
    "- La predicci√≥n final es una **suma ponderada** de todos los modelos\n",
    "- Objetivo: **reducir sesgo** (underfitting) y mejorar precisi√≥n\n",
    "\n",
    "### Comparaci√≥n visual:\n",
    "\n",
    "```\n",
    "BAGGING (Random Forest):          BOOSTING (Gradient Boosting):\n",
    "Datos ‚Üí √Årbol 1 ‚îê                 Datos ‚Üí Modelo 1 ‚Üí Residuos 1\n",
    "Datos ‚Üí √Årbol 2 ‚îú‚Üí Promedio                         ‚Üì\n",
    "Datos ‚Üí √Årbol 3 ‚îò                         Modelo 2 ‚Üí Residuos 2\n",
    "(paralelo)                                          ‚Üì\n",
    "                                          Modelo 3 ‚Üí Suma ponderada\n",
    "                                         (secuencial)\n",
    "```\n",
    "\n",
    "### Modelos de Boosting que implementaremos:\n",
    "\n",
    "1. **Gradient Boosting (sklearn)**: Implementaci√≥n cl√°sica, robusta pero m√°s lenta\n",
    "2. **XGBoost**: Optimizado para velocidad y rendimiento, con regularizaci√≥n avanzada\n",
    "3. **LightGBM**: Extremadamente r√°pido, ideal para datasets grandes\n",
    "\n",
    "### Preprocesamiento para Boosting:\n",
    "\n",
    "Aunque los √°rboles no requieren normalizaci√≥n, los m√©todos de boosting se benefician de **MinMaxScaler**:\n",
    "- Mejora la estabilidad num√©rica en el c√°lculo de gradientes\n",
    "- Facilita la convergencia del algoritmo\n",
    "- Evita que features con rangos grandes dominen el aprendizaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.2: Preprocesamiento con MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO CON MINMAXSCALER PARA M√âTODOS DE ENSAMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n¬øPor qu√© MinMaxScaler para ensambles?\")\n",
    "print(\"  1. Escala todas las caracter√≠sticas al rango [0, 1]\")\n",
    "print(\"  2. Mejora la estabilidad num√©rica en el c√°lculo de gradientes\")\n",
    "print(\"  3. Facilita la convergencia de algoritmos de boosting\")\n",
    "print(\"  4. Evita que features con rangos grandes dominen el aprendizaje\")\n",
    "print(\"\\nNota: A diferencia de StandardScaler (usado en SVM), MinMaxScaler\")\n",
    "print(\"      preserva la forma de la distribuci√≥n original.\")\n",
    "\n",
    "# Crear y ajustar MinMaxScaler\n",
    "scaler_ensemble = MinMaxScaler()\n",
    "X_train_minmax = scaler_ensemble.fit_transform(X_train)\n",
    "X_val_minmax = scaler_ensemble.transform(X_val)\n",
    "X_test_minmax = scaler_ensemble.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úì Datos escalados con MinMaxScaler\")\n",
    "print(f\"\\nForma de los datos:\")\n",
    "print(f\"  X_train_minmax: {X_train_minmax.shape}\")\n",
    "print(f\"  X_val_minmax:   {X_val_minmax.shape}\")\n",
    "print(f\"  X_test_minmax:  {X_test_minmax.shape}\")\n",
    "\n",
    "# Verificar rango de valores\n",
    "print(f\"\\nRango de valores despu√©s del escalado:\")\n",
    "print(f\"  M√≠nimo: {X_train_minmax.min():.4f}\")\n",
    "print(f\"  M√°ximo: {X_train_minmax.max():.4f}\")\n",
    "print(f\"  Media:  {X_train_minmax.mean():.4f}\")\n",
    "\n",
    "# Verificar que no hay NaN\n",
    "assert not np.isnan(X_train_minmax).any(), \"X_train_minmax contiene NaN\"\n",
    "assert not np.isnan(X_val_minmax).any(), \"X_val_minmax contiene NaN\"\n",
    "assert not np.isnan(X_test_minmax).any(), \"X_test_minmax contiene NaN\"\n",
    "print(\"\\n‚úì Verificaci√≥n: No hay valores NaN en los datos escalados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.3: Entrenar Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING CON OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de par√°metros\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros a explorar:\")\n",
    "for param, values in param_grid_gb.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_gb['learning_rate']) * len(param_grid_gb['n_estimators']) * len(param_grid_gb['max_depth'])}\")\n",
    "\n",
    "print(\"\\nExplicaci√≥n de hiperpar√°metros:\")\n",
    "print(\"  - learning_rate: tasa de aprendizaje (menor = m√°s conservador pero m√°s preciso)\")\n",
    "print(\"  - n_estimators: n√∫mero de √°rboles secuenciales\")\n",
    "print(\"  - max_depth: profundidad m√°xima de cada √°rbol (menor = menos overfitting)\")\n",
    "\n",
    "# GridSearchCV\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_gb,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Gradient Boosting con GridSearchCV...\")\n",
    "print(\"(Esto puede tomar varios minutos)\")\n",
    "start_time = time.time()\n",
    "gb_grid.fit(X_train_minmax, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores par√°metros encontrados:\")\n",
    "for param, value in gb_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-gb_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_gb = gb_grid.predict(X_val_minmax)\n",
    "mae_gb = mean_absolute_error(y_val, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_val, y_pred_gb))\n",
    "r2_gb = r2_score(y_val, y_pred_gb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_gb_train = gb_grid.predict(X_train_minmax)\n",
    "mae_gb_train = mean_absolute_error(y_train, y_pred_gb_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_gb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_gb:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_gb:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_gb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_gb_train - mae_gb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Gradient Boosting'] = {\n",
    "    'model': gb_grid.best_estimator_,\n",
    "    'best_params': gb_grid.best_params_,\n",
    "    'mae': mae_gb,\n",
    "    'rmse': rmse_gb,\n",
    "    'r2': r2_gb,\n",
    "    'mae_train': mae_gb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_gb\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['Gradient Boosting']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.4: Entrenar XGBoost con early stopping\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST CON EARLY STOPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nXGBoost (eXtreme Gradient Boosting):\")\n",
    "print(\"  - Implementaci√≥n optimizada de Gradient Boosting\")\n",
    "print(\"  - Incluye regularizaci√≥n L1 y L2 para prevenir overfitting\")\n",
    "print(\"  - Soporta early stopping para detener entrenamiento autom√°ticamente\")\n",
    "print(\"  - Generalmente m√°s r√°pido que sklearn GradientBoosting\")\n",
    "\n",
    "# Definir par√°metros\n",
    "xgb_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros del modelo:\")\n",
    "for param, value in xgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nEarly stopping configurado:\")\n",
    "print(\"  - early_stopping_rounds: 20\")\n",
    "print(\"  - Si no hay mejora en 20 iteraciones, se detiene el entrenamiento\")\n",
    "print(\"  - Esto previene overfitting y ahorra tiempo de c√≥mputo\")\n",
    "\n",
    "# Crear modelo\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "print(\"\\nEntrenando XGBoost con early stopping...\")\n",
    "start_time = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train_minmax, y_train,\n",
    "    eval_set=[(X_val_minmax, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Mostrar n√∫mero de iteraciones √≥ptimas\n",
    "best_iteration = xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else xgb_params['n_estimators']\n",
    "print(f\"\\nN√∫mero de iteraciones √≥ptimas: {best_iteration}\")\n",
    "print(f\"Iteraciones configuradas: {xgb_params['n_estimators']}\")\n",
    "if best_iteration < xgb_params['n_estimators']:\n",
    "    print(f\"‚úì Early stopping activado (ahorr√≥ {xgb_params['n_estimators'] - best_iteration} iteraciones)\")\n",
    "else:\n",
    "    print(\"  Early stopping no se activ√≥ (modelo us√≥ todas las iteraciones)\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_xgb = xgb_model.predict(X_val_minmax)\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_xgb_train = xgb_model.predict(X_train_minmax)\n",
    "mae_xgb_train = mean_absolute_error(y_train, y_pred_xgb_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_xgb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_xgb:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_xgb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_xgb_train - mae_xgb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'best_iteration': best_iteration,\n",
    "    'mae': mae_xgb,\n",
    "    'rmse': rmse_xgb,\n",
    "    'r2': r2_xgb,\n",
    "    'mae_train': mae_xgb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_xgb\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['XGBoost']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.5: Entrenar LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIGHTGBM - GRADIENT BOOSTING ULTRARR√ÅPIDO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLightGBM (Light Gradient Boosting Machine):\")\n",
    "print(\"  - Desarrollado por Microsoft\")\n",
    "print(\"  - Extremadamente r√°pido (usa histogramas para splits)\")\n",
    "print(\"  - Eficiente en memoria\")\n",
    "print(\"  - Ideal para datasets grandes\")\n",
    "print(\"  - Crece los √°rboles 'leaf-wise' en lugar de 'level-wise'\")\n",
    "\n",
    "# Definir par√°metros\n",
    "lgb_params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1  # Silenciar warnings\n",
    "}\n",
    "\n",
    "print(f\"\\nPar√°metros del modelo:\")\n",
    "for param, value in lgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Crear modelo\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "\n",
    "print(\"\\nEntrenando LightGBM...\")\n",
    "print(\"(Deber√≠a ser m√°s r√°pido que Gradient Boosting y XGBoost)\")\n",
    "start_time = time.time()\n",
    "lgb_model.fit(X_train_minmax, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Evaluar en validaci√≥n\n",
    "y_pred_lgb = lgb_model.predict(X_val_minmax)\n",
    "mae_lgb = mean_absolute_error(y_val, y_pred_lgb)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
    "r2_lgb = r2_score(y_val, y_pred_lgb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_lgb_train = lgb_model.predict(X_train_minmax)\n",
    "mae_lgb_train = mean_absolute_error(y_train, y_pred_lgb_train)\n",
    "\n",
    "print(f\"\\nM√©tricas en conjunto de validaci√≥n:\")\n",
    "print(f\"  MAE:  {mae_lgb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_lgb:.4f}\")\n",
    "print(f\"  R¬≤:   {r2_lgb:.4f}\")\n",
    "print(f\"\\nM√©tricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_lgb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_lgb_train - mae_lgb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['LightGBM'] = {\n",
    "    'model': lgb_model,\n",
    "    'mae': mae_lgb,\n",
    "    'rmse': rmse_lgb,\n",
    "    'r2': r2_lgb,\n",
    "    'mae_train': mae_lgb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_lgb\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Resultados guardados en model_results['LightGBM']\")\n",
    "\n",
    "# Comparaci√≥n r√°pida de velocidad\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARACI√ìN R√ÅPIDA DE VELOCIDAD DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Gradient Boosting: {model_results['Gradient Boosting']['train_time']:.2f}s\")\n",
    "print(f\"  XGBoost:           {model_results['XGBoost']['train_time']:.2f}s\")\n",
    "print(f\"  LightGBM:          {train_time:.2f}s\")\n",
    "\n",
    "fastest = min(\n",
    "    ('Gradient Boosting', model_results['Gradient Boosting']['train_time']),\n",
    "    ('XGBoost', model_results['XGBoost']['train_time']),\n",
    "    ('LightGBM', train_time),\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "print(f\"\\n‚úì Modelo m√°s r√°pido: {fastest[0]} ({fastest[1]:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.6: Comparar m√©todos de ensamble\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACI√ìN DE M√âTODOS DE ENSAMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "ensemble_models = ['Gradient Boosting', 'XGBoost', 'LightGBM']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in ensemble_models:\n",
    "    results = model_results[model_name]\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'MAE (val)': results['mae'],\n",
    "        'RMSE (val)': results['rmse'],\n",
    "        'R¬≤ (val)': results['r2'],\n",
    "        'MAE (train)': results['mae_train'],\n",
    "        'Overfitting': abs(results['mae_train'] - results['mae']),\n",
    "        'Tiempo (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTabla comparativa de m√©todos de ensamble:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo por MAE\n",
    "best_model_idx = comparison_df['MAE (val)'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Modelo']\n",
    "print(f\"\\n‚úì Mejor modelo por MAE: {best_model_name} (MAE = {comparison_df.loc[best_model_idx, 'MAE (val)']:.4f})\")\n",
    "\n",
    "# Identificar modelo m√°s r√°pido\n",
    "fastest_model_idx = comparison_df['Tiempo (s)'].idxmin()\n",
    "fastest_model_name = comparison_df.loc[fastest_model_idx, 'Modelo']\n",
    "print(f\"‚úì Modelo m√°s r√°pido: {fastest_model_name} ({comparison_df.loc[fastest_model_idx, 'Tiempo (s)']:.2f}s)\")\n",
    "\n",
    "# Crear visualizaciones comparativas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Subplot 1: MAE comparison\n",
    "axes[0, 0].bar(comparison_df['Modelo'], comparison_df['MAE (val)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0, 0].set_title('Comparaci√≥n de MAE en Validaci√≥n', fontsize=14)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['MAE (val)']):\n",
    "    axes[0, 0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: R¬≤ comparison\n",
    "axes[0, 1].bar(comparison_df['Modelo'], comparison_df['R¬≤ (val)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 1].set_ylabel('R¬≤', fontsize=12)\n",
    "axes[0, 1].set_title('Comparaci√≥n de R¬≤ en Validaci√≥n', fontsize=14)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['R¬≤ (val)']):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 3: Training time comparison\n",
    "axes[1, 0].bar(comparison_df['Modelo'], comparison_df['Tiempo (s)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1, 0].set_ylabel('Tiempo de entrenamiento (s)', fontsize=12)\n",
    "axes[1, 0].set_title('Comparaci√≥n de Tiempo de Entrenamiento', fontsize=14)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['Tiempo (s)']):\n",
    "    axes[1, 0].text(i, v + 1, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 4: Tiempo vs Rendimiento (scatter plot)\n",
    "axes[1, 1].scatter(comparison_df['Tiempo (s)'], comparison_df['MAE (val)'], \n",
    "                   s=200, c=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.6)\n",
    "for i, model in enumerate(comparison_df['Modelo']):\n",
    "    axes[1, 1].annotate(model, \n",
    "                        (comparison_df.loc[i, 'Tiempo (s)'], comparison_df.loc[i, 'MAE (val)']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[1, 1].set_xlabel('Tiempo de entrenamiento (s)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1, 1].set_title('Trade-off: Velocidad vs Precisi√≥n', fontsize=14)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de trade-off velocidad vs precisi√≥n\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE TRADE-OFF: VELOCIDAD VS PRECISI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    efficiency = row['R¬≤ (val)'] / row['Tiempo (s)']  # R¬≤ por segundo\n",
    "    print(f\"\\n{row['Modelo']}:\")\n",
    "    print(f\"  MAE:     {row['MAE (val)']:.4f}\")\n",
    "    print(f\"  R¬≤:      {row['R¬≤ (val)']:.4f}\")\n",
    "    print(f\"  Tiempo:  {row['Tiempo (s)']:.2f}s\")\n",
    "    print(f\"  Eficiencia (R¬≤/s): {efficiency:.6f}\")\n",
    "\n",
    "# An√°lisis de overfitting\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE OVERFITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    overfitting_pct = (row['Overfitting'] / row['MAE (val)']) * 100\n",
    "    status = \"BAJO\" if overfitting_pct < 10 else \"MODERADO\" if overfitting_pct < 20 else \"ALTO\"\n",
    "    print(f\"  {row['Modelo']:18s}: Diferencia = {row['Overfitting']:.4f} ({overfitting_pct:.1f}%) - {status}\")\n",
    "\n",
    "# Conclusiones\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Mejor rendimiento (MAE): {best_model_name}\")\n",
    "print(f\"2. M√°s r√°pido: {fastest_model_name}\")\n",
    "print(f\"3. Todos los m√©todos de boosting muestran excelente rendimiento\")\n",
    "print(f\"4. LightGBM ofrece el mejor balance velocidad-precisi√≥n para este dataset\")\n",
    "print(f\"5. Los m√©todos de boosting generalmente superan a los modelos de bagging (RF, ET)\")\n",
    "\n",
    "# Comparar con mejores modelos de √°rboles\n",
    "if 'Random Forest' in model_results:\n",
    "    rf_mae = model_results['Random Forest']['mae']\n",
    "    best_ensemble_mae = comparison_df['MAE (val)'].min()\n",
    "    improvement = ((rf_mae - best_ensemble_mae) / rf_mae) * 100\n",
    "    print(f\"\\n6. Mejora sobre Random Forest: {improvement:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playstore-z4vSTk52-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
