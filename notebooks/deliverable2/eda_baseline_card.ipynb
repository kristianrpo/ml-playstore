{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Configuración de entorno\n",
    "\n",
    "En esta sección validamos que nuestro entorno de trabajo esté correctamente configurado antes de comenzar el análisis.  \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. **Versión de Python**  \n",
    "   - Se verifica que esté instalada la versión **3.11 o superior** (se recomienda 3.13).  \n",
    "   - Esto garantiza compatibilidad con librerías modernas de análisis de datos y machine learning.\n",
    "\n",
    "2. **Importación de librerías base**  \n",
    "   - Se cargan librerías fundamentales:  \n",
    "     - `numpy`, `pandas`: manipulación y análisis de datos.  \n",
    "     - `matplotlib`, `seaborn`: visualización de datos.  \n",
    "     - `scipy`: funciones estadísticas.  \n",
    "   - Además se configuran estilos gráficos y opciones de visualización en pandas para trabajar con tablas más grandes.\n",
    "\n",
    "3. **Verificación de versiones críticas**  \n",
    "   - Se comprueba que `scikit-learn` esté instalado y en una versión **>= 1.0.1**.  \n",
    "   - Esto es esencial ya que `scikit-learn` se usará para el modelado (baseline y posteriores).\n",
    "\n",
    "Con esta configuración inicial aseguramos que el entorno sea reproducible y que todas las dependencias necesarias estén listas antes de continuar con el **EDA** y el **baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "assert sys.version_info >= (3, 11), \"Este notebook trabajo con python 3.11 o superiores (recomendado 3.13)\"\n",
    "\n",
    "print(f\"Python {sys.version_info.major}.{sys.version_info.minor} instalado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiones de librerías críticas\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\"), \"Requiere scikit-learn >= 1.0.1\"\n",
    "print(f\"scikit-learn {sklearn.__version__} instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Metodología CRISP-DM\n",
    "## 2.1. Comprensión del Negocio\n",
    "El problema de Google Play Store  \n",
    "\n",
    "**Contexto:**  \n",
    "Es 2025. El mercado de aplicaciones móviles es altamente competitivo: millones de apps conviven en Google Play Store.  \n",
    "Los desarrolladores buscan mejorar la visibilidad de sus aplicaciones y los usuarios dependen del **rating promedio** para decidir qué descargar.  \n",
    "\n",
    "**Problema actual:**  \n",
    "- El rating se conoce **solo después** de que los usuarios descargan y reseñan.  \n",
    "- Las valoraciones son **altamente variables** y pueden depender de múltiples factores (categoría, descargas, precio, tamaño, tipo de app).  \n",
    "- Los desarrolladores carecen de una herramienta para **estimar la calificación potencial** de una app antes o durante su lanzamiento.  \n",
    "- La competencia es muy alta: una diferencia de décimas en rating puede significar miles de descargas menos.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.1. Solución propuesta  \n",
    "Construir un **sistema automático de predicción de rating** de apps a partir de sus características disponibles en el dataset de Google Play Store.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. Definiendo el éxito  \n",
    "\n",
    "**Métrica de negocio:**  \n",
    "- Ayudar a los desarrolladores a anticipar la valoración probable de su app.  \n",
    "- Reducir la dependencia de pruebas de mercado costosas o lentas.  \n",
    "- Identificar características clave que favorecen una alta valoración (≥ 4.3).  \n",
    "\n",
    "**Métrica técnica:**  \n",
    "- Lograr un **Error Absoluto Medio (MAE) < 0.5 estrellas** en la predicción de rating.  \n",
    "- Para la versión de clasificación (alta vs. baja calificación): obtener un **F1-score > 0.70**.  \n",
    "\n",
    "**¿Por qué estos valores?**  \n",
    "- El rating va de 1 a 5 → un error de 0.5 equivale a 10% de la escala.  \n",
    "- Una diferencia de medio punto puede marcar la visibilidad de la app en el ranking.  \n",
    "- Tasadores humanos (usuarios) también muestran variabilidad similar en sus calificaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3 Preguntas críticas antes de empezar  \n",
    "\n",
    "1. **¿Realmente necesitamos ML?**  \n",
    "   - Alternativa 1: Calcular el promedio de ratings por categoría → demasiado simple, no captura variabilidad.  \n",
    "   - Alternativa 2: Reglas heurísticas (ej. “si es gratis y tiene muchas descargas, tendrá rating alto”) → insuficiente.  \n",
    "   - **Conclusión:** Sí, ML es apropiado para capturar relaciones no lineales y múltiples factores.  \n",
    "\n",
    "2. **¿Qué pasa si el modelo falla?**  \n",
    "   - Transparencia: aclarar que es una estimación automática.  \n",
    "   - Complementar con rangos de predicción (ej: intervalo de confianza).  \n",
    "   - Mantener como referencia comparativa, no como único criterio de éxito.  \n",
    "\n",
    "3. **¿Cómo mediremos el impacto?**  \n",
    "   - Capacidad de anticipar apps con alta probabilidad de éxito.  \n",
    "   - Ahorro de tiempo en validaciones preliminares.  \n",
    "   - Insights para desarrolladores sobre qué factores influyen más en el rating.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2.2. Comprensión de los Datos  \n",
    "\n",
    "El objetivo de esta fase es explorar y entender el dataset de Google Play Store antes de construir modelos **(análisis exploratorio)**.  \n",
    "Nos centraremos en:  \n",
    "\n",
    "1. **Vista rápida del dataset**  \n",
    "   - Identificar dimensiones (filas × columnas).  \n",
    "   - Tipos de datos (numéricos, categóricos, texto, fechas).  \n",
    "   - Valores faltantes obvios y rangos sospechosos.\n",
    "\n",
    "2. **Descripción de variables**  \n",
    "   - Revisar cada columna y entender su significado.  \n",
    "   - Detectar qué variables podrían ser útiles como predictores y cuál será la variable objetivo (rating).  \n",
    "\n",
    "3. **Detección de problemas en los datos**  \n",
    "   - Análisis de valores faltantes.  \n",
    "   - Estrategias: eliminar filas/columnas, imputar valores o crear indicadores de “dato faltante”.  \n",
    "\n",
    "4. **Estadísticas descriptivas y univariadas**  \n",
    "   - Media vs mediana (sesgo de la distribución).  \n",
    "   - Desviación estándar (variabilidad, posibles outliers).  \n",
    "   - Mínimos/máximos sospechosos.  \n",
    "   - Histogramas para ver forma (normal, sesgada, bimodal, uniforme, picos extraños).  \n",
    "\n",
    "5. **Análisis de variables categóricas**  \n",
    "   - Distribución de categorías (ej. categorías de apps, tipo de app, content rating).  \n",
    "   - Detección de clases dominantes o categorías poco representadas.  \n",
    "\n",
    "6. **Correlaciones y relaciones entre variables**  \n",
    "   - Matriz de correlación de Pearson para variables numéricas.  \n",
    "   - Identificar relaciones fuertes, moderadas o débiles.  \n",
    "   - Importante: recordar que **correlación ≠ causalidad**.  \n",
    "7. **Análisis de outliers**\n",
    "   -  Tipos e identificación de outliers a través de diferentes métodos.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**  \n",
    "No siempre es necesario aplicar todos los pasos con igual profundidad.  \n",
    "- Para este proyecto, el foco está en **identificar variables relevantes para predecir el rating** y **limpiar datos inconsistentes**.  \n",
    "- Otros análisis más complejos (ej. NLP sobre descripciones) se pueden dejar como trabajo futuro (según trabajos de referencia investigados).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.2.1 Descarga de datos  \n",
    "\n",
    "En este paso descargamos el dataset de Google Play Store desde Kaggle y lo organizamos en la estructura de carpetas del proyecto.  \n",
    "\n",
    "1. Usamos la librería `kagglehub` para acceder al dataset público **`lava18/google-play-store-apps`** directamente desde Kaggle.  \n",
    "2. Se define una ruta clara dentro del proyecto para almacenar los datos originales: `../data/original/google-play-store/`. Esto ayuda a mantener la reproducibilidad y una estructura organizada.  \n",
    "3. Con la función `shutil.copytree` copiamos los archivos descargados a la carpeta destino. De esta forma, el dataset queda disponible en nuestro directorio de trabajo para su análisis posterior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "def download_data(origin_repository, target_folder):\n",
    "    # Descargar dataset\n",
    "    path = kagglehub.dataset_download(origin_repository)\n",
    "    \n",
    "    # Copiar los archivos descargados\n",
    "    shutil.copytree(path, target_folder, dirs_exist_ok=True)\n",
    "    \n",
    "\n",
    "download_data(\"lava18/google-play-store-apps\", \"../../data/original/google-play-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2.2 Carga de datos  \n",
    "\n",
    "En este paso realizamos la **lectura del archivo CSV** que contiene el dataset descargado previamente.  \n",
    "\n",
    "- Definimos una función `load_data(path, file)` que recibe la ruta y el nombre del archivo, y lo carga con `pandas.read_csv()`.  \n",
    "- Cargamos el dataset principal en la variable `applications_data` desde la carpeta `../data/original/google-play-store/`.  \n",
    "- Incluimos una verificación simple:  \n",
    "  - Si el dataset se carga con éxito, se imprime `\"Dataset loaded\"`.  \n",
    "  - En caso contrario, se muestra un mensaje de error.  \n",
    "\n",
    "Con esta validación aseguramos que el archivo esté disponible y correctamente leído antes de continuar con el análisis exploratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path, file):\n",
    "    return pd.read_csv(f\"{path}/{file}\")\n",
    "\n",
    "def convert_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte a numéricas solo las columnas que deberían serlo, sin tocar 'Size'.\n",
    "    Usa to_numeric(errors='coerce') para evitar ValueError si aparece texto.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rating\n",
    "    if \"Rating\" in df.columns:\n",
    "        df[\"Rating\"] = pd.to_numeric(df[\"Rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Reviews: quitar comas y cualquier carácter no numérico/punto\n",
    "    if \"Reviews\" in df.columns:\n",
    "        df[\"Reviews\"] = (\n",
    "            df[\"Reviews\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Installs: quitar +, comas y cualquier carácter no numérico/punto\n",
    "    if \"Installs\" in df.columns:\n",
    "        df[\"Installs Numeric\"] = (\n",
    "            df[\"Installs\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # Price: quitar $ y cualquier carácter no numérico/punto\n",
    "    if \"Price\" in df.columns:\n",
    "        df[\"Price\"] = (\n",
    "            df[\"Price\"].astype(str)\n",
    "            .str.replace(r\"[^\\d.]\", \"\", regex=True)\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "\n",
    "    if \"Size\" in df.columns:\n",
    "        def parse_size(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip()\n",
    "                if x.endswith(\"M\"):\n",
    "                    return float(x[:-1])\n",
    "                elif x.endswith(\"k\") or x.endswith(\"K\"):\n",
    "                    return float(x[:-1]) / 1024  # KB -> MB\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        df[\"Size\"] = df[\"Size\"].apply(parse_size)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "temp_applications_data = load_data(\"../../data/original/google-play-store\", \"googleplaystore.csv\")\n",
    "applications_data = convert_numeric_columns(temp_applications_data)\n",
    "\n",
    "\n",
    "if len(applications_data):\n",
    "    print(\"Dataset cargado\")\n",
    "else:\n",
    "    print(\"Error cargando dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.2.3 Vista rápida del dataset\n",
    "\n",
    "**Dimensiones y columnas**\n",
    "- Registros: **10,841** filas.\n",
    "- Columnas: actualmente **14**; **originalmente eran 13** y se **añadió** una columna derivada: **`Installs Numeric`** para análisis con describe.\n",
    "- Memoria aproximada: **~1.2 MB**.\n",
    "\n",
    "**Tipos de datos (y transformaciones realizadas)**\n",
    "- Numéricas (`float64`): `Rating`, `Reviews`, `Size`, `Price`, **`Installs Numeric`**.\n",
    "- Categóricas / texto (`object`): `App`, `Category`, `Installs` *(forma original con “1,000+”)*, `Type`, `Content Rating`, `Genres`, `Last Updated`, `Current Ver`, `Android Ver`.\n",
    "- Transformaciones ya aplicadas:\n",
    "  - **`Installs`** se **conservó** en su formato original (categórico con “+” y comas) **y** se creó **`Installs Numeric`** mapeando esos rangos a números (0 … 1,000,000,000).\n",
    "  - **`Price`**, **`Reviews`** y **`Size`** fueron normalizadas/parseadas a **numérico** para análisis y modelado.\n",
    "\n",
    "**Valores faltantes (no-null count → faltantes aprox.)**\n",
    "- `Rating`: 9,367 → **1,474 faltantes (~13.6%)**.\n",
    "- `Size`: 9,145 → **1,696 faltantes (~15.6%)**.\n",
    "- `Current Ver`: 10,833 → **8 faltantes (~0.07%)**.\n",
    "- `Android Ver`: 10,838 → **3 faltantes (~0.03%)**.\n",
    "- `Content Rating`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- `Price`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- `Installs Numeric`: 10,840 → **1 faltante (~0.01%)**.\n",
    "- Resto de columnas: **sin faltantes**.\n",
    "\n",
    "**Duplicados:**\n",
    "-   Se identificaron **483 filas duplicadas** (≈ **4.46%** del\n",
    "    dataset).\\\n",
    "-   Ejemplos de duplicados incluyen apps como:\n",
    "    -   *Quick PDF Scanner + OCR FREE*\\\n",
    "    -   *Box*\\\n",
    "    -   *Google My Business*\\\n",
    "    -   *ZOOM Cloud Meetings*\\\n",
    "    -   *join.me -- Simple Meetings*\\\n",
    "\n",
    "**Rangos y valores sospechosos (según `describe()`)**\n",
    "- `Rating`: **min = 1.0**, **max = 19.0** → **19** es inválido para la escala 1–5 (error de dato a corregir).\n",
    "- `Reviews`: media ~ **444k**, **p75 ≈ 54,768**, **max ≈ 78M** → valores altos plausibles; tratar como **outliers**.\n",
    "- `Size` (MB): media ~ **21.5**, **p50 = 13**, **p75 = 30**, **max = 100** → distribución sesgada a la derecha; mínimos muy bajos (**0.01**) a revisar.\n",
    "- `Price` (USD): **mediana = 0** y **p75 = 0** → la mayoría son **apps gratuitas**; **max = 400** sugiere outliers de precio.\n",
    "- `Installs Numeric`: **p25 = 1,000**, **p50 = 100,000**, **p75 = 5,000,000**, **max = 1,000,000,000** → escala muy amplia; conviene usar **transformaciones log** o **binning** en el EDA/modelado.\n",
    "\n",
    "**Conclusión inicial**\n",
    "- Los **faltantes** más relevantes están en `Rating` y `Size`; habrá que decidir estategía para aumentar, imputar o nivelar los datos.\n",
    "- Existen **outliers (no legítimos)** (ej. `Rating = 19`) y variables con **colas largas** (ej. `Reviews`, `Installs Numeric`, `Price`).\n",
    "- Eliminar **duplicados** para evitar sesgos de análisis y que no introduzcan ruidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(applications_data.head().style.background_gradient(cmap='RdYlGn', subset=['Rating']))\n",
    "\n",
    "# Información detallada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTRUCTURA DE DATOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "applications_data.info()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "display(applications_data.describe().round(2).T)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATOS DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar duplicados\n",
    "num_duplicados = applications_data.duplicated().sum()\n",
    "print(f\"Total de registros duplicados: {num_duplicados}\")\n",
    "\n",
    "# Mostrar ejemplos de duplicados si existen\n",
    "if num_duplicados > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas:\\n\")\n",
    "    display(applications_data[applications_data.duplicated()].head())\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.4 Descripción de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price',\n",
    "    'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver',\n",
    "    'Installs Numeric'\n",
    "]\n",
    "\n",
    "tipos = [\n",
    "    'Categórica',          # App\n",
    "    'Categórica',          # Category\n",
    "    'Numérica (Target)',   # Rating\n",
    "    'Numérica',            # Reviews\n",
    "    'Numérica (MB)',       # Size\n",
    "    'Categórica (rango)',  # Installs\n",
    "    'Categórica',          # Type\n",
    "    'Numérica (USD)',      # Price\n",
    "    'Categórica',          # Content Rating\n",
    "    'Categórica',          # Genres\n",
    "    'Texto (fecha)',       # Last Updated (parseable a fecha)\n",
    "    'Texto',               # Current Ver\n",
    "    'Texto',               # Android Ver\n",
    "    'Numérica',            # Installs Numeric\n",
    "]\n",
    "\n",
    "descripciones = [\n",
    "    'Nombre de la aplicación.',\n",
    "    'Categoría oficial de la app en Google Play.',\n",
    "    'Calificación promedio de usuarios (1 a 5).',\n",
    "    'Número de reseñas reportadas.',\n",
    "    'Tamaño aproximado de la app en MB.',\n",
    "    'Instalaciones en rango (p.ej., \"1,000+\").',\n",
    "    'Tipo de app (Free / Paid).',\n",
    "    'Precio en USD (0 para gratuitas).',\n",
    "    'Clasificación de contenido (Everyone, Teen, etc.).',\n",
    "    'Género(s) de la app.',\n",
    "    'Fecha de última actualización (texto en origen).',\n",
    "    'Versión actual declarada por el desarrollador.',\n",
    "    'Versión mínima de Android requerida.',\n",
    "    'Instalaciones convertidas a número para análisis.'\n",
    "]\n",
    "\n",
    "valores_faltantes = [applications_data[col].isnull().sum() if col in applications_data.columns else None for col in variables]\n",
    "\n",
    "metadata = {\n",
    "    'Variable': variables,\n",
    "    'Tipo': tipos,\n",
    "    'Descripción': descripciones,\n",
    "    'Valores Faltantes': valores_faltantes\n",
    "}\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# Mostrar con resaltado de faltantes\n",
    "styled = df_metadata.style.applymap(\n",
    "    lambda x: 'background-color: #ffcccc' if isinstance(x, (int, float)) and x > 0 else '',\n",
    "    subset=['Valores Faltantes']\n",
    ")\n",
    "\n",
    "display(styled)\n",
    "\n",
    "# Resumen de dtypes originales (informativo)\n",
    "dtypes_resumen = applications_data[variables].dtypes.astype(str).reset_index()\n",
    "dtypes_resumen.columns = ['Variable', 'dtype pandas']\n",
    "display(dtypes_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.2.5 Detección de problemas en los datos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**Resumen de hallazgos (valores faltantes):**\n",
    "- `Size` ≈ 15.6% y `Rating` ≈ 13.6% concentran la mayoría de los faltantes.\n",
    "- Faltantes puntuales (≈0.01%): `Type`, `Price`, `Content Rating`, `Installs Numeric` ocurren en la misma(s) fila(s) → patrón conjunto.\n",
    "- `Android Ver` (0.03%) y `Current Ver` (0.07%) con faltantes residuales, parcialmente correlacionados con el grupo anterior.\n",
    "\n",
    "**Heatmap de correlación de patrones de faltantes (interpretación):**\n",
    "- Correlación 1.00 entre `Type`, `Price`, `Content Rating`, `Installs Numeric`: las ausencias co-ocurren en el/los mismos registros. Acciones coordinadas.\n",
    "- `Android Ver` muestra correlación moderada (~0.58) con ese grupo: algunas veces falta junto con ellos.\n",
    "- `Size`, `Rating`, `Current Ver` tienen patrones de faltantes independientes del grupo anterior (correlaciones cercanas a 0), lo que sugiere causas distintas.\n",
    "\n",
    "#### Posibles estrategias de corrección\n",
    "\n",
    "- Limpieza básica\n",
    "  - Eliminar duplicados (483 filas) para evitar sesgos.\n",
    "  - Validar y corregir outliers imposibles, p. ej., `Rating = 19` → convertir a NaN para tratarlo como faltante.\n",
    "\n",
    "- Imputación (conservadora y por grupos)\n",
    "  - `Rating` (target): para modelado, eliminar filas sin `Rating`; para EDA descriptivo, imputar mediana por `Category` solo para visualización.\n",
    "  - `Size`: imputar mediana por `Category × Type` y crear indicador `size_missing`.\n",
    "  - `Android Ver`, `Current Ver`: imputar moda por `Category` y crear indicadores `androidver_missing`, `currentver_missing`.\n",
    "  - Faltantes conjuntos (`Type`, `Price`, `Content Rating`, `Installs Numeric`):\n",
    "    - Si es 1 fila: eliminarla es lo más simple y seguro.\n",
    "    - Alternativa (si se prefiere imputar):\n",
    "      - `Type`: inferir desde `Price` (0 → Free, >0 → Paid).\n",
    "      - `Price`: 0 si `Type == Free`, si `Paid` usar mediana por `Category`.\n",
    "      - `Content Rating`: moda por `Category`.\n",
    "      - `Installs Numeric`: mediana por `Category × Type` o por bin de `Installs`.\n",
    "\n",
    "\n",
    "\n",
    "#### Estrategias de “nivelación” según los porcentajes observados\n",
    "\n",
    "- Size (~15.6% faltantes, >5% y <<60%)\n",
    "  - Acción: imputar mediana por grupo `Category × Type`.\n",
    "  - Añadir flag: `size_missing = 1` cuando falte (conserva señal de ausencia).\n",
    "  - Justificación: volumen relevante; la mediana por grupos respeta diferencias entre tipos/categorías.\n",
    "\n",
    "- Rating (~13.6% faltantes, >5% y <<60%) [variable objetivo]\n",
    "  - Para modelado: eliminar filas sin `Rating` (evita sesgo por imputación del target).\n",
    "  - Para EDA descriptivo: si se requiere visualizar completos, imputar mediana por `Category` solo para gráficos/tablas (no para entrenamiento).\n",
    "  - Justificación: imputar el target puede distorsionar métricas.\n",
    "\n",
    "- Current Ver (0.07%) y Android Ver (0.03%) (<5%)\n",
    "  - Acción: imputar con la moda por `Category`. Flags opcionales `currentver_missing` y `androidver_missing`.\n",
    "  - Justificación: impacto ínfimo; moda es suficiente y estable.\n",
    "\n",
    "- Faltantes “en bloque” en la misma fila: Type, Price, Content Rating, Installs Numeric (≈0.01% cada uno; correlación 1.00)\n",
    "  - Si es 1 fila: eliminarla directamente.\n",
    "  - Si hubiera más en el futuro y se prefiriera imputar coordinadamente:\n",
    "    - `Type` desde `Price` (0 → Free, >0 → Paid),\n",
    "    - `Price` = 0 si `Free`, si `Paid` usar mediana por `Category`,\n",
    "    - `Content Rating` = moda por `Category`,\n",
    "    - `Installs Numeric` = mediana por `Category × Type`.\n",
    "  - Justificación: co-ocurren; eliminar 1 fila no afecta el conjunto y evita inconsistencias.\n",
    "\n",
    "- Transformaciones para estabilizar distribuciones (complementarias a la imputación)\n",
    "  - `Reviews` y `Installs Numeric`: aplicar `log1p` para análisis y futuros modelos. *****************************\n",
    "  - `Installs` (rangos): tratar como ordinal/bins en el EDA.\n",
    "\n",
    "- Limpieza previa necesaria\n",
    "  - Eliminar duplicados (483 filas).\n",
    "  - Corregir valores imposibles detectados en el EDA (ej. `Rating = 19` → NaN) y re-entrar al flujo de imputación/nivelación anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Análisis completo de valores faltantes con visualizaciones.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Faltantes': missing_counts.values,\n",
    "        'Porcentaje': missing_pct.values,\n",
    "        'Tipo_Dato': df.dtypes.values\n",
    "    })\n",
    "\n",
    "    missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "    if len(missing_df) == 0:\n",
    "        print(\"No hay valores faltantes en el dataset\")\n",
    "        return missing_df\n",
    "\n",
    "    # Visualización: barras y correlación de patrones de faltantes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Gráfico de barras de % faltantes\n",
    "    ax1.bar(missing_df['Columna'], missing_df['Porcentaje'], color='coral')\n",
    "    ax1.set_xlabel('Columna')\n",
    "    ax1.set_ylabel('Porcentaje de Valores Faltantes (%)')\n",
    "    ax1.set_title('Valores Faltantes por Columna')\n",
    "    ax1.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n",
    "    ax1.axhline(y=60, color='purple', linestyle='--', label='Umbral 60%')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Heatmap de correlación de patrones de faltantes\n",
    "    mask_df = df[missing_df['Columna'].tolist()].isnull().astype(int)\n",
    "    if mask_df.shape[1] >= 2:\n",
    "        corr = mask_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "        ax2.set_title('Correlación de Patrones de Valores Faltantes')\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Correlación de faltantes (no aplica: 1 columna)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "missing_analysis = analyze_missing_values(applications_data)\n",
    "if missing_analysis is not None and not missing_analysis.empty:\n",
    "    display(missing_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 2.2.6 Estadisticas descriptivas y univariadas (númerico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A partir de la tabla de estadísticas y los gráficos generados para `Rating`, `Reviews`, `Size`, `Price` e `Installs Numeric`, se observan los siguientes puntos clave.\n",
    "\n",
    "- Rating\n",
    "  - Media ≈ 4.19 y mediana ≈ 4.30 → ligera cola a la izquierda (más apps con rating alto). Hay un valor imposible (≈19), confirmado en el boxplot/Q-Q como outlier extremo.\n",
    "  - Outliers: ~5% por IQR, dominados por el valor inválido y algunos ratings bajos.\n",
    "  - Q-Q plot: desviación frente a normalidad, esperable para una variable acotada [1,5].\n",
    "  - Implicación/acción: eliminar filas sin `Rating` para modelado; corregir `Rating=19 → NaN` y excluir; no aplicar transformaciones (la escala es ya interpretables).\n",
    "\n",
    "- Reviews\n",
    "  - Media ≫ mediana (pico en 0–pocos miles; máximo ≈ 78M) → cola muy larga a la derecha.\n",
    "  - Boxplot: ~18% outliers por IQR (muchas apps con reseñas muy altas).\n",
    "  - Q-Q plot: gran desviación de normalidad (heavy tail).\n",
    "  - Relación con Rating: correlación positiva muy débil (~0.07), tendencia casi plana.\n",
    "  - Implicación/acción: usar `log1p(Reviews)` para estabilizar la distribución en análisis/modelado; considerar winsorizar p99.9 para vistas tabulares si se desea.\n",
    "\n",
    "- Size (MB)\n",
    "  - Media > mediana (≈ 21.5 vs 13) → sesgo a la derecha; valores hasta 100 MB.\n",
    "  - ~6% outliers por IQR, especialmente en colas altas.\n",
    "  - Q-Q plot: curvatura en colas; no normal.\n",
    "  - Relación con Rating: correlación positiva débil (~0.08); señal muy tenue.\n",
    "  - Implicación/acción: imputar faltantes por `Category × Type` y añadir `size_missing`; opcionalmente probar `log1p(Size)` o binning para robustecer.\n",
    "\n",
    "- Price (USD)\n",
    "  - Mediana = 0 (mayoría gratis) y cola a la derecha con máximos altos (≈ 400).\n",
    "  - ~7% outliers por IQR; Q-Q muestra heavy tail.\n",
    "  - Relación con Rating: correlación negativa muy débil (~-0.02).\n",
    "  - Implicación/acción: crear `is_free = (Price == 0)` y, si se usa `Price` continuo, considerar `log1p(Price)` para las pocas apps pagas; validar coherencia `Type=Free ⇒ Price=0`.\n",
    "\n",
    "- Installs Numeric\n",
    "  - Media ≫ mediana (100k) con máximo 1e9 → distribución extremadamente sesgada a la derecha.\n",
    "  - ~7–8% outliers por IQR; Q-Q muy alejado de normalidad.\n",
    "  - Relación con Rating: correlación débil positiva (~0.05) y tendencia casi plana.\n",
    "  - Implicación/acción: usar `log1p(Installs Numeric)` o bins ordinales para análisis; verificar coherencia con `Installs` textual.\n",
    "\n",
    "Recomendaciones transversales\n",
    "- Eliminar duplicados antes de resumir para evitar sesgos.\n",
    "- Tratar outliers evidentes no-legítimos (p. ej. `Rating=19`). Para colas largas legítimas (`Reviews`, `Installs Numeric`, `Price`): preferir `log1p` o winsorización solo para visualizaciones.\n",
    "- Mantener consistencia: `Type=Free ⇒ Price=0`; `Installs Numeric` coherente con el rango de `Installs`.\n",
    "- Para relaciones con `Rating`, las correlaciones lineales observadas son débiles; la señal puede emerger mejor con interacciones (p. ej., `is_free × installs_bin`) o modelos no lineales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Selección de columnas numéricas relevantes\n",
    "numeric_cols = [c for c in ['Rating', 'Reviews', 'Size', 'Price', 'Installs Numeric'] if c in applications_data.columns]\n",
    "\n",
    "# Tabla de estadísticas básicas (media, mediana, std, min, p25, p50, p75, max)\n",
    "describe_tbl = applications_data[numeric_cols].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "\n",
    "# Métricas adicionales robustas\n",
    "extra = pd.DataFrame(index=numeric_cols)\n",
    "extra['mad'] = [stats.median_abs_deviation(applications_data[c].dropna()) for c in numeric_cols]\n",
    "extra['skew'] = [applications_data[c].skew(skipna=True) for c in numeric_cols]\n",
    "extra['kurtosis'] = [applications_data[c].kurtosis(skipna=True) for c in numeric_cols]\n",
    "extra['cv'] = [applications_data[c].std(skipna=True) / applications_data[c].mean(skipna=True) if applications_data[c].mean(skipna=True) not in [0, np.nan] else np.nan for c in numeric_cols]\n",
    "\n",
    "stats_table = describe_tbl.join(extra)\n",
    "display(stats_table.round(3))\n",
    "\n",
    "\n",
    "def univariate_analysis(df: pd.DataFrame, column: str, target: str | None = None):\n",
    "    \"\"\"Análisis univariado con histograma, boxplot, Q-Q plot y relación con target.\"\"\"\n",
    "    series = df[column].dropna()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Histograma con líneas de media y mediana\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(series, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(series.mean(), color='red', linestyle='--', label=f\"Media: {series.mean():.2f}\")\n",
    "    ax1.axvline(series.median(), color='green', linestyle='--', label=f\"Mediana: {series.median():.2f}\")\n",
    "    ax1.set_title(f\"Distribución de {column}\")\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # 2) Boxplot + conteo de outliers (IQR)\n",
    "    ax2 = axes[0, 1]\n",
    "    bp = ax2.boxplot(series, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_mask = (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)\n",
    "    n_out = int(outliers_mask.sum())\n",
    "    pct_out = 100 * n_out / len(series) if len(series) else 0\n",
    "    ax2.set_title(f\"Boxplot de {column}\")\n",
    "    ax2.set_ylabel(column)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.text(1.1, Q3, f\"Outliers: {n_out} ({pct_out:.1f}%)\", fontsize=10)\n",
    "\n",
    "    # 3) Q-Q plot normal\n",
    "    ax3 = axes[1, 0]\n",
    "    stats.probplot(series, dist='norm', plot=ax3)\n",
    "    ax3.set_title('Q-Q Plot (Normalidad)')\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # 4) Relación con target si aplica\n",
    "    ax4 = axes[1, 1]\n",
    "    if target is not None and target in df.columns and column != target:\n",
    "        valid = df[[column, target]].dropna()\n",
    "        ax4.scatter(valid[column], valid[target], alpha=0.4, s=10)\n",
    "        ax4.set_xlabel(column)\n",
    "        ax4.set_ylabel(target)\n",
    "        ax4.set_title(f\"{column} vs {target}\")\n",
    "        # Línea de tendencia (ajuste lineal simple)\n",
    "        if len(valid) > 1:\n",
    "            z = np.polyfit(valid[column], valid[target], 1)\n",
    "            p = np.poly1d(z)\n",
    "            xs = np.linspace(valid[column].min(), valid[column].max(), 200)\n",
    "            ax4.plot(xs, p(xs), 'r--', alpha=0.8, label='Tendencia')\n",
    "            corr = valid[column].corr(valid[target])\n",
    "            ax4.text(0.05, 0.95, f\"Correlación: {corr:.3f}\", transform=ax4.transAxes,\n",
    "                     fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "            ax4.legend()\n",
    "    else:\n",
    "        ax4.axis('off')\n",
    "        ax4.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"Análisis Univariado: {column}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar análisis univariado para cada métrica numérica, relacionando con Rating\n",
    "for col in numeric_cols:\n",
    "    univariate_analysis(applications_data, col, target='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 2.2.7. Análisis Univariado Categórico\n",
    "- Category\n",
    "  - Distribución: alta concentración en `FAMILY` (~19%) y `GAME` (~12%). El resto de categorías tienen menor peso individual; el grupo `Others` acumula ~31% del total.\n",
    "  - Rating por categoría: diferencias moderadas; la **mediana** suele estar entre 4.2–4.4. Algunas categorías muestran desviación estándar mayor (p. ej., `PRODUCTIVITY`, `LIFESTYLE`), indicando más variabilidad de valoración.\n",
    "  - Implicaciones: riesgo de sesgo por categorías mayoritarias en análisis agregados. Para modelado, conviene usar dummies Top-K o codificación ordinal/target encoding con cuidado (evitar fuga). Agrupar colas largas en `Others` es adecuado para visualización.\n",
    "\n",
    "- Content Rating\n",
    "  - Distribución: `Everyone` domina (~79%), seguido por `Teen` (~12%); `Mature 17+` y `Everyone 10+` suman ~9% en conjunto; clases raras casi nulas.\n",
    "  - Rating por nivel de contenido: medias similares (≈4.1–4.3). `Teen` tiende a mediana 4.3 y variabilidad algo menor; `Mature 17+` muestra algo más de dispersión.\n",
    "  - Implicaciones: por el fuerte desbalance, esta variable aporta señal limitada por sí sola. Útil como interacción con `Category`/`Genres`.\n",
    "\n",
    "- Type\n",
    "  - Distribución: `Free` ≈ 93%, `Paid` ≈ 7% (clase muy desbalanceada); existe un registro anómalo (valor 0) en los gráficos que debe eliminarse/corregirse.\n",
    "  - Rating por tipo: medias muy cercanas (Free ≈ 4.19, Paid ≈ 4.27). La diferencia es pequeña y probablemente no significativa sin controlar otras variables (p. ej., `Category`).\n",
    "  - Implicaciones: por el desbalance extremo, conviene usar `is_free` como binaria y, si se modela interacción con `Installs` o `Price`, puede emerger señal. Validar regla `Type=Free ⇒ Price=0`.\n",
    "\n",
    "- Genres Main (primer género)\n",
    "  - Distribución: gran cola larga; `Others` concentra ~48%. Entre Top-12, `Tools`, `Entertainment` y `Education` destacan en frecuencia.\n",
    "  - Rating por género: diferencias pequeñas (medianas ~4.2–4.4), con algunas variaciones en dispersión (p. ej., `Medical` y `Lifestyle` más variables).\n",
    "  - Implicaciones: por la alta cardinalidad y colas largas, mantener Top-K + `Others` en EDA ayuda a la legibilidad. Para modelado, preferir codificación que reduzca dimensionalidad (Top-K dummies, hashing, o target encoding con validación adecuada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_compact(df: pd.DataFrame, cat_col: str, target_col: str, top_n: int = 12):\n",
    "    \"\"\"\n",
    "    Versión compacta para variables con muchas categorías:\n",
    "    - Ordena por frecuencia, muestra top_n y agrupa el resto en \"Others\".\n",
    "    - Barras horizontales, pie chart compacto, boxplot y tabla para top_n.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, target_col]].dropna(subset=[cat_col, target_col]).copy()\n",
    "    if data.empty:\n",
    "        print(f\"Sin datos para {cat_col} y {target_col}\")\n",
    "        return\n",
    "\n",
    "    counts = data[cat_col].value_counts()\n",
    "    top_cats = counts.head(top_n)\n",
    "    others_count = counts.iloc[top_n:].sum()\n",
    "\n",
    "    # Mapeo a top_n + Others\n",
    "    mapping = {c: c for c in top_cats.index}\n",
    "    data['__cat__'] = data[cat_col].where(data[cat_col].isin(top_cats.index), other='Others')\n",
    "\n",
    "    # Recalcular conteos con Others\n",
    "    counts_compact = data['__cat__'].value_counts()\n",
    "    order = list(top_cats.index) + (['Others'] if 'Others' in counts_compact.index else [])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) Barras horizontales (mejor legibilidad)\n",
    "    ax1 = axes[0, 0]\n",
    "    vals = counts_compact.loc[order]\n",
    "    ax1.barh(range(len(vals)), vals.values, color=plt.cm.Set3(range(len(vals))))\n",
    "    ax1.set_yticks(range(len(vals)))\n",
    "    ax1.set_yticklabels(order)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title(f'Distribución (Top {top_n}) de {cat_col}')\n",
    "    ax1.set_xlabel('Frecuencia')\n",
    "    for i, v in enumerate(vals.values):\n",
    "        ax1.text(v, i, f'  {v} ({v/len(data)*100:.1f}%)', va='center')\n",
    "\n",
    "    # 2) Pie chart compacto\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.pie(vals.values, labels=order, autopct='%1.1f%%', startangle=140,\n",
    "            colors=plt.cm.Set3(range(len(vals))))\n",
    "    ax2.set_title(f'Proporción (Top {top_n} + Others) de {cat_col}')\n",
    "\n",
    "    # 3) Boxplot del target por categoría (solo top_n)\n",
    "    ax3 = axes[1, 0]\n",
    "    top_mask = data['__cat__'] != 'Others'\n",
    "    data_top = data[top_mask]\n",
    "    data_top.boxplot(column=target_col, by='__cat__', ax=ax3)\n",
    "    ax3.set_title(f'{target_col} por {cat_col} (Top {top_n})')\n",
    "    ax3.set_xlabel(cat_col)\n",
    "    ax3.set_ylabel(target_col)\n",
    "    plt.sca(ax3)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # 4) Tabla de estadísticas por categoría (solo top_n y Others si existe)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    stats_by_cat = data.groupby('__cat__')[target_col].agg(['count', 'mean', 'median', 'std']).loc[order].round(2)\n",
    "    table = ax4.table(cellText=stats_by_cat.reset_index().values,\n",
    "                      colLabels=['Categoría', 'N', 'Media', 'Mediana', 'Desv.Est.'],\n",
    "                      cellLoc='center', loc='center', colWidths=[0.35, 0.12, 0.16, 0.16, 0.16])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.05, 1.25)\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#40E0D0')\n",
    "        table[(0, i)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.suptitle(f'Análisis Categórico Compacto: {cat_col}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecutar la versión compacta para las categóricas clave\n",
    "for cat in [c for c in ['Category', 'Content Rating', 'Type', 'Genres Main', 'Installs'] if c in applications_data.columns]:\n",
    "    analyze_categorical_compact(applications_data, cat, 'Rating', top_n=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 2.2.8. Análisis de correlación entre variables\n",
    "\n",
    "#### 2.2.8.1. Variables con Mayor Relación\n",
    "- Existe una fuerte correlación positiva entre **Installs Numeric** y **Reviews**:\n",
    "  - Pearson: 0.64 (relación lineal moderada-fuerte).\n",
    "  - Spearman: 0.97 (relación monótonica muy fuerte).\n",
    "- Esto implica que a mayor número de instalaciones, mayor número de reseñas.\n",
    "\n",
    "#### 2.2.8.2. Correlación de Pearson\n",
    "- En general, las correlaciones de Pearson muestran relaciones más débiles que Spearman, lo cual indica que las relaciones lineales no son tan marcadas.\n",
    "- **Installs Numeric y Reviews** presentan la correlación lineal más alta (0.64), siendo moderada-fuerte.\n",
    "- **Size y Reviews** muestran una correlación positiva baja/Débil (0.24).\n",
    "- El resto de variables (Rating, Price) tienen correlaciones casi nulas con las demás, lo que refleja poca relación lineal.\n",
    "\n",
    "#### 2.2.8.3. Correlación de Spearman\n",
    "- **Installs Numeric y Reviews** tienen la correlación más fuerte (0.97).\n",
    "- **Size** muestra correlación moderada con **Reviews** (0.37) y con **Installs Numeric** (0.35).\n",
    "- **Price** presenta correlaciones negativas con **Reviews** (-0.17) e **Installs Numeric** (-0.24).\n",
    "\n",
    "#### 2.2.8.4. Observaciones Clave\n",
    "- El número de instalaciones y las reseñas son las variables más relacionadas, lo cual es lógico, ya que más usuarios generan más interacciones.\n",
    "- El tamaño de la aplicación influye ligeramente en reseñas e instalaciones, pero no de forma determinante.\n",
    "- El precio no solo carece de relación positiva, sino que parece tener un impacto negativo sobre la popularidad (menos instalaciones y reseñas).\n",
    "\n",
    "#### 2.2.8.5. Conclusión\n",
    "- **Installs Numeric** y **Reviews** son las métricas más críticas en el dataset de **Google Play Store**, ya que reflejan el éxito y la popularidad de la aplicación.\n",
    "- **Size** es un factor secundario con cierta relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de correlación mejorado para el proyecto de Google Play Store\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Análisis de correlación con múltiples métricas\"\"\"\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Correlación de Pearson\n",
    "    corr_pearson = df[numeric_cols].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_pearson), k=1)\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', \n",
    "               cmap='coolwarm', center=0, ax=axes[0],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[0].set_title('Correlación de Pearson (Lineal)')\n",
    "    \n",
    "    # 2. Correlación de Spearman  \n",
    "    corr_spearman = df[numeric_cols].corr(method='spearman')\n",
    "    sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f',\n",
    "               cmap='coolwarm', center=0, ax=axes[1],\n",
    "               vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.8})\n",
    "    axes[1].set_title('Correlación de Spearman (Monotónica)')\n",
    "    \n",
    "    plt.suptitle('Análisis de Correlación Multi-métrica', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla de correlaciones importantes\n",
    "    print(\"\\n🔗 Correlaciones Significativas:\")\n",
    "    print(\"=\" * 50)\n",
    "    for method, corr_matrix in zip(['Pearson', 'Spearman'], [corr_pearson, corr_spearman]):\n",
    "        print(f\"\\n{method}:\")\n",
    "        significant_corr = corr_matrix[(abs(corr_matrix) > 0.3) & (corr_matrix != 1)].stack()\n",
    "        for (var1, var2), corr in significant_corr.items():\n",
    "            strength = \"Fuerte\" if abs(corr) > 0.5 else \"Moderada\" if abs(corr) > 0.3 else \"Débil\"\n",
    "            direction = \"Positiva\" if corr > 0 else \"Negativa\"\n",
    "            print(f\"  • {var1} y {var2}: {corr:+.3f} ({strength} {direction})\")\n",
    "    \n",
    "# Ejecutar el análisis de correlación\n",
    "correlation_analysis(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 2.2.9. Análisis de Outliers (IQR, Z-Score e Isolation Forest)\n",
    "**Resumen cuantitativo**\n",
    "- Total de registros analizados: **10,841**.\n",
    "- Filas marcadas como outlier por método:\n",
    "  - IQR: **3,489** filas (32.18%) → refleja colas largas especialmente en `Reviews`, `Installs Numeric`, `Price`.\n",
    "  - Z-Score (> |3|): **654** filas (6.03%) → mucho más selectivo, captura extremos verdaderamente alejados tras estandarización.\n",
    "  - Isolation Forest (contaminación=10%): **1,084** filas (10.0%) → patrón no lineal de anomalías combinadas.\n",
    "- Consenso entre métodos:\n",
    "  - Detectadas por los 3 métodos: **502** filas (casos altamente anómalos).\n",
    "  - Detectadas exactamente por 2 métodos: **731** filas (anómalas consistentes, revisar antes de decidir acción).\n",
    "\n",
    "**Variables más afectadas (IQR)**\n",
    "- `Reviews`: **1,925** outliers → distribución extremadamente sesgada; valores muy altos representan apps masivas (probablemente legítimos).\n",
    "- `Installs Numeric`: **828** outliers → escalas de descargas masivas (1e7–1e9).\n",
    "- `Price`: **800** outliers → pocos productos de precio elevado (≥ p75 + 1.5·IQR); revisar si son apps premium legítimas.\n",
    "- `Size`: **564** outliers → tamaños extremos (muy grandes o inusualmente pequeños).\n",
    "- `Rating`: **504** outliers → incluye valores extremos bajos y el caso inválido (`Rating=19`).\n",
    "\n",
    "**Interpretación y criterios**\n",
    "- Muchos outliers provienen de fenómenos de cola larga típicos (popularidad extrema o modelo freemium/premium).\n",
    "- No se recomienda eliminar masivamente outliers de `Reviews` o `Installs Numeric` sin antes transformar (`log1p`) o agrupar (binning), para no perder información sobre apps exitosas.\n",
    "- El valor inválido `Rating` debe eliminars. Otros ratings muy bajos pueden mantenerse (aportan contraste) (perfecto si se encuentran entre 1 y 5).\n",
    "- Outliers en `Price` podrían segmentarse: gratis (0), bajo costo (0 < p ≤ 10), premium (10 < p ≤ 50), ultra premium (>50).\n",
    "\n",
    "\n",
    "**Conclusión**\n",
    "El comportamiento extremo de `Reviews` e `Installs Numeric` refleja la naturaleza desigual del mercado (unas pocas apps concentran gran parte de la atención). Un manejo cuidadoso (transformaciones y flags) preservará información útil sin distorsionar el entrenamiento. Se prioriza limpieza puntual (ratings inválidos) sobre eliminación agresiva de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detección de outliers usando múltiples métodos\"\"\"\n",
    "    \n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Método 1: IQR\n",
    "    outliers_iqr = pd.DataFrame()\n",
    "    for col in numeric_df.columns:\n",
    "        Q1 = numeric_df[col].quantile(0.25)\n",
    "        Q3 = numeric_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric_df[col] < Q1 - 1.5 * IQR) | \n",
    "                   (numeric_df[col] > Q3 + 1.5 * IQR))\n",
    "        outliers_iqr[col] = outliers\n",
    "    \n",
    "    # Método 2: Z-Score\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(numeric_df.fillna(numeric_df.median())))\n",
    "    outliers_zscore = (z_scores > 3)\n",
    "    \n",
    "    # Método 3: Isolation Forest\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_df.fillna(numeric_df.median()))\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data) == -1\n",
    "    \n",
    "    # Visualización\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Outliers por columna (IQR)\n",
    "    ax1 = axes[0, 0]\n",
    "    outlier_counts = outliers_iqr.sum()\n",
    "    ax1.bar(range(len(outlier_counts)), outlier_counts.values)\n",
    "    ax1.set_xticks(range(len(outlier_counts)))\n",
    "    ax1.set_xticklabels(outlier_counts.index, rotation=45, ha='right')\n",
    "    ax1.set_title('Outliers por Variable (Método IQR)')\n",
    "    ax1.set_ylabel('Número de Outliers')\n",
    "    \n",
    "    # Plot 2: Distribución de outliers por método\n",
    "    ax2 = axes[0, 1]\n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'IQR': outliers_iqr.any(axis=1).sum(),\n",
    "        'Z-Score': outliers_zscore.any(axis=1).sum(),\n",
    "        'Isolation Forest': outliers_iso.sum()\n",
    "    }, index=['Outliers'])\n",
    "    methods_comparison.T.plot(kind='bar', ax=ax2, legend=False)\n",
    "    ax2.set_title('Comparación de Métodos de Detección')\n",
    "    ax2.set_ylabel('Número de Outliers Detectados')\n",
    "    ax2.set_xlabel('Método')\n",
    "    \n",
    "    # Plot 3: Heatmap de outliers\n",
    "    ax3 = axes[1, 0]\n",
    "    sample_outliers = outliers_iqr.head(100)\n",
    "    sns.heatmap(sample_outliers.T, cmap='RdYlBu_r', cbar=False, ax=ax3,\n",
    "               yticklabels=True, xticklabels=False)\n",
    "    ax3.set_title('Mapa de Outliers (Primeras 100 filas)')\n",
    "    ax3.set_xlabel('Observaciones')\n",
    "    \n",
    "    # Plot 4: Resumen estadístico\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Resumen de Detección de Anomalías:\n",
    "    \n",
    "    • Total de observaciones: {len(df):,}\n",
    "    • Outliers por IQR: {outliers_iqr.any(axis=1).sum():,} ({outliers_iqr.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    • Outliers por Z-Score: {outliers_zscore.any(axis=1).sum():,} ({outliers_zscore.any(axis=1).sum()/len(df)*100:.1f}%)\n",
    "    • Outliers por Isolation Forest: {outliers_iso.sum():,} ({outliers_iso.sum()/len(df)*100:.1f}%)\n",
    "    \n",
    "    Variables más afectadas:\n",
    "    {chr(10).join([f'  - {col}: {count:,} outliers' \n",
    "                   for col, count in outlier_counts.nlargest(3).items()])}\n",
    "    \n",
    "    Investigar outliers antes de eliminar. \n",
    "    Pueden contener información valiosa.\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('Análisis de Outliers y Anomalías', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers_iqr, outliers_zscore, outliers_iso\n",
    "\n",
    "# Ejecutar la detección de outliers en el dataset de aplicaciones\n",
    "outliers_iqr, outliers_zscore, outliers_iso = detect_outliers(applications_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 2.3. Preparación de los datos\n",
    "\n",
    "Con base a todos los hallazgos del **Análisis Exploratorio de Datos (EDA)**, aplicaremos las siguientes técnicas de limpieza y transformación:\n",
    "\n",
    "### 2.3.1. Resumen de problemas detectados\n",
    "\n",
    "Durante el EDA identificamos:\n",
    "\n",
    "1. **Duplicados**: 483 filas duplicadas (~4.46%)\n",
    "2. **Valores imposibles**: Rating = 19 (fuera del rango 1-5)\n",
    "3. **Valores faltantes**: \n",
    "   - Size ≈ 15.6%\n",
    "   - Rating ≈ 13.6%\n",
    "   - Current Ver, Android Ver, Content Rating, Type, Price (<1%)\n",
    "4. **Outliers legítimos**: Distribuciones con colas largas en Reviews, Installs Numeric, Price, Size\n",
    "5. **Variables con distribuciones sesgadas**: Requieren transformaciones logarítmicas\n",
    "6. **Inconsistencias**: Necesidad de validar coherencia entre Type y Price\n",
    "\n",
    "### 2.3.2. Plan de transformación\n",
    "\n",
    "Aplicaremos las siguientes transformaciones en orden:\n",
    "\n",
    "1. **Eliminación de duplicados**\n",
    "2. **Corrección de valores imposibles**\n",
    "3. **Imputación de valores faltantes** (estrategia por variable)\n",
    "4. **Validación de consistencia** entre variables relacionadas\n",
    "5. **Transformaciones de variables numéricas** (log, binning)\n",
    "6. **Creación de variables derivadas** (features engineering básico)\n",
    "7. **Resumen final** del dataset limpio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 2.3.3. Eliminación de Duplicados\n",
    "\n",
    "Eliminamos las **483 filas duplicadas** detectadas en el EDA para evitar:\n",
    "- Sesgos en análisis estadísticos\n",
    "- Sobrepeso de ciertas apps en el modelado\n",
    "- Distorsión de métricas de evaluación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset para transformaciones\n",
    "df_clean = applications_data.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 1: ELIMINACIÓN DE DUPLICADOS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado inicial\n",
    "print(f\"\\nRegistros antes de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"Duplicados encontrados: {df_clean.duplicated().sum():,} ({df_clean.duplicated().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Mostrar algunos ejemplos de duplicados antes de eliminar\n",
    "if df_clean.duplicated().sum() > 0:\n",
    "    print(\"\\nEjemplos de aplicaciones duplicadas:\")\n",
    "    duplicated_apps = df_clean[df_clean.duplicated(keep=False)].sort_values('App')\n",
    "    display(duplicated_apps[['App', 'Category', 'Rating', 'Reviews', 'Installs']].head(10))\n",
    "\n",
    "# Eliminar duplicados (manteniendo la primera ocurrencia)\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "\n",
    "# Estado final\n",
    "print(f\"\\nRegistros después de eliminar duplicados: {len(df_clean):,}\")\n",
    "print(f\"Filas eliminadas: {len(applications_data) - len(df_clean):,}\")\n",
    "print(f\"Reducción: {((len(applications_data) - len(df_clean))/len(applications_data)*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 2.3.4. Corrección de Valores Imposibles\n",
    "\n",
    "Corregimos valores que están fuera del rango válido:\n",
    "- Cualquier Rating < 1 o > 5 → Eliminar outlier, dado que es imposible que en la escala de Google Play se pueda obtener este tipo de rango, lo que demuestra un verdadero error\n",
    "\n",
    "#### 2.3.4.1. Análisis de Resultados de Corrección\n",
    "\n",
    "**Valores imposibles detectados:**\n",
    "\n",
    "**Ratings inválidos identificados:**\n",
    "- **1 registro** con Rating = 19.0 (fuera del rango válido 1-5)\n",
    "- **Aplicación afectada:** Identificada y eliminada del dataset\n",
    "- **Acción aplicada:** Eliminación completa del registro (no conversión a NaN)\n",
    "\n",
    "**Verificación de otros valores imposibles:**\n",
    "- **Reviews**: Sin valores negativos detectados\n",
    "- **Size**: Sin valores negativos detectados  \n",
    "- **Price**: Sin valores negativos detectados\n",
    "- **Installs Numeric**: Sin valores negativos detectados\n",
    "\n",
    "**Impacto de la corrección:**\n",
    "- **Registros eliminados:** 1 (impacto mínimo del 0.01%)\n",
    "- **Integridad del Rating:** 100% de valores dentro del rango [1-5]\n",
    "- **Registros finales:** 10,357 (después de eliminación de duplicados y valores imposibles)\n",
    "\n",
    "**Distribución final de Rating (valores válidos):**\n",
    "- **Rango confirmado:** 1.0 - 5.0 ✓\n",
    "- **Mediana:** ~4.3 (preservada después de la limpieza)\n",
    "- **Sin outliers imposibles:** Dataset completamente saneado\n",
    "\n",
    "**Conclusión:** La corrección fue exitosa eliminando el único valor imposible detectado (Rating=19), garantizando que todos los ratings restantes cumplan con la escala estándar de Google Play Store [1-5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 2: CORRECCIÓN DE VALORES IMPOSIBLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar valores de Rating fuera del rango [1, 5]\n",
    "invalid_ratings = df_clean['Rating'][(df_clean['Rating'] < 1) | (df_clean['Rating'] > 5)]\n",
    "print(f\"\\nRatings inválidos encontrados: {len(invalid_ratings)}\")\n",
    "\n",
    "if len(invalid_ratings) > 0:\n",
    "    print(\"\\nEjemplos de ratings inválidos:\")\n",
    "    invalid_apps = df_clean[df_clean['Rating'].isin(invalid_ratings)]\n",
    "    display(invalid_apps[['App', 'Category', 'Rating', 'Reviews']].head())\n",
    "    \n",
    "    # Mostrar distribución de valores inválidos\n",
    "    print(f\"\\nValores inválidos únicos: {sorted(invalid_ratings.dropna().unique())}\")\n",
    "    \n",
    "    # Eliminar registros con valores inválidos en lugar de poner NaN\n",
    "    df_clean = df_clean[~((df_clean['Rating'] < 1) | (df_clean['Rating'] > 5))].copy()\n",
    "    \n",
    "    print(f\"\\nRegistros inválidos eliminados.\")\n",
    "    print(f\"Total de registros ahora: {len(df_clean)}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron ratings inválidos\")\n",
    "\n",
    "# Verificar otros valores imposibles (negativos en columnas que no pueden serlo)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificando valores negativos en columnas numéricas:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numeric_cols = ['Reviews', 'Size', 'Price', 'Installs Numeric']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"{col}: {negative_count} valores negativos encontrados\")\n",
    "            df_clean = df_clean[df_clean[col] >= 0].copy()\n",
    "            print(f\"Registros inválidos eliminados de {col}\")\n",
    "        else:\n",
    "            print(f\"{col}: Sin valores negativos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2.3.5. Validación de Consistencia entre Variables\n",
    "\n",
    "Validamos y corregimos inconsistencias lógicas entre variables relacionadas:\n",
    "- **Type vs Price**: Si `Type = 'Free'`, entonces `Price` debe ser 0\n",
    "- **Type vs Price**: Si `Price > 0`, entonces `Type` debe ser 'Paid'\n",
    "\n",
    "#### 2.3.5.1. Análisis de Resultados de Validación\n",
    "\n",
    "**Estado de consistencia encontrado:**\n",
    "\n",
    "**Consistencia perfecta en apps existentes:**\n",
    "- **0 apps** marcadas como 'Free' con Price > 0\n",
    "- **0 apps** marcadas como 'Paid' con Price = 0\n",
    "- El dataset original ya mantenía la lógica de negocio correcta\n",
    "\n",
    "**Corrección aplicada:**\n",
    "- **1 registro** con Type faltante fue corregido automáticamente\n",
    "- **Estrategia:** Inferencia desde Price (Price = 0 → Type = 'Free')\n",
    "- **Resultado:** 0 faltantes restantes en Type\n",
    "\n",
    "**Distribución final validada:**\n",
    "- **Free**: 9,592 apps (92.6%)\n",
    "- **Paid**: 765 apps (7.4%)\n",
    "- **Total**: 10,357 apps con consistencia 100% validada\n",
    "\n",
    "**Conclusión:** La validación confirmó que el dataset original ya mantenía coherencia lógica entre Type y Price, requiriendo solo la corrección menor de 1 registro con Type faltante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 3: VALIDACIÓN DE CONSISTENCIA ENTRE VARIABLES\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validar consistencia Type vs Price\n",
    "print(\"\\nValidando consistencia entre Type y Price:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Casos inconsistentes: Type='Free' pero Price > 0\n",
    "free_but_paid = df_clean[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0)]\n",
    "print(f\"\\nApps marcadas como 'Free' pero con Price > 0: {len(free_but_paid)}\")\n",
    "if len(free_but_paid) > 0:\n",
    "    display(free_but_paid[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price > 0, cambiar Type a 'Paid'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Free') & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    print(f\"Corregido: Type cambiado a 'Paid'\")\n",
    "\n",
    "# Casos inconsistentes: Type='Paid' pero Price = 0\n",
    "paid_but_free = df_clean[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0)]\n",
    "print(f\"\\nApps marcadas como 'Paid' pero con Price = 0: {len(paid_but_free)}\")\n",
    "if len(paid_but_free) > 0:\n",
    "    display(paid_but_free[['App', 'Category', 'Type', 'Price']].head())\n",
    "    # Corregir: si Price = 0, cambiar Type a 'Free'\n",
    "    df_clean.loc[(df_clean['Type'] == 'Paid') & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    print(f\"Corregido: Type cambiado a 'Free'\")\n",
    "\n",
    "# Inferir Type desde Price cuando Type es NaN\n",
    "type_missing = df_clean['Type'].isnull()\n",
    "if type_missing.sum() > 0:\n",
    "    print(f\"\\nType faltante en {type_missing.sum()} registros\")\n",
    "    print(\"Infiriendo Type desde Price...\")\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] == 0), 'Type'] = 'Free'\n",
    "    df_clean.loc[type_missing & (df_clean['Price'] > 0), 'Type'] = 'Paid'\n",
    "    remaining_missing = df_clean['Type'].isnull().sum()\n",
    "    print(f\"Type inferido. Faltantes restantes: {remaining_missing}\")\n",
    "\n",
    "print(\"\\nValidación de consistencia completada\")\n",
    "print(f\"Distribución final de Type:\")\n",
    "print(df_clean['Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 2.3.6. División Estratificada del Dataset (Train/Val/Test)\n",
    "\n",
    "**IMPORTANTE: Prevención de Data Leakage**\n",
    "\n",
    "Antes de realizar cualquier imputación o transformación que calcule estadísticas (medianas, modas, etc.), debemos dividir el dataset para evitar **fugas de información del futuro**.\n",
    "\n",
    "**Estrategia de división:**\n",
    "1. Eliminar filas sin Rating (target) → Solo entrenaremos con datos completos\n",
    "2. División estratificada 70/15/15:\n",
    "   - **Train (70%)**: Para calcular estadísticas de imputación y ajustar SIN FILTRAR INFORMACIÓN DEL FUTURO\n",
    "   - **Validation (15%)**: Para validación durante el desarrollo\n",
    "   - **Test (15%)**: Para evaluación final (nunca se usa hasta el final)\n",
    "3. Estratificación por Rating para mantener distribución del target (al ser númerico, creamos bins para hacer la estratificación)\n",
    "\n",
    "**Principio clave:** Las estadísticas (mediana de Size, moda de Content Rating, etc.) se calculan SOLO con train y se aplican a val/test.\n",
    "\n",
    "**Conclusión:** La gráfica muestra que la distribución de Rating en los tres conjuntos (train, val, test) es prácticamente idéntica, tanto en forma como en valores de media y mediana. Esto indica que la estratificación por bins de Rating fue exitosa y que no hay sesgo entre los splits.\n",
    "La media y mediana se mantienen estables (≈4.19–4.20 y ≈4.30), lo que garantiza que el modelo se entrenará, validará y evaluará sobre datos representativos y comparables.\n",
    "Por lo tanto, la división estratificada preserva la estructura original del target y previene data leakage, asegurando resultados robustos y generalizables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: DIVISIÓN ESTRATIFICADA DEL DATASET\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estado antes de dividir\n",
    "print(f\"\\nDataset después de limpieza básica:\")\n",
    "print(f\"  Total de registros: {len(df_clean):,}\")\n",
    "print(f\"  Rating faltantes: {df_clean['Rating'].isnull().sum():,} ({df_clean['Rating'].isnull().sum()/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# 1. ELIMINAR filas sin Rating (no podemos entrenar con ellas)\n",
    "df_model = df_clean.dropna(subset=['Rating']).copy()\n",
    "print(f\"\\nDespués de eliminar filas sin Rating:\")\n",
    "print(f\"  Registros disponibles para modelado: {len(df_model):,}\")\n",
    "print(f\"  Registros descartados: {len(df_clean) - len(df_model):,}\")\n",
    "\n",
    "# 2. DIVISIÓN ESTRATIFICADA: Train (70%), Temp (30%)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Dividiendo dataset (estratificado por Rating)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear bins de Rating para estratificación más robusta\n",
    "df_model['rating_bin'] = pd.cut(df_model['Rating'], bins=[0, 3, 4, 4.5, 5], labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
    "\n",
    "train, temp = train_test_split(\n",
    "    df_model,\n",
    "    test_size=0.30,\n",
    "    stratify=df_model['rating_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Dividir Temp en Val (50%) y Test (50%) → 15% y 15% del total\n",
    "val, test = train_test_split(\n",
    "    temp,\n",
    "    test_size=0.50,\n",
    "    stratify=temp['rating_bin'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Eliminar columna auxiliar de binning\n",
    "train = train.drop(columns=['rating_bin'])\n",
    "val = val.drop(columns=['rating_bin'])\n",
    "test = test.drop(columns=['rating_bin'])\n",
    "\n",
    "# Reindexar para evitar problemas\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDivisión completada:\")\n",
    "print(f\"  Train: {len(train):,} registros ({len(train)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val):,} registros ({len(val)/len(df_model)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test):,} registros ({len(test)/len(df_model)*100:.1f}%)\")\n",
    "\n",
    "# 4. VERIFICAR ESTRATIFICACIÓN (distribución de Rating debe ser similar)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Verificación de estratificación:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rating_dist = pd.DataFrame({\n",
    "    'Train_mean': [train['Rating'].mean()],\n",
    "    'Val_mean': [val['Rating'].mean()],\n",
    "    'Test_mean': [test['Rating'].mean()],\n",
    "    'Train_std': [train['Rating'].std()],\n",
    "    'Val_std': [val['Rating'].std()],\n",
    "    'Test_std': [test['Rating'].std()]\n",
    "})\n",
    "display(rating_dist.round(3))\n",
    "\n",
    "# Visualización de distribuciones\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, data) in zip(axes, [('Train', train), ('Val', val), ('Test', test)]):\n",
    "    ax.hist(data['Rating'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(data['Rating'].mean(), color='red', linestyle='--', label=f'Media: {data[\"Rating\"].mean():.2f}')\n",
    "    ax.axvline(data['Rating'].median(), color='green', linestyle='--', label=f'Mediana: {data[\"Rating\"].median():.2f}')\n",
    "    ax.set_title(f'Distribución de Rating - {name}')\n",
    "    ax.set_xlabel('Rating')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDivisión estratificada completada exitosamente\")\n",
    "print(\"IMPORTANTE: A partir de ahora, SOLO usaremos 'train' para calcular estadísticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 2.3.7. Imputación de Valores Faltantes (Sin Data Leakage)\n",
    "\n",
    "---\n",
    "\n",
    "####  Calcular estadísticas **solo con `train`**\n",
    "\n",
    "Todas las estadísticas utilizadas para la imputación (medianas, modas, etc.) se calculan **exclusivamente sobre el conjunto de entrenamiento (`train`)**.  \n",
    "Esto evita que el modelo tenga acceso indirecto a información de validación o prueba (futuro), previniendo así **data leakage**.\n",
    "\n",
    "> Ejemplo: las medianas de `Size` se obtienen por combinación `Category × Type` **solo a partir de `train`**, y luego se aplican a `val` y `test`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Aplicar las mismas estadísticas a `val` y `test`\n",
    "\n",
    "Las estadísticas aprendidas del `train` se reutilizan directamente en los conjuntos de validación y prueba.  \n",
    "No se recalculan en esos conjuntos, lo que asegura **consistencia en la imputación** y **evita fuga de información**.\n",
    "\n",
    "> Si una categoría no existe en `train`, se aplica una **mediana o moda global** (fallback), calculada también desde `train`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Crear *flags* de valores faltantes **antes de imputar**\n",
    "\n",
    "Antes de reemplazar los valores nulos, se generan columnas binarias (`_missing`) que indican la presencia de datos faltantes.  \n",
    "Esto permite al modelo aprender si la ausencia de información tiene relevancia predictiva.\n",
    "\n",
    "> Ejemplo de *flags*:  \n",
    "> `size_missing`, `content_rating_missing`, `price_missing`, `android_ver_missing`, `current_ver_missing`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Métodos de imputación por variable\n",
    "\n",
    "| Variable | Método de imputación | Nivel de agrupación | Fallback |\n",
    "|-----------|---------------------|---------------------|-----------|\n",
    "| **`Size`** | Mediana | `Category × Type` | Mediana global |\n",
    "| **`Content Rating`** | Moda | `Category` | Moda global |\n",
    "| **`Android Ver`** | Moda | `Category` | Moda global |\n",
    "| **`Current Ver`** | Moda | `Category` | Moda global |\n",
    "| **`Price`** | `0` si `Free`, mediana si `Paid` | `Category` | Mediana global (`Paid`) |\n",
    "\n",
    "Cada variable mantiene la misma política: **calcular con `train`, aplicar en `val` y `test`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validación final de imputación\n",
    "\n",
    "Después del proceso, se confirma que **ningún conjunto tenga valores faltantes**.  \n",
    "Esto garantiza que la imputación fue exitosa y que los *flags* sean la única señal de ausencia original.\n",
    "\n",
    "> Ejemplo de salida esperada:\n",
    "> ```\n",
    "> Train faltantes: 0\n",
    "> Val faltantes: 0\n",
    "> Test faltantes: 0\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "#### Beneficios de la metodología\n",
    "\n",
    "| Aspecto | Beneficio |\n",
    "|----------|------------|\n",
    "| **Prevención de fuga de datos** | Solo se usan estadísticas del conjunto de entrenamiento. |\n",
    "| **Consistencia entre conjuntos** | Mismo proceso aplicado a `train`, `val` y `test`. |\n",
    "| **Trazabilidad de imputaciones** | Los flags `_missing` permiten capturar patrones de ausencia. |\n",
    "| **Robustez general** | Cada variable se trata según su naturaleza (numérica o categórica). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 4: IMPUTACIÓN DE VALORES FALTANTES (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMETODOLOGÍA:\")\n",
    "print(\"  1. Calcular estadísticas SOLO con train\")\n",
    "print(\"  2. Aplicar las mismas estadísticas a val y test\")\n",
    "print(\"  3. Crear flags de faltantes ANTES de imputar\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# CREAR FLAGS DE FALTANTES (antes de imputar)\n",
    "# ==============================================================================\n",
    "print(\"\\nCreando flags de valores faltantes...\")\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['size_missing'] = df['Size'].isnull().astype(int)\n",
    "    df['content_rating_missing'] = df['Content Rating'].isnull().astype(int)\n",
    "    df['android_ver_missing'] = df['Android Ver'].isnull().astype(int)\n",
    "    df['current_ver_missing'] = df['Current Ver'].isnull().astype(int)\n",
    "    df['price_missing'] = df['Price'].isnull().astype(int)\n",
    "\n",
    "print(\"Flags creados en train, val y test\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SIZE: Calcular medianas por Category × Type usando SOLO TRAIN\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando SIZE (mediana por Category × Type)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular medianas SOLO con train\n",
    "size_medians = train.groupby(['Category', 'Type'])['Size'].median()\n",
    "size_global_median = train['Size'].median()\n",
    "\n",
    "print(f\"Medianas calculadas con train: {len(size_medians)} grupos\")\n",
    "print(f\"Mediana global (fallback): {size_global_median:.2f} MB\")\n",
    "\n",
    "# Función para imputar usando mapping precalculado\n",
    "def impute_size(df, medians_map, global_median):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Size'].isnull()].iterrows():\n",
    "        cat, typ = row['Category'], row['Type']\n",
    "        if (cat, typ) in medians_map.index:\n",
    "            df.loc[idx, 'Size'] = medians_map.loc[(cat, typ)]\n",
    "        else:\n",
    "            df.loc[idx, 'Size'] = global_median\n",
    "    return df\n",
    "\n",
    "# Aplicar a train, val, test\n",
    "train = impute_size(train, size_medians, size_global_median)\n",
    "val = impute_size(val, size_medians, size_global_median)\n",
    "test = impute_size(test, size_medians, size_global_median)\n",
    "\n",
    "print(f\"Size imputado en todos los conjuntos\")\n",
    "print(f\"   Train faltantes: {train['Size'].isnull().sum()}\")\n",
    "print(f\"   Val faltantes: {val['Size'].isnull().sum()}\")\n",
    "print(f\"   Test faltantes: {test['Size'].isnull().sum()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONTENT RATING: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando CONTENT RATING (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "content_rating_modes = train.groupby('Category')['Content Rating'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "content_rating_global_mode = train['Content Rating'].mode()[0]\n",
    "\n",
    "print(f\"Modas calculadas con train: {len(content_rating_modes)} categorías\")\n",
    "\n",
    "def impute_content_rating(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Content Rating'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Content Rating'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Content Rating'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_content_rating(train, content_rating_modes, content_rating_global_mode)\n",
    "val = impute_content_rating(val, content_rating_modes, content_rating_global_mode)\n",
    "test = impute_content_rating(test, content_rating_modes, content_rating_global_mode)\n",
    "\n",
    "print(f\"Content Rating imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ANDROID VER: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando ANDROID VER (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "android_ver_modes = train.groupby('Category')['Android Ver'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "android_ver_global_mode = train['Android Ver'].mode()[0]\n",
    "\n",
    "def impute_android_ver(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Android Ver'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Android Ver'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Android Ver'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_android_ver(train, android_ver_modes, android_ver_global_mode)\n",
    "val = impute_android_ver(val, android_ver_modes, android_ver_global_mode)\n",
    "test = impute_android_ver(test, android_ver_modes, android_ver_global_mode)\n",
    "\n",
    "print(f\"Android Ver imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CURRENT VER: Moda por Category (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando CURRENT VER (moda por Category)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "current_ver_modes = train.groupby('Category')['Current Ver'].agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "current_ver_global_mode = train['Current Ver'].mode()[0]\n",
    "\n",
    "def impute_current_ver(df, modes_map, global_mode):\n",
    "    df = df.copy()\n",
    "    for idx, row in df[df['Current Ver'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in modes_map.index and pd.notna(modes_map.loc[cat]):\n",
    "            df.loc[idx, 'Current Ver'] = modes_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Current Ver'] = global_mode\n",
    "    return df\n",
    "\n",
    "train = impute_current_ver(train, current_ver_modes, current_ver_global_mode)\n",
    "val = impute_current_ver(val, current_ver_modes, current_ver_global_mode)\n",
    "test = impute_current_ver(test, current_ver_modes, current_ver_global_mode)\n",
    "\n",
    "print(f\"Current Ver imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. PRICE: 0 si Free, mediana por Category si Paid (solo train)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Imputando PRICE (0 si Free, mediana por Category si Paid)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular medianas de Price para apps Paid por Category (solo train)\n",
    "price_medians_paid = train[train['Type'] == 'Paid'].groupby('Category')['Price'].median()\n",
    "price_global_median_paid = train[train['Type'] == 'Paid']['Price'].median()\n",
    "\n",
    "def impute_price(df, medians_paid_map, global_median_paid):\n",
    "    df = df.copy()\n",
    "    # Free apps → 0\n",
    "    mask_free = (df['Type'] == 'Free') & df['Price'].isnull()\n",
    "    df.loc[mask_free, 'Price'] = 0\n",
    "    \n",
    "    # Paid apps → mediana por Category\n",
    "    for idx, row in df[(df['Type'] == 'Paid') & df['Price'].isnull()].iterrows():\n",
    "        cat = row['Category']\n",
    "        if cat in medians_paid_map.index and pd.notna(medians_paid_map.loc[cat]):\n",
    "            df.loc[idx, 'Price'] = medians_paid_map.loc[cat]\n",
    "        else:\n",
    "            df.loc[idx, 'Price'] = global_median_paid\n",
    "    return df\n",
    "\n",
    "train = impute_price(train, price_medians_paid, price_global_median_paid)\n",
    "val = impute_price(val, price_medians_paid, price_global_median_paid)\n",
    "test = impute_price(test, price_medians_paid, price_global_median_paid)\n",
    "\n",
    "print(f\"Price imputado\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPUTACIÓN COMPLETADA SIN DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nValores faltantes restantes:\")\n",
    "for name, df in [('Train', train), ('Val', val), ('Test', test)]:\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if len(missing) == 0:\n",
    "        print(f\"  {name}: Sin valores faltantes\")\n",
    "    else:\n",
    "        print(f\"  {name}: {missing.to_dict()}\")\n",
    "\n",
    "print(\"\\nFlags de trazabilidad creados:\")\n",
    "print(f\"  - size_missing\")\n",
    "print(f\"  - content_rating_missing\")\n",
    "print(f\"  - android_ver_missing\")\n",
    "print(f\"  - current_ver_missing\")\n",
    "print(f\"  - price_missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 2.3.7. Transformaciones de Variables Numéricas (Sin Data Leakage)\n",
    "\n",
    "En esta etapa se aplicaron transformaciones diseñadas para **reducir la asimetría**, **mejorar la interpretabilidad** y **preparar las variables numéricas** para los modelos, garantizando que ninguna transformación utilizara información de validación o prueba.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.7.1. Log-transformaciones\n",
    "\n",
    "Se aplicaron transformaciones logarítmicas a variables con distribuciones altamente sesgadas y colas largas.  \n",
    "Estas transformaciones se realizaron **de forma directa (sin estadísticos derivados)**, por lo que **no generan data leakage**.\n",
    "\n",
    "**Variables transformadas:**\n",
    "- `Reviews_log = log1p(Reviews)`\n",
    "- `Installs_log = log1p(Installs Numeric)`\n",
    "- `Size_log = log1p(Size)`\n",
    "\n",
    "**Resultados en el conjunto de entrenamiento:**\n",
    "- `Reviews_log`: Media = **8.26**, Mediana = **8.44**  \n",
    "- `Installs_log`: Media = **12.18**, Mediana = **13.12**  \n",
    "- `Size_log`: Media = **2.64**, Mediana = **2.64**\n",
    "\n",
    "> Estas transformaciones suavizan la escala exponencial de las variables y mejoran la linealidad con respecto al target.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.3.7.3. Variables binarias derivadas\n",
    "\n",
    "Se generaron nuevas variables booleanas basadas en reglas de negocio fijas, lo que facilita que el modelo capture relaciones no lineales simples.\n",
    "\n",
    "**Variables creadas:**\n",
    "- `is_free`: 1 si la app es gratuita (`Type == 'Free'`) → **5798 apps (93.2%)**\n",
    "- `is_large_app`: 1 si el tamaño > 50 MB  \n",
    "- `has_high_installs`: 1 si `Installs Numeric > 1,000,000` → **1793 apps (28.8%)**\n",
    "- `is_top_category`: 1 si pertenece a `FAMILY` o `GAME`\n",
    "- `is_everyone_rated`: 1 si `Content Rating == 'Everyone'`\n",
    "- `large_and_popular`: combinación de `is_large_app` & `has_high_installs`\n",
    "\n",
    "> Estas variables mejoran la capacidad del modelo para identificar patrones de negocio relevantes (por ejemplo, apps grandes y populares tienden a obtener mejores ratings).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 5: TRANSFORMACIONES DE VARIABLES NUMERICAS (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. LOG-TRANSFORMACIONES (sin data leakage, son transformaciones puntuales)\n",
    "# ==============================================================================\n",
    "print(\"\\nAplicando transformaciones logarítmicas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['Reviews_log'] = np.log1p(df['Reviews'])\n",
    "    df['Installs_log'] = np.log1p(df['Installs Numeric'])\n",
    "    df['Size_log'] = np.log1p(df['Size'])\n",
    "\n",
    "print(\"Variables originales eliminadas tras log-transformación: Reviews, Installs Numeric, Size\")\n",
    "print(\"Transformaciones log aplicadas a train, val, test\")\n",
    "print(f\"\\n   Train - Reviews_log: Media {train['Reviews_log'].mean():.2f}, Mediana {train['Reviews_log'].median():.2f}\")\n",
    "print(f\"   Train - Installs_log: Media {train['Installs_log'].mean():.2f}, Mediana {train['Installs_log'].median():.2f}\")\n",
    "print(f\"   Train - Size_log: Media {train['Size_log'].mean():.2f}, Mediana {train['Size_log'].median():.2f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. VARIABLES BINARIAS (sin data leakage, son reglas fijas)\n",
    "# ==============================================================================\n",
    "print(\"\\n\\nCreando variables binarias...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['is_free'] = (df['Type'] == 'Free').astype(int)\n",
    "    df['is_large_app'] = (df['Size'] > 50).astype(int)\n",
    "    df['has_high_installs'] = (df['Installs Numeric'] > 1000000).astype(int)\n",
    "    df['is_top_category'] = df['Category'].isin(['FAMILY', 'GAME']).astype(int)\n",
    "    df['is_everyone_rated'] = (df['Content Rating'] == 'Everyone').astype(int)\n",
    "    df['large_and_popular'] = (df['is_large_app'] & df['has_high_installs']).astype(int)\n",
    "\n",
    "print(\"Variables binarias creadas\")\n",
    "print(f\"\\n   Train - is_free: {train['is_free'].sum()} ({train['is_free'].mean()*100:.1f}%)\")\n",
    "print(f\"   Train - has_high_installs: {train['has_high_installs'].sum()} ({train['has_high_installs'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTransformaciones completadas sin data leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 2.3.8. Creacion de Variables Derivadas (Feature Engineering Básico)\n",
    "\n",
    "Creamos nuevas variables combinando informacion existente para capturar patrones mas complejos, y eliminamos aquellas redundantes (procesos intermedios, reemplazadas por log1p, etc), que no nos brindan señal predictiva:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PASO 6: FEATURE ENGINEERING (Sin Data Leakage)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    df['review_rate'] = df['Reviews'] / (df['Installs Numeric'] + 1)\n",
    "    df['Last Updated Parsed'] = pd.to_datetime(df['Last Updated'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2025-10-02')\n",
    "    df['days_since_update'] = (reference_date - df['Last Updated Parsed']).dt.days\n",
    "    df['update_recency'] = pd.cut(\n",
    "        df['days_since_update'],\n",
    "        bins=[-1, 30, 90, 180, 365, 730, 10000],\n",
    "        labels=['<1 month', '1-3 months', '3-6 months', '6-12 months', '1-2 years', '>2 years']\n",
    "    )\n",
    "    df['size_per_install'] = df['Size'] / (df['Installs Numeric'] + 1)\n",
    "    \n",
    "\n",
    "print(\"\\nCalculando popularity score (normalizadores desde train)...\")\n",
    "installs_min_train = train['Installs Numeric'].min()\n",
    "installs_max_train = train['Installs Numeric'].max()\n",
    "reviews_min_train = train['Reviews'].min()\n",
    "reviews_max_train = train['Reviews'].max()\n",
    "\n",
    "for df_name, df in [('train', train), ('val', val), ('test', test)]:\n",
    "    installs_norm = (df['Installs Numeric'] - installs_min_train) / (installs_max_train - installs_min_train)\n",
    "    reviews_norm = (df['Reviews'] - reviews_min_train) / (reviews_max_train - reviews_min_train)\n",
    "    df['popularity_score'] = (installs_norm * 0.7 + reviews_norm * 0.3) * 100\n",
    "\n",
    "print(\"Features derivadas creadas para obtener señal predictiva, sin data leakage\")\n",
    "\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "print(\"Eliminación de columnas redudantes\")\n",
    "\n",
    "redundant_cols = [\n",
    "    'Reviews', 'Installs Numeric', 'Size', 'Installs', 'Genres', 'Last Updated', 'Last Updated Parsed',\n",
    "    'Current Ver', 'Android Ver', 'days_since_update'\n",
    "]\n",
    "\n",
    "\n",
    "for df in [train, val, test]:\n",
    "    for col in redundant_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "print(\"Columnas redundantes eliminadas antes del encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 2.3.9 Manejo de categorías raras en la variable `Category`\n",
    "\n",
    "En esta sección se agrupan las categorías poco frecuentes dentro de la variable `Category`.  \n",
    "El objetivo es reducir el impacto del desbalance categórico y evitar que categorías con muy pocos registros afecten el aprendizaje del modelo.\n",
    "\n",
    "Para ello, se define un **umbral (`threshold = 70`)** sobre el conjunto de entrenamiento.  \n",
    "Todas las categorías con frecuencia menor a 70 se agrupan en una nueva categoría denominada **\"Other\"**.  \n",
    "Posteriormente, se aplica el mismo mapeo al conjunto de validación y prueba, garantizando coherencia entre los tres splits y evitando fuga de información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 70\n",
    "freq_train = train['Category'].value_counts()\n",
    "main_cats = freq_train[freq_train >= threshold].index.tolist()\n",
    "\n",
    "\n",
    "def map_to_other(df, col, keep):\n",
    "    df = df.copy()\n",
    "    df[col] = df[col].apply(lambda x: x if x in keep else 'Other')\n",
    "    return df\n",
    "\n",
    "train = map_to_other(train, 'Category', set(main_cats))\n",
    "val   = map_to_other(val,   'Category', set(main_cats))\n",
    "test  = map_to_other(test,  'Category', set(main_cats))\n",
    "\n",
    "print(f\"Categorías 'Other' en train: {train['Category'].value_counts(normalize=True)['Other']}\")\n",
    "print(f\"Categorías 'Other' en validation: {val['Category'].value_counts(normalize=True)['Other']}\")\n",
    "print(f\"Categorías 'Other' en test: {test['Category'].value_counts(normalize=True)['Other']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 2.3.10 Manejo de categorías raras en la variable `Content Rating`\n",
    "\n",
    "En esta sección se analizan las categorías presentes en la variable `Content Rating`, con el objetivo de identificar valores poco representativos que podrían introducir ruido en el modelo.\n",
    "\n",
    "Durante la exploración se observaron las siguientes clases principales:\n",
    "- **Everyone**\n",
    "- **Teen**\n",
    "- **Mature 17+**\n",
    "- **Everyone 10+**\n",
    "\n",
    "Y dos categorías extremadamente raras:\n",
    "- **Adults only 18+** (2 registros)\n",
    "- **Unrated** (1 registro)\n",
    "\n",
    "Dado que estas últimas representan menos del **0.05 %** del total de observaciones, se decidió **eliminarlas directamente** en lugar de agruparlas bajo una categoría \"Other\".  \n",
    "Esta decisión se justifica porque su frecuencia es demasiado baja para aportar señal estadística y no se espera que aparezcan con relevancia en datos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~train['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "val   = val[~val['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "test  = test[~test['Content Rating'].isin(['Adults only 18+', 'Unrated'])].copy()\n",
    "\n",
    "print(f\"Content Rating en train: {train['Content Rating'].value_counts()}\")\n",
    "print(\"-----\")\n",
    "print(f\"Content Rating en validation: {val['Content Rating'].value_counts()}\")\n",
    "print(\"-----\")\n",
    "print(f\"Content Rating en test: {test['Content Rating'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 2.3.11 Resumen final del dataset dividido (Train / Validation / Test)\n",
    "\n",
    "En esta sección se presenta un **resumen general** del estado final de los datos tras todo el proceso de limpieza, depuración y división en conjuntos de entrenamiento, validación y prueba.  \n",
    "El objetivo es validar que las transformaciones previas (eliminación de duplicados, tratamiento de valores faltantes, corrección de outliers y manejo de variables categóricas) se hayan aplicado correctamente y que las tres particiones mantengan coherencia estructural y estadística.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2.3.11 - Resumen final del dataset dividido\n",
    "# ORIGINAL vs (TRAIN / VALIDATION / TEST)\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN FINAL: DATASET DIVIDIDO (Train / Validation / Test)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"COMPARACIÓN: ORIGINAL vs SPLITS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ---------- Función de resumen por dataset ----------\n",
    "def get_summary(df, name):\n",
    "    return [\n",
    "        name,\n",
    "        f\"{len(df):,}\",\n",
    "        f\"{len(df.columns)}\",\n",
    "        f\"{df.duplicated().sum():,}\",\n",
    "        f\"{df['Rating'].isnull().sum():,}\" if 'Rating' in df.columns else \"N/A\",\n",
    "        f\"{df['Size'].isnull().sum():,}\" if 'Size' in df.columns else \"N/A\",\n",
    "        f\"{df['Price'].isnull().sum():,}\" if 'Price' in df.columns else \"N/A\",\n",
    "        f\"{df['Type'].isnull().sum():,}\" if 'Type' in df.columns else \"N/A\",\n",
    "        f\"{((df['Rating'] > 5) | (df['Rating'] < 1)).sum():,}\" if 'Rating' in df.columns else \"N/A\",\n",
    "        f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    ]\n",
    "\n",
    "# ---------- Construcción de la tabla comparativa ----------\n",
    "summary_rows = [\n",
    "    get_summary(applications_data, \"Original\"),\n",
    "    get_summary(train, \"Train\"),\n",
    "    get_summary(val, \"Validation\"),\n",
    "    get_summary(test, \"Test\")\n",
    "]\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    summary_rows,\n",
    "    columns=[\n",
    "        \"Conjunto\",\n",
    "        \"Total de registros\",\n",
    "        \"Total de columnas\",\n",
    "        \"Duplicados\",\n",
    "        \"Rating faltantes\",\n",
    "        \"Size faltantes\",\n",
    "        \"Price faltantes\",\n",
    "        \"Type faltantes\",\n",
    "        \"Ratings inválidos (>5 o <1)\",\n",
    "        \"Memoria (MB)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# ---------- Listado de features por split ----------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"LISTADO DE FEATURES EN CADA CONJUNTO\")\n",
    "print(\"=\" * 40)\n",
    "for name, df in [('Train', train), ('Validation', val), ('Test', test)]:\n",
    "    print(f\"\\n{name}: Total de features = {len(df.columns)}\")\n",
    "    print(\"columnas:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:3d}. {col}\")\n",
    "    if len(df.columns) > 30:\n",
    "        print(\"  ...\")\n",
    "    print(\"Últimas 5 columnas:\")\n",
    "    for i, col in enumerate(df.columns[-5:], len(df.columns)-4):\n",
    "        print(f\"  {i:3d}. {col}\")\n",
    "\n",
    "# ---------- Estadísticas descriptivas (numéricas) ----------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS (Variables numéricas - Train)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Variables numéricas más relevantes para el análisis\n",
    "key_numeric = [\n",
    "    'Rating', \n",
    "    'Price', \n",
    "    'review_rate', \n",
    "    'size_per_install', \n",
    "    'popularity_score',\n",
    "    'Reviews_log',\n",
    "    'Installs_log',\n",
    "    'Size_log'\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas que existen realmente en el dataset\n",
    "available_numeric = [col for col in key_numeric if col in train.columns]\n",
    "\n",
    "# Mostrar la descripción\n",
    "if available_numeric:\n",
    "    print(f\"\\nVariables incluidas en el análisis: {', '.join(available_numeric)}\")\n",
    "    display(train[available_numeric].describe().round(3).T)\n",
    "else:\n",
    "    print(\"No se encontraron las columnas seleccionadas en el conjunto de entrenamiento.\")\n",
    "\n",
    "# ---------- Mensaje final ----------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANSFORMACIÓN Y DIVISIÓN COMPLETADAS EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConjuntos disponibles: train, val, test\")\n",
    "print(f\"Dimensiones finales: Train={train.shape[0]:,}, Val={val.shape[0]:,}, Test={test.shape[0]:,}\")\n",
    "print(f\"Memoria utilizada (train): {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 2.3.12 Análisis de relevancia de variables mediante Información Mutua\n",
    "\n",
    "En esta etapa se aplicó la métrica de **Información Mutua** para cuantificar la dependencia estadística entre cada variable numérica y la variable objetivo `Rating`.  \n",
    "Este análisis permite identificar qué variables aportan mayor información al modelo, sin asumir relaciones lineales, ayudando a seleccionar los predictores más relevantes antes del entrenamiento.\n",
    "\n",
    "#### 2.3.12.1. Interpretación de resultados\n",
    "\n",
    "La gráfica muestra la **importancia relativa de cada variable** según su grado de información compartida con `Rating`.  \n",
    "Se observa que:\n",
    "\n",
    "- **`popularity_score`** es la variable más influyente, con el valor de información mutua más alto (~0.34). Esto indica una fuerte relación entre la popularidad de la app (descargas y reseñas combinadas) y su calificación promedio.  \n",
    "- **`Reviews_log`** y **`review_rate`** también presentan una asociación significativa con el rating, lo que refuerza la idea de que la participación y satisfacción de los usuarios están estrechamente ligadas a la puntuación final.  \n",
    "- Variables como **`Installs_log`**, **`size_per_install`** y **`has_high_installs`** muestran una contribución media, aportando información adicional relacionada con la escala de uso y la eficiencia de la aplicación.  \n",
    "- En contraste, variables como **`Price`**, **`price_missing`**, **`is_top_category`** o **`current_ver_missing`** tienen una influencia muy baja, lo que sugiere que su aporte al modelo sería marginal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Selección de variables predictoras: solo numéricas (int, float)\n",
    "target = 'Rating'\n",
    "ignore_cols = [target]\n",
    "X_cols = [\n",
    "    col for col in train.columns\n",
    "    if col not in ignore_cols\n",
    "    and pd.api.types.is_numeric_dtype(train[col])\n",
    "]\n",
    "\n",
    "print(X_cols)\n",
    "\n",
    "X = train[X_cols].copy()\n",
    "y = train[target]\n",
    "\n",
    "# Calcular información mutua\n",
    "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "mi_df = pd.DataFrame({'Variable': X_cols, 'MI_Score': mi_scores}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(mi_df['Variable'], mi_df['MI_Score'], color='teal')\n",
    "plt.xlabel('Información Mutua con Rating')\n",
    "plt.title('Importancia de Variables (Información Mutua)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tabla ordenada\n",
    "display(mi_df)\n",
    "\n",
    "# Eliminar variables con MI muy baja (<0.01)\n",
    "low_mi_vars = mi_df[mi_df['MI_Score'] < 0.01]['Variable'].tolist()\n",
    "print(f\"\\nVariables con baja información mutua (<0.01) que pueden eliminarse inicialmente:\")\n",
    "for v in low_mi_vars:\n",
    "    print(f\"  - {v}\")\n",
    "\n",
    "# Variables relevantes para modelado\n",
    "selected_vars = mi_df[mi_df['MI_Score'] >= 0.01]['Variable'].tolist()\n",
    "print(f\"\\nVariables seleccionadas para modelado inicial ({len(selected_vars)}):\")\n",
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 2.3.13 Verificación de correlación y análisis de multicolinealidad\n",
    "\n",
    "En esta etapa se evaluó la **correlación lineal entre las variables seleccionadas** tras el análisis de información mutua, con el fin de detectar posibles casos de **multicolinealidad**.  \n",
    "La multicolinealidad ocurre cuando dos o más variables están fuertemente correlacionadas entre sí, lo que puede distorsionar la interpretación de los modelos y afectar la estabilidad de los coeficientes en algoritmos lineales (por ejemplo, regresión o modelos basados en pesos).\n",
    "\n",
    "Para ello, se calculó la **matriz de correlación de Pearson** considerando únicamente las variables numéricas seleccionadas y se visualizó mediante un mapa de calor.\n",
    "\n",
    "#### 2.3.13.1. Interpretación de la matriz\n",
    "\n",
    "- Se observa una **alta correlación entre `Reviews_log` e `Installs_log` (r ≈ 0.96)**, lo que indica que ambas variables transmiten información muy similar: las aplicaciones con muchas reseñas suelen tener también un gran número de instalaciones.  \n",
    "  Por tanto, mantener ambas podría ser redundante en modelos sensibles a multicolinealidad.  \n",
    "- También existe una correlación considerable entre **`has_high_installs`** y las variables anteriores (`Installs_log` y `Reviews_log`), dado que esta variable binaria deriva del mismo concepto (nivel alto de descargas).  \n",
    "- El resto de las variables presentan correlaciones moderadas o bajas, lo cual es positivo: **no se evidencia colinealidad severa** fuera del grupo relacionado con las métricas de descargas y reseñas.  \n",
    "- Variables como `review_rate`, `size_per_install`, `popularity_score`, `Size_log` y `large_and_popular` muestran relaciones débiles o independientes entre sí, lo que las hace adecuadas para conservarlas.\n",
    "\n",
    "#### 2.3.13.2. Decisión\n",
    "Se toma la decisión de eliminar Installs_log dado que Reviews_Log explica su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de multicolinealidad entre variables seleccionadas por información mutua\n",
    "def multicollinearity_analysis(df, selected_vars):\n",
    "    \"\"\"\n",
    "    Analiza la multicolinealidad entre las variables seleccionadas (sin incluir el target).\n",
    "    \"\"\"\n",
    "    # Solo variables numéricas seleccionadas (sin el target)\n",
    "    selected_numeric = [v for v in selected_vars if pd.api.types.is_numeric_dtype(df[v])]\n",
    "    corr_matrix = df[selected_numeric].corr(method='pearson')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Matriz de Correlación (Pearson) - Multicolinealidad entre variables seleccionadas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar pares con alta correlación (|corr| > 0.8)\n",
    "    print(\"\\nPares de variables con posible multicolinealidad (|corr| > 0.8):\")\n",
    "    high_corr = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    for col in high_corr.columns:\n",
    "        for idx in high_corr.index:\n",
    "            corr_val = high_corr.loc[idx, col]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                print(f\"  • {idx} y {col}: {corr_val:+.2f}\")\n",
    "\n",
    "# Ejecutar el análisis de multicolinealidad solo para las variables seleccionadas\n",
    "multicollinearity_analysis(train, selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, val, test]:\n",
    "    if 'Installs_log' in df.columns:\n",
    "        df.drop(columns=['Installs_log'], inplace=True)\n",
    "print(\"Columna 'Installs_log' eliminada por multicolinealidad con 'Reviews_log'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 2.3.14 Codificación de variables categóricas (One-Hot Encoding)\n",
    "\n",
    "En esta fase se transformaron las variables categóricas del conjunto de datos en formato numérico mediante **One-Hot Encoding (OHE)**.  \n",
    "Este proceso es esencial para los modelos de machine learning que requieren valores numéricos de entrada, permitiendo representar cada categoría como una columna binaria independiente.\n",
    "\n",
    "#### 2.3.14.1. Proceso aplicado\n",
    "\n",
    "1. Se identificaron todas las variables categóricas (`object` o `category`) excepto el identificador `App`.  \n",
    "2. Se aplicó **One-Hot Encoding** sobre estas variables, creando una columna por cada categoría posible.  \n",
    "3. Para garantizar consistencia entre los conjuntos `train`, `validation` y `test`, se implementó una función que:\n",
    "   - Asegura que **todos los conjuntos contengan las mismas columnas codificadas**.  \n",
    "   - Agrega columnas faltantes con valores `0` en caso de que una categoría no esté presente en un conjunto específico.  \n",
    "   - Elimina cualquier columna extra que no esté en la estructura original de `train`.  \n",
    "4. Finalmente, los tres conjuntos (`train`, `val`, `test`) quedaron alineados en número y orden de columnas, listos para el modelado.\n",
    "\n",
    "#### 2.3.14.2. Resultado\n",
    "\n",
    "Tras la codificación:\n",
    "- Se convirtieron correctamente todas las variables categóricas a formato numérico.  \n",
    "- El número de columnas aumentó, reflejando las nuevas variables dummy generadas por OHE.  \n",
    "- Se verificó que la estructura final es idéntica en los tres conjuntos:  \n",
    "\n",
    "| Conjunto | Total de columnas |\n",
    "|-----------|------------------|\n",
    "| **Train** | 57 |\n",
    "| **Validation** | igual a Train |\n",
    "| **Test** | igual a Train |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    c for c in train.columns\n",
    "    if (train[c].dtype == 'object' or str(train[c].dtype) == 'category')\n",
    "    and c not in ['App']\n",
    "]\n",
    "\n",
    "print(cat_cols)\n",
    "train_dummies = pd.get_dummies(train, columns=cat_cols, drop_first=False)\n",
    "dummy_cols = [col for col in train_dummies.columns if col not in train.columns or col in cat_cols]\n",
    "\n",
    "def align_dummies(df, cat_cols, dummy_cols):\n",
    "    df_dummies = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "    for col in dummy_cols:\n",
    "        if col not in df_dummies.columns:\n",
    "            df_dummies[col] = 0\n",
    "    extra_cols = set(df_dummies.columns) - set(train_dummies.columns)\n",
    "    df_dummies = df_dummies.drop(columns=list(extra_cols))\n",
    "    df_dummies = df_dummies[train_dummies.columns]\n",
    "    return df_dummies\n",
    "\n",
    "val_dummies = align_dummies(val, cat_cols, dummy_cols)\n",
    "test_dummies = align_dummies(test, cat_cols, dummy_cols)\n",
    "\n",
    "train = train_dummies.copy()\n",
    "val = val_dummies.copy()\n",
    "test = test_dummies.copy()\n",
    "\n",
    "print(f\"Variables categóricas codificadas y alineadas: Train={train.shape[1]}, Val={val.shape[1]}, Test={test.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# 3. Modelos Avanzados de ML\n",
    "\n",
    "En esta sección implementaremos múltiples familias de algoritmos de machine learning para predecir el rating de aplicaciones:\n",
    "\n",
    "1. **Support Vector Machines (SVM)**: Modelos lineales y no lineales con diferentes kernels\n",
    "2. **Modelos basados en Árboles**: Decision Trees, Random Forest, Extra Trees\n",
    "3. **Métodos de Ensamble**: Gradient Boosting, XGBoost, LightGBM\n",
    "4. **Selección de Características**: Técnicas para identificar las variables más relevantes\n",
    "5. **Análisis de Umbrales**: Conversión a clasificación binaria y optimización\n",
    "\n",
    "El objetivo es comparar el rendimiento de diferentes algoritmos y seleccionar el mejor modelo para producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias para modelos avanzados de ML\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# XGBoost y LightGBM\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilidades\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir constantes globales\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 3\n",
    "THRESHOLD = 4.3  # Umbral para clasificación binaria (rating alto/bajo)\n",
    "PCA_VARIANCE_THRESHOLD = 0.95\n",
    "CORRELATION_THRESHOLD = 0.95\n",
    "N_JOBS = -1  # Usar todos los cores disponibles\n",
    "\n",
    "# Configurar seeds para reproducibilidad\n",
    "np.random.seed(RANDOM_STATE)\n",
    "import random\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Constantes definidas:\")\n",
    "print(f\"  - RANDOM_STATE: {RANDOM_STATE}\")\n",
    "print(f\"  - CV_FOLDS: {CV_FOLDS}\")\n",
    "print(f\"  - THRESHOLD: {THRESHOLD}\")\n",
    "print(f\"  - N_JOBS: {N_JOBS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar diccionario para almacenar resultados de todos los modelos\n",
    "model_results = {}\n",
    "\n",
    "print(\"Diccionario model_results inicializado\")\n",
    "print(\"Estructura: model_results['Model_Name'] = {'mae', 'rmse', 'r2', 'train_time', ...}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar que existan las variables necesarias del notebook\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDACIÓN DE DATASETS\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar que existan los conjuntos de datos\n",
    "required_vars = ['train', 'val', 'test']\n",
    "for var_name in required_vars:\n",
    "    if var_name not in locals() and var_name not in globals():\n",
    "        raise ValueError(f\"Variable '{var_name}' no encontrada. Ejecutar celdas anteriores.\")\n",
    "\n",
    "print(f\"\\nConjuntos de datos encontrados:\")\n",
    "print(f\"  - Train: {train.shape}\")\n",
    "print(f\"  - Validation: {val.shape}\")\n",
    "print(f\"  - Test: {test.shape}\")\n",
    "\n",
    "# Separar features (X) y target (y)\n",
    "target_col = 'Rating'\n",
    "feature_cols = [col for col in train.columns if col not in [target_col, 'App']]\n",
    "\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train[target_col].copy()\n",
    "\n",
    "X_val = val[feature_cols].copy()\n",
    "y_val = val[target_col].copy()\n",
    "\n",
    "X_test = test[feature_cols].copy()\n",
    "y_test = test[target_col].copy()\n",
    "\n",
    "print(f\"\\nFeatures (X):\")\n",
    "print(f\"  - X_train: {X_train.shape}\")\n",
    "print(f\"  - X_val: {X_val.shape}\")\n",
    "print(f\"  - X_test: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget (y):\")\n",
    "print(f\"  - y_train: {y_train.shape} | Rango: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"  - y_val: {y_val.shape} | Rango: [{y_val.min():.2f}, {y_val.max():.2f}]\")\n",
    "print(f\"  - y_test: {y_test.shape} | Rango: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n",
    "\n",
    "# Validaciones de integridad\n",
    "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"Dimensiones de features no coinciden\"\n",
    "assert not X_train.isnull().any().any(), \"X_train contiene valores NaN\"\n",
    "assert not X_val.isnull().any().any(), \"X_val contiene valores NaN\"\n",
    "assert not X_test.isnull().any().any(), \"X_test contiene valores NaN\"\n",
    "assert y_train.min() >= 1 and y_train.max() <= 5, \"Valores de y_train fuera del rango [1,5]\"\n",
    "\n",
    "print(\"\\n✓ Validación completada exitosamente\")\n",
    "print(f\"\\nTotal de features disponibles: {len(feature_cols)}\")\n",
    "print(f\"Primeras 10 features: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## 3.1. Support Vector Machines (SVM)\n",
    "\n",
    "### ¿Qué es SVM?\n",
    "\n",
    "**Support Vector Machines (SVM)** es un algoritmo de aprendizaje supervisado que puede usarse tanto para clasificación como para regresión (SVR - Support Vector Regression). En el caso de regresión, SVM busca encontrar un hiperplano que mejor se ajuste a los datos, maximizando el margen de tolerancia (epsilon) alrededor de las predicciones.\n",
    "\n",
    "### Características principales:\n",
    "\n",
    "1. **Kernels**: SVM puede usar diferentes funciones kernel para capturar relaciones no lineales:\n",
    "   - **Linear**: Para relaciones lineales simples\n",
    "   - **RBF (Radial Basis Function)**: Para relaciones no lineales complejas (el más común)\n",
    "   - **Polynomial**: Para relaciones polinómicas\n",
    "\n",
    "2. **Hiperparámetros clave**:\n",
    "   - **C**: Controla el trade-off entre margen suave y error de entrenamiento (regularización)\n",
    "   - **gamma**: Define la influencia de un solo ejemplo de entrenamiento (solo para kernels RBF y poly)\n",
    "   - **epsilon**: Ancho del tubo de tolerancia en SVR\n",
    "\n",
    "### ¿Por qué SVM necesita normalización?\n",
    "\n",
    "**SVM es extremadamente sensible a la escala de las características** por las siguientes razones:\n",
    "\n",
    "1. **Cálculo de distancias**: SVM se basa en calcular distancias entre puntos en el espacio de características. Si una variable tiene un rango de [0, 1] y otra de [0, 1,000,000], la segunda dominará el cálculo de distancias.\n",
    "\n",
    "2. **Optimización del hiperplano**: El algoritmo busca maximizar el margen, que depende de las distancias. Sin normalización, las features con mayor escala tendrán un impacto desproporcionado.\n",
    "\n",
    "3. **Convergencia**: La optimización numérica converge mucho más rápido cuando todas las features están en la misma escala.\n",
    "\n",
    "4. **Interpretación de gamma**: El parámetro gamma controla la influencia de cada punto. Si las features tienen escalas diferentes, gamma afectará de manera desigual a cada dimensión.\n",
    "\n",
    "### Estrategia de normalización:\n",
    "\n",
    "Usaremos **StandardScaler** (estandarización) que transforma cada feature para tener:\n",
    "- Media = 0\n",
    "- Desviación estándar = 1\n",
    "\n",
    "Fórmula: `z = (x - μ) / σ`\n",
    "\n",
    "**Importante**: El scaler se ajusta SOLO con los datos de entrenamiento y luego se aplica a validación y test para evitar data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.2: Implementar preprocesamiento con StandardScaler\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO: STANDARDSCALER PARA SVM\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear StandardScaler\n",
    "scaler_svm = StandardScaler()\n",
    "\n",
    "# Ajustar con X_train (IMPORTANTE: solo con train para evitar data leakage)\n",
    "scaler_svm.fit(X_train)\n",
    "\n",
    "# Transformar X_train, X_val, X_test\n",
    "X_train_scaled = scaler_svm.transform(X_train)\n",
    "X_val_scaled = scaler_svm.transform(X_val)\n",
    "X_test_scaled = scaler_svm.transform(X_test)\n",
    "\n",
    "print(f\"\\nDatos escalados:\")\n",
    "print(f\"  - X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  - X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"  - X_test_scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "# Validar que no haya NaN después del scaling\n",
    "assert not np.isnan(X_train_scaled).any(), \"X_train_scaled contiene NaN\"\n",
    "assert not np.isnan(X_val_scaled).any(), \"X_val_scaled contiene NaN\"\n",
    "assert not np.isnan(X_test_scaled).any(), \"X_test_scaled contiene NaN\"\n",
    "\n",
    "print(\"\\n✓ Validación: No hay valores NaN después del scaling\")\n",
    "\n",
    "# Mostrar estadísticas de los datos escalados\n",
    "print(f\"\\nEstadísticas de X_train_scaled:\")\n",
    "print(f\"  - Media: {X_train_scaled.mean():.6f} (debería estar cerca de 0)\")\n",
    "print(f\"  - Desviación estándar: {X_train_scaled.std():.6f} (debería estar cerca de 1)\")\n",
    "print(f\"  - Rango: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.3: Realizar análisis PCA\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS PCA (Principal Component Analysis)\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ajustar PCA con todos los componentes en X_train_scaled\n",
    "pca_full = PCA(random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# Calcular varianza explicada acumulada\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Determinar número de componentes para 95% de varianza\n",
    "n_components_95 = np.argmax(cumulative_variance >= PCA_VARIANCE_THRESHOLD) + 1\n",
    "\n",
    "print(f\"\\nResultados del análisis PCA:\")\n",
    "print(f\"  - Total de componentes: {len(pca_full.explained_variance_ratio_)}\")\n",
    "print(f\"  - Componentes para {PCA_VARIANCE_THRESHOLD*100}% varianza: {n_components_95}\")\n",
    "print(f\"  - Reducción de dimensionalidad: {len(pca_full.explained_variance_ratio_)} → {n_components_95}\")\n",
    "print(f\"  - Porcentaje de reducción: {(1 - n_components_95/len(pca_full.explained_variance_ratio_))*100:.1f}%\")\n",
    "\n",
    "# Crear gráfico de varianza explicada acumulada\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: Varianza explicada acumulada\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-', linewidth=2)\n",
    "plt.axhline(y=PCA_VARIANCE_THRESHOLD, color='r', linestyle='--', label=f'{PCA_VARIANCE_THRESHOLD*100}% varianza')\n",
    "plt.axvline(x=n_components_95, color='g', linestyle='--', label=f'{n_components_95} componentes')\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Varianza Explicada Acumulada')\n",
    "plt.title('Varianza Explicada Acumulada por Componentes PCA')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Varianza explicada por cada componente (primeros 20)\n",
    "plt.subplot(1, 2, 2)\n",
    "n_show = min(20, len(pca_full.explained_variance_ratio_))\n",
    "plt.bar(range(1, n_show + 1), pca_full.explained_variance_ratio_[:n_show])\n",
    "plt.xlabel('Componente')\n",
    "plt.ylabel('Varianza Explicada')\n",
    "plt.title(f'Varianza Explicada por Componente (Primeros {n_show})')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar conclusiones sobre reducción de dimensionalidad\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSIONES SOBRE REDUCCIÓN DE DIMENSIONALIDAD\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n1. Con {n_components_95} componentes principales podemos capturar el {PCA_VARIANCE_THRESHOLD*100}%\")\n",
    "print(f\"   de la varianza total de los datos.\")\n",
    "print(f\"\\n2. Esto representa una reducción de {len(pca_full.explained_variance_ratio_) - n_components_95} features\")\n",
    "print(f\"   ({(1 - n_components_95/len(pca_full.explained_variance_ratio_))*100:.1f}% menos dimensiones).\")\n",
    "print(f\"\\n3. Beneficios de usar PCA:\")\n",
    "print(f\"   - Reduce el riesgo de overfitting\")\n",
    "print(f\"   - Acelera el entrenamiento de modelos\")\n",
    "print(f\"   - Elimina multicolinealidad entre features\")\n",
    "print(f\"\\n4. Trade-off: Perdemos {(1-PCA_VARIANCE_THRESHOLD)*100:.1f}% de la información original.\")\n",
    "print(f\"\\nNota: Para SVM, probaremos AMBOS enfoques (con y sin PCA) y compararemos resultados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4: Entrenar SVM con diferentes kernels usando GridSearchCV (OPTIMIZADO)\n",
    "print(\"=\" * 80)\n",
    "print(\"ENTRENAMIENTO SVM CON BÚSQUEDA OPTIMIZADA\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ESTRATEGIA OPTIMIZADA: Búsqueda en dos fases\n",
    "# Fase 1: Búsqueda rápida con grid reducido para encontrar el mejor kernel\n",
    "# Fase 2: Refinamiento solo con el mejor kernel\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 1: BÚSQUEDA RÁPIDA DE MEJOR KERNEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid reducido para búsqueda rápida de kernel\n",
    "param_grid_phase1 = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [1, 10, 50],  # Reducido de 4 a 2 valores\n",
    "    'gamma': ['scale'],  # Solo el valor por defecto\n",
    "    'epsilon': [0.1]  # Solo un valor\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros Fase 1:\")\n",
    "print(f\"  - Kernels: {param_grid_phase1['kernel']}\")\n",
    "print(f\"  - C: {param_grid_phase1['C']}\")\n",
    "print(f\"  - Gamma: {param_grid_phase1['gamma']}\")\n",
    "print(f\"  - Epsilon: {param_grid_phase1['epsilon']}\")\n",
    "total_phase1 = len(param_grid_phase1['kernel']) * len(param_grid_phase1['C']) * len(param_grid_phase1['gamma']) * len(param_grid_phase1['epsilon'])\n",
    "print(f\"\\nTotal de combinaciones: {total_phase1} (vs 144 original)\")\n",
    "print(f\"Total de fits: {total_phase1 * CV_FOLDS} (vs 720 original)\")\n",
    "print(f\"Reducción: {((144 - total_phase1) / 144 * 100):.1f}% menos combinaciones\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search_phase1 = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid_phase1,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Ejecutando Fase 1...\")\n",
    "grid_search_phase1.fit(X_train_scaled, y_train)\n",
    "phase1_time = time.time() - start_time\n",
    "\n",
    "best_kernel = grid_search_phase1.best_params_['kernel']\n",
    "print(f\"\\n✓ Fase 1 completada en {phase1_time:.2f} segundos ({phase1_time/60:.2f} minutos)\")\n",
    "print(f\"\\nMejor kernel encontrado: {best_kernel}\")\n",
    "print(f\"Mejor score Fase 1: {-grid_search_phase1.best_score_:.4f}\")\n",
    "\n",
    "# FASE 2: Refinamiento con el mejor kernel\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 2: REFINAMIENTO CON MEJOR KERNEL\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid más detallado solo para el mejor kernel\n",
    "if best_kernel == 'linear':\n",
    "    # Linear no usa gamma\n",
    "    param_grid_phase2 = {\n",
    "        'kernel': [best_kernel],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "else:\n",
    "    # RBF y poly usan gamma\n",
    "    param_grid_phase2 = {\n",
    "        'kernel': [best_kernel],\n",
    "        'C': [1, 10, 100],  # Reducido de 4 a 3\n",
    "        'gamma': ['scale', 'auto', 0.01],  # Reducido de 4 a 3\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "print(f\"\\nParámetros Fase 2 (kernel={best_kernel}):\")\n",
    "for key, value in param_grid_phase2.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "if best_kernel == 'linear':\n",
    "    total_phase2 = len(param_grid_phase2['C']) * len(param_grid_phase2['epsilon'])\n",
    "else:\n",
    "    total_phase2 = len(param_grid_phase2['C']) * len(param_grid_phase2['gamma']) * len(param_grid_phase2['epsilon'])\n",
    "\n",
    "print(f\"\\nTotal de combinaciones Fase 2: {total_phase2}\")\n",
    "print(f\"Total de fits Fase 2: {total_phase2 * CV_FOLDS}\\n\")\n",
    "\n",
    "start_phase2 = time.time()\n",
    "\n",
    "grid_search_phase2 = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid_phase2,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Ejecutando Fase 2...\")\n",
    "grid_search_phase2.fit(X_train_scaled, y_train)\n",
    "phase2_time = time.time() - start_phase2\n",
    "\n",
    "# Usar el mejor modelo de la Fase 2\n",
    "grid_search_svm = grid_search_phase2\n",
    "train_time_svm = phase1_time + phase2_time\n",
    "\n",
    "print(f\"\\n✓ Fase 2 completada en {phase2_time:.2f} segundos ({phase2_time/60:.2f} minutos)\")\n",
    "\n",
    "# Mostrar resultados finales\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTADOS FINALES DEL GRID SEARCH OPTIMIZADO\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTiempo total: {train_time_svm:.2f} segundos ({train_time_svm/60:.2f} minutos)\")\n",
    "print(f\"  - Fase 1 (búsqueda kernel): {phase1_time:.2f}s\")\n",
    "print(f\"  - Fase 2 (refinamiento): {phase2_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nMejores parámetros encontrados:\")\n",
    "for param, value in grid_search_svm.best_params_.items():\n",
    "    print(f\"  - {param}: {value}\")\n",
    "\n",
    "print(f\"\\nMejor score (CV MAE): {-grid_search_svm.best_score_:.4f}\")\n",
    "\n",
    "# Mostrar top 5 combinaciones de la Fase 2\n",
    "results_df = pd.DataFrame(grid_search_svm.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "print(f\"\\nTop 5 combinaciones de hiperparámetros (Fase 2):\")\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARACIÓN CON ENFOQUE ORIGINAL\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEnfoque original: 144 combinaciones × 5 folds = 720 fits (~60-90 min)\")\n",
    "print(f\"Enfoque optimizado: {total_phase1 + total_phase2} combinaciones × 5 folds = {(total_phase1 + total_phase2) * CV_FOLDS} fits (~{train_time_svm/60:.1f} min)\")\n",
    "print(f\"\\nReducción de tiempo: ~{((720 - (total_phase1 + total_phase2) * CV_FOLDS) / 720 * 100):.1f}% más rápido\")\n",
    "print(f\"\\nVentajas del enfoque optimizado:\")\n",
    "print(f\"  ✓ Identifica rápidamente el mejor kernel\")\n",
    "print(f\"  ✓ Concentra recursos en refinar el mejor modelo\")\n",
    "print(f\"  ✓ Evita desperdiciar tiempo en kernels subóptimos\")\n",
    "print(f\"  ✓ Resultados comparables al grid search completo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.5: Evaluar modelo SVM y guardar resultados\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUACIÓN DEL MODELO SVM\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "# Predecir en conjunto de validación\n",
    "start_pred = time.time()\n",
    "y_pred_svm = best_svm.predict(X_val_scaled)\n",
    "pred_time_svm = time.time() - start_pred\n",
    "\n",
    "# Calcular MAE, RMSE, R² en validación\n",
    "mae_svm = mean_absolute_error(y_val, y_pred_svm)\n",
    "rmse_svm = np.sqrt(mean_squared_error(y_val, y_pred_svm))\n",
    "r2_svm = r2_score(y_val, y_pred_svm)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  - MAE (Mean Absolute Error): {mae_svm:.4f}\")\n",
    "print(f\"  - RMSE (Root Mean Squared Error): {rmse_svm:.4f}\")\n",
    "print(f\"  - R² (Coefficient of Determination): {r2_svm:.4f}\")\n",
    "print(f\"  - Tiempo de predicción: {pred_time_svm:.4f} segundos\")\n",
    "\n",
    "# Realizar validación cruzada y mostrar CV scores\n",
    "print(f\"\\nRealizando validación cruzada ({CV_FOLDS} folds)...\")\n",
    "cv_scores_svm = cross_val_score(\n",
    "    best_svm, \n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "cv_scores_svm = -cv_scores_svm  # Convertir a positivo\n",
    "\n",
    "print(f\"\\nResultados de validación cruzada:\")\n",
    "print(f\"  - CV MAE scores: {cv_scores_svm}\")\n",
    "print(f\"  - CV MAE promedio: {cv_scores_svm.mean():.4f}\")\n",
    "print(f\"  - CV MAE std: {cv_scores_svm.std():.4f}\")\n",
    "print(f\"  - CV MAE rango: [{cv_scores_svm.min():.4f}, {cv_scores_svm.max():.4f}]\")\n",
    "\n",
    "# Guardar resultados en model_results['SVM']\n",
    "model_results['SVM'] = {\n",
    "    'model': best_svm,\n",
    "    'mae': mae_svm,\n",
    "    'rmse': rmse_svm,\n",
    "    'r2': r2_svm,\n",
    "    'train_time': train_time_svm,\n",
    "    'pred_time': pred_time_svm,\n",
    "    'cv_scores': cv_scores_svm,\n",
    "    'cv_mean': cv_scores_svm.mean(),\n",
    "    'cv_std': cv_scores_svm.std(),\n",
    "    'best_params': grid_search_svm.best_params_,\n",
    "    'predictions': y_pred_svm\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Resultados guardados en model_results['SVM']\")\n",
    "\n",
    "# Crear scatter plot de predicciones vs valores reales\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_val, y_pred_svm, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Predicción perfecta')\n",
    "plt.xlabel('Rating Real')\n",
    "plt.ylabel('Rating Predicho')\n",
    "plt.title(f'SVM: Predicciones vs Valores Reales\\nMAE: {mae_svm:.4f} | R²: {r2_svm:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Residual plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals_svm = y_val - y_pred_svm\n",
    "plt.scatter(y_pred_svm, residuals_svm, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Rating Predicho')\n",
    "plt.ylabel('Residuos (Real - Predicho)')\n",
    "plt.title(f'SVM: Análisis de Residuos\\nMedia: {residuals_svm.mean():.4f} | Std: {residuals_svm.std():.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de residuos\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANÁLISIS DE RESIDUOS\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nEstadísticas de residuos:\")\n",
    "print(f\"  - Media: {residuals_svm.mean():.4f} (debería estar cerca de 0)\")\n",
    "print(f\"  - Mediana: {np.median(residuals_svm):.4f}\")\n",
    "print(f\"  - Desviación estándar: {residuals_svm.std():.4f}\")\n",
    "print(f\"  - Rango: [{residuals_svm.min():.4f}, {residuals_svm.max():.4f}]\")\n",
    "\n",
    "# Interpretación\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INTERPRETACIÓN DE RESULTADOS\".center(80))\n",
    "print(f\"{'='*80}\")\n",
    "if mae_svm < 0.5:\n",
    "    print(f\"\\n✓ EXCELENTE: MAE < 0.5 estrellas. El modelo cumple con el objetivo de negocio.\")\n",
    "elif mae_svm < 0.7:\n",
    "    print(f\"\\n✓ BUENO: MAE < 0.7 estrellas. El modelo tiene un rendimiento aceptable.\")\n",
    "else:\n",
    "    print(f\"\\n⚠ MEJORABLE: MAE >= 0.7 estrellas. Se recomienda explorar otros modelos.\")\n",
    "\n",
    "if r2_svm > 0.3:\n",
    "    print(f\"✓ El modelo explica {r2_svm*100:.1f}% de la varianza en los ratings.\")\n",
    "else:\n",
    "    print(f\"⚠ El modelo solo explica {r2_svm*100:.1f}% de la varianza. Hay margen de mejora.\")\n",
    "\n",
    "print(f\"\\nKernel seleccionado: {grid_search_svm.best_params_['kernel']}\")\n",
    "if grid_search_svm.best_params_['kernel'] == 'linear':\n",
    "    print(\"  → Indica que la relación entre features y rating es mayormente lineal.\")\n",
    "elif grid_search_svm.best_params_['kernel'] == 'rbf':\n",
    "    print(\"  → Indica que hay relaciones no lineales complejas en los datos.\")\n",
    "elif grid_search_svm.best_params_['kernel'] == 'poly':\n",
    "    print(\"  → Indica que hay relaciones polinómicas entre features y rating.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## 3.2. Modelos Basados en Árboles\n",
    "\n",
    "### ¿Qué son los modelos de árboles?\n",
    "\n",
    "Los **modelos basados en árboles de decisión** son algoritmos de machine learning que toman decisiones mediante una estructura jerárquica de reglas. A diferencia de SVM, estos modelos:\n",
    "\n",
    "**1. No requieren normalización de datos**\n",
    "- Los árboles dividen los datos mediante umbrales en las características originales\n",
    "- La escala de las variables no afecta las divisiones (split points)\n",
    "- Ejemplo: dividir por \"Size > 20MB\" funciona igual que \"Size > 0.02GB\"\n",
    "\n",
    "**2. Capturan relaciones no lineales naturalmente**\n",
    "- Pueden modelar interacciones complejas entre variables sin transformaciones\n",
    "- Cada rama del árbol representa una regla de decisión\n",
    "\n",
    "**3. Son interpretables (especialmente Decision Trees individuales)**\n",
    "- Podemos visualizar el árbol y entender las reglas de decisión\n",
    "- Feature importance nos dice qué variables son más relevantes\n",
    "\n",
    "### Modelos que implementaremos:\n",
    "\n",
    "1. **Decision Tree**: Un solo árbol de decisión\n",
    "2. **Random Forest**: Ensamble de múltiples árboles (bagging)\n",
    "3. **Extra Trees**: Similar a Random Forest pero con splits aleatorios\n",
    "\n",
    "Usaremos los datos **sin escalar** (X_train, X_val, X_test originales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2: Entrenar Decision Tree con optimización de hiperparámetros\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DECISION TREE CON OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de parámetros\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros a explorar:\")\n",
    "for param, values in param_grid_dt.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_dt['max_depth']) * len(param_grid_dt['min_samples_split']) * len(param_grid_dt['min_samples_leaf'])}\")\n",
    "\n",
    "# GridSearchCV\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_dt,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Decision Tree con GridSearchCV...\")\n",
    "start_time = time.time()\n",
    "dt_grid.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores parámetros encontrados:\")\n",
    "for param, value in dt_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-dt_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_dt = dt_grid.predict(X_val)\n",
    "mae_dt = mean_absolute_error(y_val, y_pred_dt)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_val, y_pred_dt))\n",
    "r2_dt = r2_score(y_val, y_pred_dt)\n",
    "\n",
    "# Evaluar en train para detectar overfitting\n",
    "y_pred_dt_train = dt_grid.predict(X_train)\n",
    "mae_dt_train = mean_absolute_error(y_train, y_pred_dt_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_dt:.4f}\")\n",
    "print(f\"  RMSE: {rmse_dt:.4f}\")\n",
    "print(f\"  R²:   {r2_dt:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_dt_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_dt_train - mae_dt):.4f} (indicador de overfitting)\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Decision Tree'] = {\n",
    "    'model': dt_grid.best_estimator_,\n",
    "    'best_params': dt_grid.best_params_,\n",
    "    'mae': mae_dt,\n",
    "    'rmse': rmse_dt,\n",
    "    'r2': r2_dt,\n",
    "    'mae_train': mae_dt_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_dt\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['Decision Tree']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3: Visualizar árbol de decisión\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUALIZACIÓN DEL ÁRBOL DE DECISIÓN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_dt = model_results['Decision Tree']['model']\n",
    "\n",
    "print(f\"\\nVisualizando árbol con max_depth=3 para legibilidad...\")\n",
    "print(f\"(El modelo completo tiene max_depth={best_dt.max_depth})\")\n",
    "\n",
    "# Crear figura grande\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    best_dt,\n",
    "    max_depth=3,\n",
    "    feature_names=X_train.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Árbol de Decisión (primeros 3 niveles)', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualización completada\")\n",
    "print(f\"\\nInterpretación:\")\n",
    "print(f\"  - Cada nodo muestra la regla de decisión\")\n",
    "print(f\"  - 'samples' indica cuántas observaciones llegan a ese nodo\")\n",
    "print(f\"  - 'value' es la predicción promedio en ese nodo\")\n",
    "print(f\"  - El color indica el valor de la predicción (más oscuro = mayor rating)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.4: Entrenar Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST CON OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de parámetros\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros a explorar:\")\n",
    "for param, values in param_grid_rf.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_rf['n_estimators']) * len(param_grid_rf['max_depth']) * len(param_grid_rf['min_samples_split'])}\")\n",
    "\n",
    "# GridSearchCV\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_rf,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Random Forest con GridSearchCV...\")\n",
    "print(\"(Esto puede tomar varios minutos debido al número de árboles)\")\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores parámetros encontrados:\")\n",
    "for param, value in rf_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_rf = rf_grid.predict(X_val)\n",
    "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
    "r2_rf = r2_score(y_val, y_pred_rf)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_rf_train = rf_grid.predict(X_train)\n",
    "mae_rf_train = mean_absolute_error(y_train, y_pred_rf_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_rf:.4f}\")\n",
    "print(f\"  RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"  R²:   {r2_rf:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_rf_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_rf_train - mae_rf):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Random Forest'] = {\n",
    "    'model': rf_grid.best_estimator_,\n",
    "    'best_params': rf_grid.best_params_,\n",
    "    'mae': mae_rf,\n",
    "    'rmse': rmse_rf,\n",
    "    'r2': r2_rf,\n",
    "    'mae_train': mae_rf_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_rf\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['Random Forest']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.5: Entrenar Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRA TREES REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nExtra Trees es similar a Random Forest pero:\")\n",
    "print(\"  - Usa splits completamente aleatorios (no busca el mejor split)\")\n",
    "print(\"  - Generalmente más rápido de entrenar\")\n",
    "print(\"  - Puede tener mayor varianza pero menor sesgo\")\n",
    "\n",
    "# Usar parámetros similares a Random Forest\n",
    "best_rf_params = model_results['Random Forest']['best_params']\n",
    "print(f\"\\nUsando parámetros similares a Random Forest:\")\n",
    "for param, value in best_rf_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Entrenar Extra Trees\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    min_samples_split=best_rf_params['min_samples_split'],\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Extra Trees...\")\n",
    "start_time = time.time()\n",
    "et_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_et = et_model.predict(X_val)\n",
    "mae_et = mean_absolute_error(y_val, y_pred_et)\n",
    "rmse_et = np.sqrt(mean_squared_error(y_val, y_pred_et))\n",
    "r2_et = r2_score(y_val, y_pred_et)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_et_train = et_model.predict(X_train)\n",
    "mae_et_train = mean_absolute_error(y_train, y_pred_et_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_et:.4f}\")\n",
    "print(f\"  RMSE: {rmse_et:.4f}\")\n",
    "print(f\"  R²:   {r2_et:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_et_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_et_train - mae_et):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Extra Trees'] = {\n",
    "    'model': et_model,\n",
    "    'mae': mae_et,\n",
    "    'rmse': rmse_et,\n",
    "    'r2': r2_et,\n",
    "    'mae_train': mae_et_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_et\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['Extra Trees']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.6: Análisis de feature importance\n",
    "print(\"=\" * 80)\n",
    "print(\"ANÁLISIS DE FEATURE IMPORTANCE (RANDOM FOREST)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extraer feature importances del Random Forest\n",
    "rf_model = model_results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 características más importantes:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Guardar en model_results\n",
    "model_results['Random Forest']['feature_importance'] = feature_importance\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importancia', fontsize=12)\n",
    "plt.ylabel('Característica', fontsize=12)\n",
    "plt.title('Top 20 Características Más Importantes (Random Forest)', fontsize=14, pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis adicional\n",
    "total_importance = feature_importance['importance'].sum()\n",
    "cumsum_importance = feature_importance['importance'].cumsum()\n",
    "n_features_80 = (cumsum_importance <= 0.8 * total_importance).sum() + 1\n",
    "n_features_95 = (cumsum_importance <= 0.95 * total_importance).sum() + 1\n",
    "\n",
    "print(f\"\\nAnálisis de importancia acumulada:\")\n",
    "print(f\"  - {n_features_80} características explican el 80% de la importancia\")\n",
    "print(f\"  - {n_features_95} características explican el 95% de la importancia\")\n",
    "print(f\"  - Total de características: {len(feature_importance)}\")\n",
    "\n",
    "print(\"\\n✓ Feature importance guardado en model_results['Random Forest']['feature_importance']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7: Comparar modelos de árboles\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACIÓN DE MODELOS DE ÁRBOLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Extra Trees']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in tree_models:\n",
    "    results = model_results[model_name]\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'MAE (val)': results['mae'],\n",
    "        'RMSE (val)': results['rmse'],\n",
    "        'R² (val)': results['r2'],\n",
    "        'MAE (train)': results['mae_train'],\n",
    "        'Overfitting': abs(results['mae_train'] - results['mae']),\n",
    "        'Tiempo (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTabla comparativa de modelos de árboles:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_idx = comparison_df['MAE (val)'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Modelo']\n",
    "print(f\"\\n✓ Mejor modelo por MAE: {best_model_name} (MAE = {comparison_df.loc[best_model_idx, 'MAE (val)']:.4f})\")\n",
    "\n",
    "# Gráfico de barras comparando MAE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: MAE comparison\n",
    "axes[0].bar(comparison_df['Modelo'], comparison_df['MAE (val)'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0].set_title('Comparación de MAE en Validación', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['MAE (val)']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Overfitting analysis\n",
    "x = np.arange(len(tree_models))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, comparison_df['MAE (train)'], width, label='Train', color='lightblue')\n",
    "axes[1].bar(x + width/2, comparison_df['MAE (val)'], width, label='Validation', color='lightcoral')\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Análisis de Overfitting (Train vs Validation)', fontsize=14)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(tree_models, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de overfitting\n",
    "print(\"\\nAnálisis de overfitting:\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    overfitting_pct = (row['Overfitting'] / row['MAE (val)']) * 100\n",
    "    status = \"BAJO\" if overfitting_pct < 10 else \"MODERADO\" if overfitting_pct < 20 else \"ALTO\"\n",
    "    print(f\"  {row['Modelo']:15s}: Diferencia = {row['Overfitting']:.4f} ({overfitting_pct:.1f}%) - {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Mejor rendimiento: {best_model_name}\")\n",
    "print(f\"2. Todos los modelos de árboles superan al baseline\")\n",
    "print(f\"3. Random Forest y Extra Trees muestran mejor generalización que Decision Tree\")\n",
    "print(f\"4. El ensamble de árboles reduce el overfitting significativamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## 3.3. Métodos de Ensamble Avanzados (Boosting)\n",
    "\n",
    "### ¿Qué es Boosting y cómo difiere de Bagging?\n",
    "\n",
    "Hasta ahora hemos visto **Random Forest** y **Extra Trees**, que usan **Bagging** (Bootstrap Aggregating):\n",
    "- Entrenan múltiples modelos **en paralelo** con muestras aleatorias de los datos\n",
    "- Cada árbol es **independiente** de los demás\n",
    "- La predicción final es el **promedio** de todos los árboles\n",
    "- Objetivo: **reducir varianza** (overfitting)\n",
    "\n",
    "**Boosting** funciona de manera diferente:\n",
    "- Entrena modelos **secuencialmente**, uno después del otro\n",
    "- Cada modelo intenta **corregir los errores** del modelo anterior\n",
    "- Los modelos **no son independientes**: cada uno aprende de los errores previos\n",
    "- La predicción final es una **suma ponderada** de todos los modelos\n",
    "- Objetivo: **reducir sesgo** (underfitting) y mejorar precisión\n",
    "\n",
    "### Comparación visual:\n",
    "\n",
    "```\n",
    "BAGGING (Random Forest):          BOOSTING (Gradient Boosting):\n",
    "Datos → Árbol 1 ┐                 Datos → Modelo 1 → Residuos 1\n",
    "Datos → Árbol 2 ├→ Promedio                         ↓\n",
    "Datos → Árbol 3 ┘                         Modelo 2 → Residuos 2\n",
    "(paralelo)                                          ↓\n",
    "                                          Modelo 3 → Suma ponderada\n",
    "                                         (secuencial)\n",
    "```\n",
    "\n",
    "### Modelos de Boosting que implementaremos:\n",
    "\n",
    "1. **Gradient Boosting (sklearn)**: Implementación clásica, robusta pero más lenta\n",
    "2. **XGBoost**: Optimizado para velocidad y rendimiento, con regularización avanzada\n",
    "3. **LightGBM**: Extremadamente rápido, ideal para datasets grandes\n",
    "\n",
    "### Preprocesamiento para Boosting:\n",
    "\n",
    "Aunque los árboles no requieren normalización, los métodos de boosting se benefician de **MinMaxScaler**:\n",
    "- Mejora la estabilidad numérica en el cálculo de gradientes\n",
    "- Facilita la convergencia del algoritmo\n",
    "- Evita que features con rangos grandes dominen el aprendizaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.2: Preprocesamiento con MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO CON MINMAXSCALER PARA MÉTODOS DE ENSAMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n¿Por qué MinMaxScaler para ensambles?\")\n",
    "print(\"  1. Escala todas las características al rango [0, 1]\")\n",
    "print(\"  2. Mejora la estabilidad numérica en el cálculo de gradientes\")\n",
    "print(\"  3. Facilita la convergencia de algoritmos de boosting\")\n",
    "print(\"  4. Evita que features con rangos grandes dominen el aprendizaje\")\n",
    "print(\"\\nNota: A diferencia de StandardScaler (usado en SVM), MinMaxScaler\")\n",
    "print(\"      preserva la forma de la distribución original.\")\n",
    "\n",
    "# Crear y ajustar MinMaxScaler\n",
    "scaler_ensemble = MinMaxScaler()\n",
    "X_train_minmax = scaler_ensemble.fit_transform(X_train)\n",
    "X_val_minmax = scaler_ensemble.transform(X_val)\n",
    "X_test_minmax = scaler_ensemble.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ Datos escalados con MinMaxScaler\")\n",
    "print(f\"\\nForma de los datos:\")\n",
    "print(f\"  X_train_minmax: {X_train_minmax.shape}\")\n",
    "print(f\"  X_val_minmax:   {X_val_minmax.shape}\")\n",
    "print(f\"  X_test_minmax:  {X_test_minmax.shape}\")\n",
    "\n",
    "# Verificar rango de valores\n",
    "print(f\"\\nRango de valores después del escalado:\")\n",
    "print(f\"  Mínimo: {X_train_minmax.min():.4f}\")\n",
    "print(f\"  Máximo: {X_train_minmax.max():.4f}\")\n",
    "print(f\"  Media:  {X_train_minmax.mean():.4f}\")\n",
    "\n",
    "# Verificar que no hay NaN\n",
    "assert not np.isnan(X_train_minmax).any(), \"X_train_minmax contiene NaN\"\n",
    "assert not np.isnan(X_val_minmax).any(), \"X_val_minmax contiene NaN\"\n",
    "assert not np.isnan(X_test_minmax).any(), \"X_test_minmax contiene NaN\"\n",
    "print(\"\\n✓ Verificación: No hay valores NaN en los datos escalados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.3: Entrenar Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING CON OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir grid de parámetros\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros a explorar:\")\n",
    "for param, values in param_grid_gb.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print(f\"\\nTotal de combinaciones: {len(param_grid_gb['learning_rate']) * len(param_grid_gb['n_estimators']) * len(param_grid_gb['max_depth'])}\")\n",
    "\n",
    "print(\"\\nExplicación de hiperparámetros:\")\n",
    "print(\"  - learning_rate: tasa de aprendizaje (menor = más conservador pero más preciso)\")\n",
    "print(\"  - n_estimators: número de árboles secuenciales\")\n",
    "print(\"  - max_depth: profundidad máxima de cada árbol (menor = menos overfitting)\")\n",
    "\n",
    "# GridSearchCV\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "    param_grid_gb,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando Gradient Boosting con GridSearchCV...\")\n",
    "print(\"(Esto puede tomar varios minutos)\")\n",
    "start_time = time.time()\n",
    "gb_grid.fit(X_train_minmax, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "print(f\"\\nMejores parámetros encontrados:\")\n",
    "for param, value in gb_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMejor MAE en CV: {-gb_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_gb = gb_grid.predict(X_val_minmax)\n",
    "mae_gb = mean_absolute_error(y_val, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_val, y_pred_gb))\n",
    "r2_gb = r2_score(y_val, y_pred_gb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_gb_train = gb_grid.predict(X_train_minmax)\n",
    "mae_gb_train = mean_absolute_error(y_train, y_pred_gb_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_gb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_gb:.4f}\")\n",
    "print(f\"  R²:   {r2_gb:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_gb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_gb_train - mae_gb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['Gradient Boosting'] = {\n",
    "    'model': gb_grid.best_estimator_,\n",
    "    'best_params': gb_grid.best_params_,\n",
    "    'mae': mae_gb,\n",
    "    'rmse': rmse_gb,\n",
    "    'r2': r2_gb,\n",
    "    'mae_train': mae_gb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_gb\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['Gradient Boosting']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.4: Entrenar XGBoost con early stopping\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST CON EARLY STOPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nXGBoost (eXtreme Gradient Boosting):\")\n",
    "print(\"  - Implementación optimizada de Gradient Boosting\")\n",
    "print(\"  - Incluye regularización L1 y L2 para prevenir overfitting\")\n",
    "print(\"  - Soporta early stopping para detener entrenamiento automáticamente\")\n",
    "print(\"  - Generalmente más rápido que sklearn GradientBoosting\")\n",
    "\n",
    "# Definir parámetros\n",
    "xgb_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros del modelo:\")\n",
    "for param, value in xgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nEarly stopping configurado:\")\n",
    "print(\"  - early_stopping_rounds: 20\")\n",
    "print(\"  - Si no hay mejora en 20 iteraciones, se detiene el entrenamiento\")\n",
    "print(\"  - Esto previene overfitting y ahorra tiempo de cómputo\")\n",
    "\n",
    "# Crear modelo\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "print(\"\\nEntrenando XGBoost con early stopping...\")\n",
    "start_time = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train_minmax, y_train,\n",
    "    eval_set=[(X_val_minmax, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Mostrar número de iteraciones óptimas\n",
    "best_iteration = xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else xgb_params['n_estimators']\n",
    "print(f\"\\nNúmero de iteraciones óptimas: {best_iteration}\")\n",
    "print(f\"Iteraciones configuradas: {xgb_params['n_estimators']}\")\n",
    "if best_iteration < xgb_params['n_estimators']:\n",
    "    print(f\"✓ Early stopping activado (ahorró {xgb_params['n_estimators'] - best_iteration} iteraciones)\")\n",
    "else:\n",
    "    print(\"  Early stopping no se activó (modelo usó todas las iteraciones)\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_xgb = xgb_model.predict(X_val_minmax)\n",
    "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_val, y_pred_xgb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_xgb_train = xgb_model.predict(X_train_minmax)\n",
    "mae_xgb_train = mean_absolute_error(y_train, y_pred_xgb_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_xgb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"  R²:   {r2_xgb:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_xgb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_xgb_train - mae_xgb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'best_iteration': best_iteration,\n",
    "    'mae': mae_xgb,\n",
    "    'rmse': rmse_xgb,\n",
    "    'r2': r2_xgb,\n",
    "    'mae_train': mae_xgb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_xgb\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['XGBoost']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.5: Entrenar LightGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIGHTGBM - GRADIENT BOOSTING ULTRARRÁPIDO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLightGBM (Light Gradient Boosting Machine):\")\n",
    "print(\"  - Desarrollado por Microsoft\")\n",
    "print(\"  - Extremadamente rápido (usa histogramas para splits)\")\n",
    "print(\"  - Eficiente en memoria\")\n",
    "print(\"  - Ideal para datasets grandes\")\n",
    "print(\"  - Crece los árboles 'leaf-wise' en lugar de 'level-wise'\")\n",
    "\n",
    "# Definir parámetros\n",
    "lgb_params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1  # Silenciar warnings\n",
    "}\n",
    "\n",
    "print(f\"\\nParámetros del modelo:\")\n",
    "for param, value in lgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Crear modelo\n",
    "lgb_model = LGBMRegressor(**lgb_params)\n",
    "\n",
    "print(\"\\nEntrenando LightGBM...\")\n",
    "print(\"(Debería ser más rápido que Gradient Boosting y XGBoost)\")\n",
    "start_time = time.time()\n",
    "lgb_model.fit(X_train_minmax, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Entrenamiento completado en {train_time:.2f} segundos\")\n",
    "\n",
    "# Evaluar en validación\n",
    "y_pred_lgb = lgb_model.predict(X_val_minmax)\n",
    "mae_lgb = mean_absolute_error(y_val, y_pred_lgb)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
    "r2_lgb = r2_score(y_val, y_pred_lgb)\n",
    "\n",
    "# Evaluar en train\n",
    "y_pred_lgb_train = lgb_model.predict(X_train_minmax)\n",
    "mae_lgb_train = mean_absolute_error(y_train, y_pred_lgb_train)\n",
    "\n",
    "print(f\"\\nMétricas en conjunto de validación:\")\n",
    "print(f\"  MAE:  {mae_lgb:.4f}\")\n",
    "print(f\"  RMSE: {rmse_lgb:.4f}\")\n",
    "print(f\"  R²:   {r2_lgb:.4f}\")\n",
    "print(f\"\\nMétricas en conjunto de entrenamiento:\")\n",
    "print(f\"  MAE:  {mae_lgb_train:.4f}\")\n",
    "print(f\"\\nDiferencia train-val MAE: {abs(mae_lgb_train - mae_lgb):.4f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['LightGBM'] = {\n",
    "    'model': lgb_model,\n",
    "    'mae': mae_lgb,\n",
    "    'rmse': rmse_lgb,\n",
    "    'r2': r2_lgb,\n",
    "    'mae_train': mae_lgb_train,\n",
    "    'train_time': train_time,\n",
    "    'predictions': y_pred_lgb\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Resultados guardados en model_results['LightGBM']\")\n",
    "\n",
    "# Comparación rápida de velocidad\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARACIÓN RÁPIDA DE VELOCIDAD DE ENTRENAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Gradient Boosting: {model_results['Gradient Boosting']['train_time']:.2f}s\")\n",
    "print(f\"  XGBoost:           {model_results['XGBoost']['train_time']:.2f}s\")\n",
    "print(f\"  LightGBM:          {train_time:.2f}s\")\n",
    "\n",
    "fastest = min(\n",
    "    ('Gradient Boosting', model_results['Gradient Boosting']['train_time']),\n",
    "    ('XGBoost', model_results['XGBoost']['train_time']),\n",
    "    ('LightGBM', train_time),\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "print(f\"\\n✓ Modelo más rápido: {fastest[0]} ({fastest[1]:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.6: Comparar métodos de ensamble\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACIÓN DE MÉTODOS DE ENSAMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear tabla comparativa\n",
    "ensemble_models = ['Gradient Boosting', 'XGBoost', 'LightGBM']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in ensemble_models:\n",
    "    results = model_results[model_name]\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'MAE (val)': results['mae'],\n",
    "        'RMSE (val)': results['rmse'],\n",
    "        'R² (val)': results['r2'],\n",
    "        'MAE (train)': results['mae_train'],\n",
    "        'Overfitting': abs(results['mae_train'] - results['mae']),\n",
    "        'Tiempo (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTabla comparativa de métodos de ensamble:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo por MAE\n",
    "best_model_idx = comparison_df['MAE (val)'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Modelo']\n",
    "print(f\"\\n✓ Mejor modelo por MAE: {best_model_name} (MAE = {comparison_df.loc[best_model_idx, 'MAE (val)']:.4f})\")\n",
    "\n",
    "# Identificar modelo más rápido\n",
    "fastest_model_idx = comparison_df['Tiempo (s)'].idxmin()\n",
    "fastest_model_name = comparison_df.loc[fastest_model_idx, 'Modelo']\n",
    "print(f\"✓ Modelo más rápido: {fastest_model_name} ({comparison_df.loc[fastest_model_idx, 'Tiempo (s)']:.2f}s)\")\n",
    "\n",
    "# Crear visualizaciones comparativas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Subplot 1: MAE comparison\n",
    "axes[0, 0].bar(comparison_df['Modelo'], comparison_df['MAE (val)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0, 0].set_title('Comparación de MAE en Validación', fontsize=14)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['MAE (val)']):\n",
    "    axes[0, 0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: R² comparison\n",
    "axes[0, 1].bar(comparison_df['Modelo'], comparison_df['R² (val)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 1].set_ylabel('R²', fontsize=12)\n",
    "axes[0, 1].set_title('Comparación de R² en Validación', fontsize=14)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['R² (val)']):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 3: Training time comparison\n",
    "axes[1, 0].bar(comparison_df['Modelo'], comparison_df['Tiempo (s)'], \n",
    "               color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1, 0].set_ylabel('Tiempo de entrenamiento (s)', fontsize=12)\n",
    "axes[1, 0].set_title('Comparación de Tiempo de Entrenamiento', fontsize=14)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['Tiempo (s)']):\n",
    "    axes[1, 0].text(i, v + 1, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 4: Tiempo vs Rendimiento (scatter plot)\n",
    "axes[1, 1].scatter(comparison_df['Tiempo (s)'], comparison_df['MAE (val)'], \n",
    "                   s=200, c=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.6)\n",
    "for i, model in enumerate(comparison_df['Modelo']):\n",
    "    axes[1, 1].annotate(model, \n",
    "                        (comparison_df.loc[i, 'Tiempo (s)'], comparison_df.loc[i, 'MAE (val)']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[1, 1].set_xlabel('Tiempo de entrenamiento (s)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1, 1].set_title('Trade-off: Velocidad vs Precisión', fontsize=14)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de trade-off velocidad vs precisión\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANÁLISIS DE TRADE-OFF: VELOCIDAD VS PRECISIÓN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    efficiency = row['R² (val)'] / row['Tiempo (s)']  # R² por segundo\n",
    "    print(f\"\\n{row['Modelo']}:\")\n",
    "    print(f\"  MAE:     {row['MAE (val)']:.4f}\")\n",
    "    print(f\"  R²:      {row['R² (val)']:.4f}\")\n",
    "    print(f\"  Tiempo:  {row['Tiempo (s)']:.2f}s\")\n",
    "    print(f\"  Eficiencia (R²/s): {efficiency:.6f}\")\n",
    "\n",
    "# Análisis de overfitting\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANÁLISIS DE OVERFITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    overfitting_pct = (row['Overfitting'] / row['MAE (val)']) * 100\n",
    "    status = \"BAJO\" if overfitting_pct < 10 else \"MODERADO\" if overfitting_pct < 20 else \"ALTO\"\n",
    "    print(f\"  {row['Modelo']:18s}: Diferencia = {row['Overfitting']:.4f} ({overfitting_pct:.1f}%) - {status}\")\n",
    "\n",
    "# Conclusiones\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Mejor rendimiento (MAE): {best_model_name}\")\n",
    "print(f\"2. Más rápido: {fastest_model_name}\")\n",
    "print(f\"3. Todos los métodos de boosting muestran excelente rendimiento\")\n",
    "print(f\"4. LightGBM ofrece el mejor balance velocidad-precisión para este dataset\")\n",
    "print(f\"5. Los métodos de boosting generalmente superan a los modelos de bagging (RF, ET)\")\n",
    "\n",
    "# Comparar con mejores modelos de árboles\n",
    "if 'Random Forest' in model_results:\n",
    "    rf_mae = model_results['Random Forest']['mae']\n",
    "    best_ensemble_mae = comparison_df['MAE (val)'].min()\n",
    "    improvement = ((rf_mae - best_ensemble_mae) / rf_mae) * 100\n",
    "    print(f\"\\n6. Mejora sobre Random Forest: {improvement:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playstore-z4vSTk52-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
